<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2025//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_250101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">40443773</PMID><DateRevised><Year>2025</Year><Month>05</Month><Day>30</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">2624-8212</ISSN><JournalIssue CitedMedium="Internet"><Volume>8</Volume><PubDate><Year>2025</Year></PubDate></JournalIssue><Title>Frontiers in artificial intelligence</Title><ISOAbbreviation>Front Artif Intell</ISOAbbreviation></Journal><ArticleTitle>Co-Learning: code learning for multi-agent reinforcement collaborative framework with conversational natural language interfaces.</ArticleTitle><Pagination><StartPage>1431003</StartPage><MedlinePgn>1431003</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">1431003</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3389/frai.2025.1431003</ELocationID><Abstract><AbstractText>Online question-and-answer (Q&amp;A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. However, beginners in programming often struggle to correct code errors independently, limiting their learning efficiency. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3% improvement in Precision score and 15% improvement in time cost as compared with no E-RL method respectively. The results indicate that integrating E-RL with a multi-agent selection strategy can effectively enhance both the accuracy and efficiency of LLM-based code correction systems, making them more practical for educational and professional programming support scenarios.</AbstractText><CopyrightInformation>Copyright &#xa9; 2025 Yu, Wu, Zhan, Guo, Xu and Lee.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Yu</LastName><ForeName>Jiapeng</ForeName><Initials>J</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Wu</LastName><ForeName>Yuqian</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhan</LastName><ForeName>Yajing</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Guo</LastName><ForeName>Wenhao</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Xu</LastName><ForeName>Zhou</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lee</LastName><ForeName>Raymond</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, Faculty of Science and Technology, Beijing Normal University-Hong Kong Baptist University United International College, Zhuhai, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2025</Year><Month>05</Month><Day>15</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Artif Intell</MedlineTA><NlmUniqueID>101770551</NlmUniqueID><ISSNLinking>2624-8212</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">education</Keyword><Keyword MajorTopicYN="N">large language model</Keyword><Keyword MajorTopicYN="N">multi-agent</Keyword><Keyword MajorTopicYN="N">prompting</Keyword><Keyword MajorTopicYN="N">reinforcement learning</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>5</Month><Day>11</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2025</Year><Month>4</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2025</Year><Month>5</Month><Day>30</Day><Hour>8</Hour><Minute>31</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2025</Year><Month>5</Month><Day>30</Day><Hour>8</Hour><Minute>30</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2025</Year><Month>5</Month><Day>30</Day><Hour>4</Hour><Minute>29</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2025</Year><Month>5</Month><Day>15</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">40443773</ArticleId><ArticleId IdType="pmc">PMC12120352</ArticleId><ArticleId IdType="doi">10.3389/frai.2025.1431003</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Austin J., Odena A., Nye M., Bosma M., Michalewski H., Dohan D., et al. . (2021). Program synthesis with large language models. arXiv [Preprint]. arXiv:2108.07732.</Citation></Reference><Reference><Citation>Barrault L., Chung Y. A., Meglioli M. C., Dale D., Dong N., Duquenne P. A., et al. . (2023). SeamlessM4T-massively multilingual &amp; multimodal machine translation. arXiv [Preprint]. arXiv:2308.11596.</Citation></Reference><Reference><Citation>Bellifemine F., Poggi A., Rimassa G. (2001). Developing multi-agent systems with JADE. In Intelligent agents VII agent theories architectures and languages: 7th international workshop, ATAL 2000 Boston, MA, USA, July 7&#x2013;9, 2000 proceedings 7 (pp. 89&#x2013;103). Springer, Berlin Heidelberg.</Citation></Reference><Reference><Citation>Bradshaw J. M. (1997). An introduction to software agents. Software Agents
4, 3&#x2013;46. Available at: https://scholar.google.com.hk/citations?user=M5fazi4AAAAJ&amp;hl=zh-CN&amp;oi=sra</Citation></Reference><Reference><Citation>Chen X., Lin M., Sch&#xe4;rli N., Zhou D. (2023). Teaching large language models to self-debug. arXiv [Preprint]. arXiv:2304.05128.</Citation></Reference><Reference><Citation>Fang R., Bindu R., Gupta A., Kang D. (2024). LLM agents can autonomously exploit one-day vulnerabilities. arXiv [Preprint]. arXiv:2404.08144.</Citation></Reference><Reference><Citation>Ganguli D., Askell A., Schiefer N., Liao T. I., Luko&#x161;i&#x16b;t&#x117; K., Chen A., et al. . (2023). The capacity for moral self-correction in large language models. arXiv [Preprint]. arXiv:2302.07459.</Citation></Reference><Reference><Citation>iFLYTEK . (2023). Spark Desk V3.0. Version 1.0. Available online at: https://console.xfyun.cn/services/bm3 (Accessed November 9, 2023).</Citation></Reference><Reference><Citation>Kaelbling L. P., Littman M. L., Moore A. W. (1996). Reinforcement learning: a survey. J. Artif. Intell. Res. 4, 237&#x2013;285. doi: 10.1613/jair.301</Citation><ArticleIdList><ArticleId IdType="doi">10.1613/jair.301</ArticleId></ArticleIdList></Reference><Reference><Citation>Kiran B. R., Sobh I., Talpaert V., Mannion P., Al Sallab A. A., Yogamani S., et al. . (2021). Deep reinforcement learning for autonomous driving: a survey. IEEE Trans. Intell. Transp. Syst. 23, 4909&#x2013;4926. doi: 10.1109/TITS.2021.3054625, PMID:</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TITS.2021.3054625</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee R. S. (2005). Fuzzy-neuro approach to agent applications: From the AI perspective to modern ontology. Berlin: Springer Science &amp; Business Media.</Citation></Reference><Reference><Citation>Lee R. S., Liu J. N. (2001). I Jade Stock predictor &#x2013; an intelligent multi-agent based time series stock prediction system. In intelligent agent technology: Research and Development (pp. 495&#x2013;499).</Citation></Reference><Reference><Citation>Li H., Hao Y., Zhai Y., Qian Z. (2023). The Hitchhiker&#x2019;s guide to program analysis: a journey with large language models. arXiv [Preprint]. arXiv:2308.00245.</Citation></Reference><Reference><Citation>Lillicrap T. P., Hunt J. J., Pritzel A., Heess N., Erez T., Tassa Y., et al. . (2015). Continuous control with deep reinforcement learning. arXiv [Preprint]. arXiv:1509.02971.</Citation></Reference><Reference><Citation>Madaan A., Tandon N., Gupta P., Hallinan S., Gao L., Clark P., et al. . (2024). Self-refine: Iterative refinement with self-feedback. Adv Neural Inf Process Syst. 36.</Citation></Reference><Reference><Citation>Melo L. S., Sampaio R. F., Le&#xe3;o R. P. S., Barroso G. C., Bezerra J. R. (2019). Python-based multi-agent platform for application on power grids. Int. Trans. Electr. Energy Syst. 29:e12012. doi: 10.1002/2050-7038.12012</Citation><ArticleIdList><ArticleId IdType="doi">10.1002/2050-7038.12012</ArticleId></ArticleIdList></Reference><Reference><Citation>Nair V., Schumacher E., Tso G., Kannan A. (2023). DERA: enhancing large language model completions with dialog-enabled resolving agents. arXiv [Preprint]. arXiv:2303.17071.</Citation></Reference><Reference><Citation>Nijkamp E., Pang B., Hayashi H., Tu L., Wang H., Zhou Y., et al. . (2022). Codegen: an open large language model for code with multi-turn program synthesis. arXiv [Preprint]. arXiv:2203.13474.</Citation></Reference><Reference><Citation>OpenAI (2023). Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5).</Citation></Reference><Reference><Citation>Schick T., Sch&#xfc;tze H. (2020). Exploiting cloze questions for few shot text classification and natural language inference. arXiv [Preprint]. arXiv:2001.07676.</Citation></Reference><Reference><Citation>Shinn N., Labash B., Gopinath A. (2023). Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv [Preprint]. arXiv:2303.11366.</Citation></Reference><Reference><Citation>Silver D., Huang A., Maddison C. J., Guez A., Sifre L., Van Den Driessche G., et al. . (2016). Mastering the game of go with deep neural networks and tree search. Nature 529, 484&#x2013;489. doi: 10.1038/nature16961, PMID:</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/nature16961</ArticleId><ArticleId IdType="pubmed">26819042</ArticleId></ArticleIdList></Reference><Reference><Citation>Sumers T. R., Yao S., Narasimhan K., Griffiths T. L. (2023). Cognitive architectures for language agents. arXiv [Preprint]. arXiv:2309.02427.</Citation></Reference><Reference><Citation>Sun Y., Wang S., Feng S., Ding S., Pang C., Shang J., et al. . (2021). Ernie 3.0: large-scale knowledge enhanced pre-training for language understanding and generation. arXiv [Preprint]. arXiv:2107.02137.</Citation></Reference><Reference><Citation>Touvron H., Martin L., Stone K., Albert P., Almahairi A., Babaei Y., et al. . (2023). Llama 2: open foundation and fine-tuned chat models. arXiv [Preprint]. arXiv:2307.09288.</Citation></Reference><Reference><Citation>Wang L., Ma C., Feng X., Zhang Z., Yang H., Zhang J., et al. . (2024). A survey on large language model based autonomous agents. Front. Comp. Sci. 18, 1&#x2013;26. doi: 10.1007/s11704-024-40231-1, PMID:</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11704-024-40231-1</ArticleId></ArticleIdList></Reference><Reference><Citation>Xi Z., Chen W., Guo X., He W., Ding Y., Hong B., et al. . (2023). The rise and potential of large language model based agents: a survey. arXiv [Preprint]. arXiv:2309.07864.</Citation></Reference><Reference><Citation>Zhang H., Du W., Shan J., Zhou Q., Du Y., Tenenbaum J. B., et al. . (2023). Building cooperative embodied agents modularly with large language models. arXiv [Preprint]. arXiv:2307.02485.</Citation></Reference><Reference><Citation>Zhou W., Jiang Y. E., Li L., Wu J., Wang T., Qiu S., et al. . (2023). Agents: an open-source framework for autonomous language agents. arXiv [Preprint]. arXiv:2309.07870.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>