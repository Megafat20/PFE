<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2025//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_250101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">40447746</PMID><DateCompleted><Year>2025</Year><Month>05</Month><Day>30</Day></DateCompleted><DateRevised><Year>2025</Year><Month>05</Month><Day>30</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>15</Volume><Issue>1</Issue><PubDate><Year>2025</Year><Month>May</Month><Day>30</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Evaluating performance of large language models for atrial fibrillation management using different prompting strategies and languages.</ArticleTitle><Pagination><StartPage>19028</StartPage><MedlinePgn>19028</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">19028</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-025-04309-5</ELocationID><Abstract><AbstractText>This study evaluated large language models (LLMs) using 30 questions, each derived from a recommendation in the 2024 European Society of Cardiology (ESC) guidelines for atrial fibrillation (AF) management. These recommendations were stratified by class of recommendation and level of evidence. The primary objective was to assess the reliability and consistency of LLM-generated classifications compared to those in the ESC guidelines. Additionally, the study assessed the impact of different prompting strategies and working languages on LLM performance. Three prompting strategies were tested: Input-output (IO), 0-shot-Chain of thought (0-COT) and Performed-Chain of thought (P-COT) prompting. Each question, presented in both English and Chinese, was input into three LLMs: ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. The reliability of the different LLM-prompt combinations showed moderate to substantial agreement (Fleiss kappa ranged from 0.449 to 0.763). Claude 3.5 with P-COT prompting had the highest recommendation classification consistency (60.3%). No significant differences were observed between English and Chinese across most LLM-prompt combinations. Bias analysis of inconsistent outcomes revealed a propensity towards more recommended treatments and stronger evidence levels across most LLM-prompt combinations. The characteristics of clinical questions potentially influence LLM performance. This study highlights the limitations in the accuracy of LLM responses to AF-related questions. To gather more comprehensive insights, conducting repeated queries is advisable. Future efforts should focus on expanding the use of diverse prompting strategies, conducting ongoing model evaluation and refinement, and establishing a comprehensive, objective benchmarking system.</AbstractText><CopyrightInformation>&#xa9; 2025. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y" EqualContrib="Y"><LastName>Li</LastName><ForeName>Zexi</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y" EqualContrib="Y"><LastName>Yan</LastName><ForeName>Chunyi</ForeName><Initials>C</Initials><AffiliationInfo><Affiliation>Department of Pediatric Cardiology, West China Second University Hospital, Sichuan University, Chengdu, 610041, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Cao</LastName><ForeName>Ying</ForeName><Initials>Y</Initials><AffiliationInfo><Affiliation>Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gong</LastName><ForeName>Aobo</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Fanghui</ForeName><Initials>F</Initials><AffiliationInfo><Affiliation>Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zeng</LastName><ForeName>Rui</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 610041, Sichuan, China. zengrui_0524@126.com.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2023HXFH002</GrantID><Agency>1.3.5 Project for Disciplines of Excellence-Clinical Research Incubation Project, West China Hospital of Sichuan University</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant><Grant><GrantID>2024YFFK0046</GrantID><Agency>Sichuan Science and Technology Program</Agency><Country/></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2025</Year><Month>05</Month><Day>30</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D001281" MajorTopicYN="Y">Atrial Fibrillation</DescriptorName><QualifierName UI="Q000628" MajorTopicYN="N">therapy</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007802" MajorTopicYN="Y">Language</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D015203" MajorTopicYN="N">Reproducibility of Results</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D017410" MajorTopicYN="N">Practice Guidelines as Topic</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000098342" MajorTopicYN="N">Large Language Models</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">Artificial intelligence</Keyword><Keyword MajorTopicYN="N">Atrial fibrillation</Keyword><Keyword MajorTopicYN="N">ChatGPT</Keyword><Keyword MajorTopicYN="N">Large language models</Keyword><Keyword MajorTopicYN="N">Prompt engineering</Keyword></KeywordList><CoiStatement>Declarations. Competing interests: The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2025</Year><Month>2</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2025</Year><Month>5</Month><Day>26</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2025</Year><Month>5</Month><Day>31</Day><Hour>0</Hour><Minute>37</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2025</Year><Month>5</Month><Day>31</Day><Hour>0</Hour><Minute>36</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2025</Year><Month>5</Month><Day>30</Day><Hour>23</Hour><Minute>24</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2025</Year><Month>5</Month><Day>30</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">40447746</ArticleId><ArticleId IdType="doi">10.1038/s41598-025-04309-5</ArticleId><ArticleId IdType="pii">10.1038/s41598-025-04309-5</ArticleId><ArticleId IdType="pmc">PMC12125184</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Holzinger, A., Keiblinger, K., Holub, P., Zatloukal, K. &amp; M&#xfc;ller, H. AI for life: trends in artificial intelligence for biotechnology. New Biotechnol.74, 16&#x2013;24. 10.1016/j.nbt.2023.02.001 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36754147</ArticleId></ArticleIdList></Reference><Reference><Citation>Lu, Y., Wu, H., Qi, S. &amp; Cheng, K. Artificial intelligence in intensive care medicine: toward a ChatGPT/GPT-4 way?? Ann. Biomed. Eng.51, 1898&#x2013;1903. 10.1007/s10439-023-03234-w (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10182840</ArticleId><ArticleId IdType="pubmed">37179277</ArticleId></ArticleIdList></Reference><Reference><Citation>Sufi, F. Generative pre-trained transformer (GPT) in research: A systematic review on data augmentation. 15 99 (2024).</Citation></Reference><Reference><Citation>Kuroiwa, T. et al. The potential of ChatGPT as a Self-Diagnostic tool in common orthopedic diseases: exploratory study. J. Med. Internet. Res.25, e47621. 10.2196/47621 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10541638</ArticleId><ArticleId IdType="pubmed">37713254</ArticleId></ArticleIdList></Reference><Reference><Citation>Zandi, R. et al. Exploring diagnostic precision and triage proficiency: A comparative study of GPT-4 and bard in addressing common ophthalmic complaints. Bioengineering (Basel Switzerland)1110.3390/bioengineering11020120 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10886029</ArticleId><ArticleId IdType="pubmed">38391606</ArticleId></ArticleIdList></Reference><Reference><Citation>Passby, L., Jenko, N. &amp; Wernham, A. Performance of ChatGPT on specialty certificate examination in dermatology multiple-choice questions. Clin. Exp. Dermatol.49, 722&#x2013;727. 10.1093/ced/llad197 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">37264670</ArticleId></ArticleIdList></Reference><Reference><Citation>Yu, P. et al. Performance of ChatGPT on the Chinese postgraduate examination for clinical medicine: survey study. JMIR Med. Educ.10, e48514. 10.2196/48514 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10891494</ArticleId><ArticleId IdType="pubmed">38335017</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu, S. et al. Using AI-generated suggestions from ChatGPT to optimize clinical decision support. J. Am. Med. Inf. Association: JAMIA. 30, 1237&#x2013;1245. 10.1093/jamia/ocad072 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10280357</ArticleId><ArticleId IdType="pubmed">37087108</ArticleId></ArticleIdList></Reference><Reference><Citation>Su&#xe1;rez, A. et al. Unveiling the ChatGPT phenomenon: evaluating the consistency and accuracy of endodontic question answers. Int. Endod. J.57, 108&#x2013;113. 10.1111/iej.13985 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">37814369</ArticleId></ArticleIdList></Reference><Reference><Citation>Akakpo, M. G. The role of care-seeking behavior and patient communication pattern in online health information-seeking behavior - a cross-sectional survey. Pan Afr. Med. J.42, 124. 10.11604/pamj.2022.42.124.33623 (2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9430892</ArticleId><ArticleId IdType="pubmed">36060849</ArticleId></ArticleIdList></Reference><Reference><Citation>Chandran, V. P. et al. Mobile applications in medical education: A systematic review and meta-analysis. PloS One. 17, e0265927. 10.1371/journal.pone.0265927 (2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8947018</ArticleId><ArticleId IdType="pubmed">35324994</ArticleId></ArticleIdList></Reference><Reference><Citation>Alowais, S. A. et al. Revolutionizing healthcare: the role of artificial intelligence in clinical practice. BMC Med. Educ.2310.1186/s12909-023-04698-z (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10517477</ArticleId><ArticleId IdType="pubmed">37740191</ArticleId></ArticleIdList></Reference><Reference><Citation>Moons, P. &amp; Van Bulck, L. ChatGPT: can artificial intelligence Language models be of value for cardiovascular nurses and allied health professionals. Eur. J. Cardiovasc. Nurs.22, e55&#x2013;e59. 10.1093/eurjcn/zvad022 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36752788</ArticleId></ArticleIdList></Reference><Reference><Citation>Hopkins, A. M., Logan, J. M., Kichenadasse, G. &amp; Sorich, M. J. Artificial intelligence chatbots will revolutionize how cancer patients access information: ChatGPT represents a paradigm-shift. JNCI Cancer Spectr.710.1093/jncics/pkad010 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10013638</ArticleId><ArticleId IdType="pubmed">36808255</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, L. et al. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. NPJ Digit. Med.710.1038/s41746-024-01029-4 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10879172</ArticleId><ArticleId IdType="pubmed">38378899</ArticleId></ArticleIdList></Reference><Reference><Citation>Azimi, I., Qi, M., Wang, L., Rahmani, A. M. &amp; Li, Y. Evaluation of LLMs accuracy and consistency in the registered dietitian exam through prompt engineering and knowledge retrieval. Sci. Rep.15, 1506. 10.1038/s41598-024-85003-w (2025).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11718202</ArticleId><ArticleId IdType="pubmed">39789057</ArticleId></ArticleIdList></Reference><Reference><Citation>Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. 35 24824&#x2013;24837 (2022).</Citation></Reference><Reference><Citation>Lucas, M. M., Yang, J., Pomeroy, J. K. &amp; Yang, C. C. Reasoning with large Language models for medical question answering. J. Am. Med. Inf. Association: JAMIA. 31, 1964&#x2013;1975. 10.1093/jamia/ocae131 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11339506</ArticleId><ArticleId IdType="pubmed">38960731</ArticleId></ArticleIdList></Reference><Reference><Citation>Tsao, C. W. et al. Heart disease and stroke Statistics-2023 update: A report from the American heart association. Circulation147, e93&#x2013;e621. 10.1161/cir.0000000000001123 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36695182</ArticleId></ArticleIdList></Reference><Reference><Citation>Lee, T. J. et al. Evaluating ChatGPT responses on atrial fibrillation for patient education. Cureus16, e61680. 10.7759/cureus.61680 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11151148</ArticleId><ArticleId IdType="pubmed">38841294</ArticleId></ArticleIdList></Reference><Reference><Citation>Hillmann, H. A. K. et al. Accuracy and comprehensibility of chat-based artificial intelligence for patient information on atrial fibrillation and cardiac implantable electronic devices. Europace Eur. Pac. Arrhythm. Cardiac Electrophysiol. J. Work. Groups Cardiac Pac. Arrhythm. Cardiac Cell. Electrophysiol. Eur. Soc. Cardiol.2610.1093/europace/euad369 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10824484</ArticleId><ArticleId IdType="pubmed">38127304</ArticleId></ArticleIdList></Reference><Reference><Citation>Van Gelder, I. C. et al. 2024 ESC guidelines for the management of atrial fibrillation developed in collaboration with the European association for Cardio-Thoracic surgery (EACTS). Eur. Heart J.45, 3314&#x2013;3414. 10.1093/eurheartj/ehae176 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">39210723</ArticleId></ArticleIdList></Reference><Reference><Citation>Shi, S. et al. Prevalence and risk of atrial fibrillation in China: A National cross-sectional epidemiological study. Lancet Reg. Health Western Pac.23, 100439. 10.1016/j.lanwpc.2022.100439 (2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9252928</ArticleId><ArticleId IdType="pubmed">35800039</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, Y., Chen, Y. &amp; Sheng, J. Assessing ChatGPT as a medical consultation assistant for chronic hepatitis B: Cross-Language study of english and Chinese. JMIR Med. Inf.12, e56426. 10.2196/56426 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11342014</ArticleId><ArticleId IdType="pubmed">39115930</ArticleId></ArticleIdList></Reference><Reference><Citation>Suga, T., Uehara, O., Abiko, Y. &amp; Toyofuku, A. Evaluating large Language models for burning mouth syndrome diagnosis. J. Pain Res.18, 1387&#x2013;1405. 10.2147/jpr.S509845 (2025).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11930279</ArticleId><ArticleId IdType="pubmed">40124539</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao, F. F. et al. Benchmarking the performance of large Language models in uveitis: a comparative analysis of ChatGPT-3.5, ChatGPT-4.0, Google Gemini, and anthropic Claude3. Eye (London England). 39, 1132&#x2013;1137. 10.1038/s41433-024-03545-9 (2025).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11978972</ArticleId><ArticleId IdType="pubmed">39690303</ArticleId></ArticleIdList></Reference><Reference><Citation>Salmi, L. et al. A proof-of-concept study for patient use of open notes with large Language models. JAMIA Open.8, ooaf021. 10.1093/jamiaopen/ooaf021 (2025).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11980777</ArticleId><ArticleId IdType="pubmed">40206786</ArticleId></ArticleIdList></Reference><Reference><Citation>Patel, D. et al. Evaluating prompt engineering on GPT-3.5&#x2019;s performance in USMLE-style medical calculations and clinical scenarios generated by GPT-4. Sci. Rep.14, 17341. 10.1038/s41598-024-66933-x (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11284198</ArticleId><ArticleId IdType="pubmed">39069520</ArticleId></ArticleIdList></Reference><Reference><Citation>Ting, Y. T. et al. Performance of ChatGPT incorporated chain-of-thought method in bilingual nuclear medicine physician board examinations. Digit. Health. 10, 20552076231224074. 10.1177/20552076231224074 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10771043</ArticleId><ArticleId IdType="pubmed">38188855</ArticleId></ArticleIdList></Reference><Reference><Citation>Zapf, A., Castell, S., Morawietz, L. &amp; Karch, A. Measuring inter-rater reliability for nominal data - which coefficients and confidence intervals are appropriate? BMC Med. Res. Methodol.1610.1186/s12874-016-0200-9 (2016).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC4974794</ArticleId><ArticleId IdType="pubmed">27495131</ArticleId></ArticleIdList></Reference><Reference><Citation>Walker, H. L. et al. Reliability of medical information provided by ChatGPT: assessment against clinical guidelines and patient information quality instrument. J. Med. Internet. Res.25, e47479. 10.2196/47479 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10365578</ArticleId><ArticleId IdType="pubmed">37389908</ArticleId></ArticleIdList></Reference><Reference><Citation>Miyazaki, Y. et al. Performance of ChatGPT-4o on the Japanese medical licensing examination: evalution of accuracy in Text-Only and Image-Based questions. JMIR Med. Educ.10, e63129. 10.2196/63129 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11687171</ArticleId><ArticleId IdType="pubmed">39718557</ArticleId></ArticleIdList></Reference><Reference><Citation>Mihalache, A., Huang, R. S., Popovic, M. M. &amp; Muni, R. H. ChatGPT-4: an assessment of an upgraded artificial intelligence chatbot in the united States medical licensing examination. Med. Teach.46, 366&#x2013;372. 10.1080/0142159x.2023.2249588 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">37839017</ArticleId></ArticleIdList></Reference><Reference><Citation>Roos, J., Kasapovic, A., Jansen, T. &amp; Kaczmarczyk, R. Artificial intelligence in medical education: comparative analysis of ChatGPT, Bing, and medical students in Germany. JMIR Med. Educ.9, e46482. 10.2196/46482 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10507517</ArticleId><ArticleId IdType="pubmed">37665620</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin, S. Y., Chan, P. K., Hsu, W. H. &amp; Kao, C. H. Exploring the proficiency of ChatGPT-4: an evaluation of its performance in the Taiwan advanced medical licensing examination. Digit. Health. 10, 20552076241237678. 10.1177/20552076241237678 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10916498</ArticleId><ArticleId IdType="pubmed">38449683</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, Z. et al. Assessing the role of GPT-4 in thyroid ultrasound diagnosis and treatment recommendations: enhancing interpretability with a chain of thought approach. Quant. Imaging Med. Surg.14, 1602&#x2013;1615. 10.21037/qims-23-1180 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10895085</ArticleId><ArticleId IdType="pubmed">38415150</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. (2022).</Citation></Reference><Reference><Citation>Seghier, M. L. ChatGPT: not all languages are equal. Nature615, 216. 10.1038/d41586-023-00680-3 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36882613</ArticleId></ArticleIdList></Reference><Reference><Citation>Song, E. S. &amp; Lee, S. P. Comparative analysis of the response accuracies of large Language models in the Korean National dental hygienist examination across Korean and english questions. Int. J. Dental Hygiene. 10.1111/idh.12848 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11982589</ArticleId><ArticleId IdType="pubmed">39415339</ArticleId></ArticleIdList></Reference><Reference><Citation>Pak, R., Rovira, E. &amp; McLaughlin, A. Polite AI mitigates user susceptibility to AI hallucinations. Ergonomics 1&#x2013;11. 10.1080/00140139.2024.2434604 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">39610202</ArticleId></ArticleIdList></Reference><Reference><Citation>Hatem, R., Simmons, B. &amp; Thornton, J. E. A call to address AI hallucinations and how healthcare professionals can mitigate their risks. Cureus15, e44720. 10.7759/cureus.44720 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10552880</ArticleId><ArticleId IdType="pubmed">37809168</ArticleId></ArticleIdList></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>