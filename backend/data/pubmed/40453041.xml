<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2025//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_250101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">40453041</PMID><DateRevised><Year>2025</Year><Month>06</Month><Day>02</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">2296-9144</ISSN><JournalIssue CitedMedium="Internet"><Volume>12</Volume><PubDate><Year>2025</Year></PubDate></JournalIssue><Title>Frontiers in robotics and AI</Title><ISOAbbreviation>Front Robot AI</ISOAbbreviation></Journal><ArticleTitle>Simultaneous text and gesture generation for social robots with small language models.</ArticleTitle><Pagination><StartPage>1581024</StartPage><MedlinePgn>1581024</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.3389/frobt.2025.1581024</ELocationID><Abstract><AbstractText Label="INTRODUCTION" NlmCategory="UNASSIGNED">As social robots gain advanced communication capabilities, users increasingly expect coherent verbal and non-verbal behaviours. Recent work has shown that Large Language Models (LLMs) can support autonomous generation of such multimodal behaviours. However, current LLM-based approaches to non-verbal behaviour often involve multi-step reasoning with large, closed-source models-resulting in significant computational overhead and limiting their feasibility in low-resource or privacy-constrained environments.</AbstractText><AbstractText Label="METHODS" NlmCategory="UNASSIGNED">To address these limitations, we propose a novel method for simultaneous generation of text and gestures with minimal computational overhead compared to plain text generation. Our system does not produce low-level joint trajectories, but instead predicts high-level communicative intentions, which are mapped to platform-specific expressions. Central to our approach is the introduction of lightweight, robot-specific "gesture heads" derived from the LLM's architecture, requiring no pose-based datasets and enabling generalisability across platforms.</AbstractText><AbstractText Label="RESULTS" NlmCategory="UNASSIGNED">We evaluate our method on two distinct robot platforms: Furhat (facial expressions) and Pepper (bodily gestures). Experimental results demonstrate that our method maintains behavioural quality while introducing negligible computational and memory overhead. Furthermore, the gesture heads operate in parallel with the language generation component, ensuring scalability and responsiveness even on small or locally deployed models.</AbstractText><AbstractText Label="DISCUSSION" NlmCategory="UNASSIGNED">Our approach supports the use of Small Language Models for multimodal generation, offering an effective alternative to existing high-resource methods. By abstracting gesture generation and eliminating reliance on platform-specific motion data, we enable broader applicability in real-world, low-resource, and privacy-sensitive HRI settings.</AbstractText><CopyrightInformation>Copyright &#xa9; 2025 Galatolo and Winkle.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Galatolo</LastName><ForeName>Alessio</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Department of Information Technology, Uppsala University, Uppsala, Sweden.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Department of Women and Children's Health, Uppsala University, Uppsala, Sweden.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Winkle</LastName><ForeName>Katie</ForeName><Initials>K</Initials><AffiliationInfo><Affiliation>Department of Information Technology, Uppsala University, Uppsala, Sweden.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2025</Year><Month>05</Month><Day>16</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Front Robot AI</MedlineTA><NlmUniqueID>101749350</NlmUniqueID><ISSNLinking>2296-9144</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">behavior generation</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">generative model</Keyword><Keyword MajorTopicYN="N">interactive behaviors</Keyword><Keyword MajorTopicYN="N">multimodal behavior</Keyword><Keyword MajorTopicYN="N">social robot</Keyword></KeywordList><CoiStatement>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="medline"><Year>2025</Year><Month>6</Month><Day>2</Day><Hour>6</Hour><Minute>29</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2025</Year><Month>6</Month><Day>2</Day><Hour>6</Hour><Minute>28</Minute></PubMedPubDate><PubMedPubDate PubStatus="received"><Year>2025</Year><Month>2</Month><Day>21</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2025</Year><Month>4</Month><Day>25</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2025</Year><Month>6</Month><Day>2</Day><Hour>5</Hour><Minute>38</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">40453041</ArticleId><ArticleId IdType="pmc">PMC12122315</ArticleId><ArticleId IdType="doi">10.3389/frobt.2025.1581024</ArticleId><ArticleId IdType="pii">1581024</ArticleId></ArticleIdList></PubmedData></PubmedArticle></PubmedArticleSet>