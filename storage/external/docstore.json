{"docstore/data": {"3971ea48-3f84-4e36-8e07-2cba5766bbd0": {"__data__": {"id_": "3971ea48-3f84-4e36-8e07-2cba5766bbd0", "embedding": null, "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "464da0fd-1c4b-416e-a69e-4d2665cead28", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1b484b19ce46d721ea8a1b4f49a36e86efa6afedb75057f73db0ee2e45fc51b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AN ANALYSIS ON LARGE LANGUAGE MODELS IN HEALTHCARE :\nA CASE STUDY OF BIOBERT\nA PREPRINT\nShyni Sharaf\nSchool of Computer Science and Engineering\nKerala University of Digital Sciences-\nInnovation and Technology\nThiruvananthapuram, India\nshyni.cs22@duk.ac.in\nV . S. Anoop\nApplied NLP Research Lab\nSchool of Digital Sciences\nKerala University of Digital Sciences-\nInnovation and Technology\nThiruvananthapuram, India\nanoop.vs@duk.ac.in\nABSTRACT\nThis paper conducts a comprehensive investigation into applying large language models, particularly\non BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing\n(NLP) approaches in healthcare, shedding light on the limitations and challenges these methods\nface. Following that, this research explores the path that led to the incorporation of BioBERT\ninto healthcare applications, highlighting its suitability for addressing the specific requirements\nof tasks related to biomedical text mining. The analysis outlines a systematic methodology for\nfine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes\nvarious components, including the gathering of data from a wide range of healthcare sources, data\nannotation for tasks like identifying medical entities and categorizing them, and the application\nof specialized preprocessing techniques tailored to handle the complexities found in biomedical\ntexts. Additionally, the paper covers aspects related to model evaluation, with a focus on healthcare\nbenchmarks and functions like processing of natural language in biomedical, question-answering,\nclinical document classification, and medical entity recognition. It explores techniques to improve\nthe model\u2019s interpretability and validates its performance compared to existing healthcare-focused\nlanguage models. The paper thoroughly examines ethical considerations, particularly patient privacy\nand data security. It highlights the benefits of incorporating BioBERT into healthcare contexts,\nincluding enhanced clinical decision support and more efficient information retrieval. Nevertheless,\nit acknowledges the impediments and complexities of this integration, encompassing concerns\nregarding data privacy, integrity, bias mitigation, transparency, resource-intensive requirements, and\nthe necessity for model customization to align with diverse healthcare domains.\nKeywords Large language models \u00b7 Healthcare \u00b7 BioBERT \u00b7 Health informatics \u00b7 Natural Language Processing\n1 Introduction\nNLP processing has evolved to become LLM (Large Language Model). In the 1940s, after World War II, people\nrealized the importance of translation between languages and wanted to create a machine that could perform automatic\ntranslation. Early NLP systems were rule-based systems that humans manually programmed with rules for processing\nlanguage. These systems often had many limitations in handling complex language and could be easily deceived by\nunexpected inputs. In the early 2000s, statistical NLP models began to emerge. These models were trained on large\ntext datasets and learned to predict the next word in a sequence based on preceding words. Statistical NLP models\nexhibited greater robustness compared to rule-based systemsWang et al. [2023a] and could handle a wider range of\nlanguage tasks. By the mid-2010s, deep learning models started to revolutionize NLP. These models, based on artificial\nneural networks, had the capability to learn intricate patterns from data. Deep learning models Lavanya and Sasikala\n[2021]quickly outperformed statistical NLP models such as machine translation, summarization of texts, and question\nanswering. In 2017, the Transformer La Quatra and Cagliero [2022]a deep learning model designed for processing\narXiv:2310.07282v2  [cs.AI]  12 Oct 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a28f41d6-04ff-4068-8062-af73ed0ec56c": {"__data__": {"id_": "a28f41d6-04ff-4068-8062-af73ed0ec56c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da847f53-93e8-4a6e-8dfc-a6107e68db88", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a08d6b5c8d20467d53250c235952703c52778a90bff612b891052d4a556a5713", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nsequential data, such as text, marked a significant milestone. It achieved state-of-the-art results across numerous NLP\ntasks and soon became the standard architecture for training LLMs. This shift led to the emergence of natural language\nprocessing (NLP) as we know it today. NLP has radically transformed into the era of large language models (LLMs).\nLLMs are trained on massive text datasets, sometimes containing hundreds of billions or even trillions of words.\nThis extensive training enables LLMs to understand the intricate patterns and relationships inherent in language. As\na result, LLMs have fundamentally altered how we interact with and harness the power of language. Popular LLMs\nlike GPT-3.5, GPT-4, PaLM, Cohere, LaMDA, Llama, and others have revolutionized our interaction with data by\nredefining the boundaries of language understanding and generation. Natural Language Processing (NLP) is a branch\nof artificial intelligence (AI) and computational linguistics that facilitates interaction between computers and humans\nthrough natural language. LLMsReddy [2023]process vast amounts of textual data, learn the underlying patterns, and\ngenerate contextually relevant human-like text. This technology has not only catalyzed but has become a driving force\nin transforming healthcare and biomedical applications. In this article, we conduct a comparative analysis of the\ndiverse applications of LLMs in the healthcare and biomedical domains. We explore how LLMs are reshaping the\nlandscape by offering innovative solutions to long-standing challenges. Current healthcare and biomedical systems often\noperate inefficiently, have limited access to relevant information, and involve cumbersome documentation processes.\nLLMs can address these challenges by providing rapid, context-aware responses to medical queries, extracting valuable\ninsights from unstructured data, and automating clinical documentation. The major contributions of this article are as\nfollows:\n\u2022 Conducts a detailed evaluation of the existing prominent state-of-the-art large language models introduced in\nthe healthcare domain.\n\u2022 Taking BioBERT as a reference pre-trained language model, we check the applications of the same in\nhealthcare.\n\u2022 Discusses the prominence of BioBERT in downstream clinical natural language processing tasks and discusses\nthem in detail.\n\u2022 Outlines the challenges with LLM in healthcare and presents the future research directions.\n1.1 An overview of LLM components\n\u2022 Input Text: Initially, the LLM receives raw text as input. This text can be in sentences, paragraphs, or\ndocuments.\n\u2022 Tokenization: The text we give is divided into individual tokens. Tokens can be words, subwords, or characters,\ndepending on LLMs tokenization scheme. This step breaks down the text into manageable units for processing.\n\u2022 Word Embeddings: Each token is transformed into a high-dimensional vector through word embeddings.\nThese vectors capture the token\u2019s meaning and context. Word embeddings are learned during the model\u2019s\ntraining using a vast amount of text data.\n\u2022 Transformer Layers: The embedded vectors are passed through multiple transformer layers. Each transformer\nlayer consists of two main components:\n\u2013 Multi-Head Self-Attention: This component weighs the importance of each token in relation to others,\ncapturing dependencies and context.\n\u2013 Feedforward Neural Networks: Complex transformations are applied to the vectors, enhancing the\nmodel\u2019s understanding of patterns and relationships.\n\u2022 Output Layer: After processing through the transformer layers, the output is fed into a linear layer. This\nlinear layer generates a probability distribution that spans the model\u2019s vocabulary. It estimates the probability\nof different words or word sequences following the input text.\n\u2022 Probability Estimation: The probability distribution generated in the previous step is used for various tasks\nsuch as language generation, text completion, and question answering.\n\u2022 Training and Fine-Tuning: LLMs are initially trained on a large text corpus to learn embeddings and model\nparameters. Fine-tuning can follow, where the model is further trained on task-specific data to adapt its\nlanguage understanding to the specific task or domain.\n2 NLP for Healthcare Applications\nLLMs have emerged as a transformative technology in healthcare, enabling an extensive range of applications, from\nclinical decision support to medical data analysis. LLMs allow healthcare professionals to harness the power of language\ndata for improved patient care, research, and administrative tasks.\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19773e18-7996-40aa-bb27-425f411c2ac4": {"__data__": {"id_": "19773e18-7996-40aa-bb27-425f411c2ac4", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfc256a1-d765-40cd-abfd-aeb0027a80df", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7a95a04cecd74aaeb975827038661b043b91477734bafcfd65b28fc251ee627", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "394523af-3ce3-4c15-89aa-32ca5282a11a", "node_type": "1", "metadata": {}, "hash": "b61c09940c23f84ad760b5c7d695794f95dc5da7076f44a0cf4abd86a68e65cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Medical Question Answering: LLMs can answer medical questions, providing quick and accurate responses.\nThis application aids healthcare professionals in accessing medical knowledge and information rapidly.\n\u2022 Electronic Health Record (EHR) Analysis: LLMs can analyze unstructured text in electronic health records,\nextracting valuable insights about patient histories, diagnoses, treatments, and clinical notes. This supports\nclinical decision-making and research.\n\u2022 Clinical Documentation: LLMs can assist healthcare providers in generating clinical notes, reports, and\ndocumentation. This streamlines the documentation process, allowing clinicians to focus more on patient care.\n\u2022 Medical Imaging: LLMs can assist in medical image interpretation by generating natural language descriptions\nof images. This can improve communication between radiologists and referring physiciansWang et al.\n[2023b]Rao et al. [2023].\n\u2022 Clinical Decision Support: LLMs can provide context-aware information to support clinical decisions. They\ncan recommend treatment options, predict patient outcomes, and identify potential risks Rao et al. [2023].\n\u2022 Healthcare Communication: LLMs can improve doctor-patient communication by offering language transla-\ntion services, ensuring effective communication in multilingual healthcare settings Yunxiang et al. [2023]\n\u2022 Patient Engagement: LLMs can be used in chatbots and virtual assistants Ray [2023] to engage with patients,\nanswer their healthcare queries, and provide health-related information and guidance. Healthcare professionals\ncan use NLP to extract relevant information from patient records, such as medical history, medication allergies,\nand previous diagnoses, enabling the creation of personalized treatment plans and early identification of\nhigh-risk patients for disease prevention.\n\u2022 Enhancing Medical Research: NLP can also analyze large amounts of medical data to identify patterns and\ntrendsHao et al. [2018]. It helps researchers to develop new treatments and therapies.\n\u2022 Improving Clinical Trials: NLP algorithms can sift through much data and extract information relevant to the\nclinical trial. NLP helps clinical trials Chen et al. [2020]by finding the right participants faster and cheaper\nthrough patient data analysis and improves efficiency. It reduces the time and cost.\n\u2022 Improving Digital Health Records: NLP can make digital patient records more correct and complete. These\nrecords hold information about a person\u2019s health history and treatments. NLP helps doctors to get the right\ndetails from these health records Costea [2020]so they can make better decisions for patient care.\n\u2022 Supports Medical Practitioners: NLP makes many everyday tasks of health professionals easierDemner-\nFushman et al. [2009]. For instance, it finds possible issues with medicines, helps doctors adjust treatment\nplans, and helps doctors write notes faster by saving time and reducing errors, so they can spend more time\ncaring for patients. Also, NLP aids in extracting Information from medical literature, helping healthcare\nprofessionals to learn Henwood and Lister [2007]stay current with the latest research and best practices.\n3 Language Models and Healthcare\nLarge Language Models (LLMs) are one of the most exciting areas in Artificial Intelligence (AI) research that have\nthe ability to process and generate human-like text for various healthcare applications. The more data we train, the\nmore predictions will be more accurate. Mainly used LLM are GPT-3, BERT, and RoBERTa Liu et al. [2019], which\nare trained on billions of words and patterns. so these models can understand the structure easily and generate text.\nOnce the model is generated it can be fine-tuned for a specific Task. The applications of LLM Hao et al. [2018]\nhealthcare have many different aspects and have the power to bring about significant positive changes in various\nfields. These technologies offer real-time assistance to healthcare professionals by helping them diagnose diseases and\ngive the right treatments without Errors. Predictive Analytics in healthcare can use data to predict disease outbreaks\nand enhance healthcare delivery efficiency. The significance of studying LLM applications in healthcare is that it is\nversatile. LLMs should be used in healthcare in a collaborative and verified way to ensure responsible and effective\nuse, ultimately improving patient care. This means the use of LLMs should be carefully monitored and evaluated,\nand thereby identifying potential problems or risks. LLMs are a powerful tool that has the potential to revolutionize\nhealthcare.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "394523af-3ce3-4c15-89aa-32ca5282a11a": {"__data__": {"id_": "394523af-3ce3-4c15-89aa-32ca5282a11a", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfc256a1-d765-40cd-abfd-aeb0027a80df", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7a95a04cecd74aaeb975827038661b043b91477734bafcfd65b28fc251ee627", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19773e18-7996-40aa-bb27-425f411c2ac4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4ba240beb77dfa93e9545791c0a0e9b24abfc2a42e3a9eec8f207a26d99f9fe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Predictive Analytics in healthcare can use data to predict disease outbreaks\nand enhance healthcare delivery efficiency. The significance of studying LLM applications in healthcare is that it is\nversatile. LLMs should be used in healthcare in a collaborative and verified way to ensure responsible and effective\nuse, ultimately improving patient care. This means the use of LLMs should be carefully monitored and evaluated,\nand thereby identifying potential problems or risks. LLMs are a powerful tool that has the potential to revolutionize\nhealthcare.\n3.1 Benefits of LLMs in Healthcare:\n\u2022 Improved Support for Clinical Decisions: LLMs assist healthcare providers in decision-making by providing\naccess to a vast amount of medical knowledge and up-to-date research. They can suggest potential diagnoses,\ntreatment options, or relevant research articles quickly. LLMs can make the diagnosis and data more accurate\nthan humans; thereby, the quality of outcomes and care of patients can be improved.\n3", "mimetype": "text/plain", "start_char_idx": 4140, "end_char_idx": 5140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e5bdf5d1-d26f-487b-a671-1c2a3f5a4fd7": {"__data__": {"id_": "e5bdf5d1-d26f-487b-a671-1c2a3f5a4fd7", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "013f11cf-c5c8-4d29-8a5e-7ecd98587909", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c6265914e11897d33f6edc7f2e259de3cf708c011816813ea5c7927bf37bb4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a918fb02-3a2d-48ad-9db9-4ed4e30e7d2c", "node_type": "1", "metadata": {}, "hash": "1f619f194410287b2cf95d1d86a091c0987d8372991bd39c9bbd74bb321ca598", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Efficient Information Retrieval: LLMs can be used to clarification medical tests by analyzing the results\nproviding valuable information and helping to find out the abnormalities. By this, we can reduce the time and\ncost of interpreting the results and improve accuracy and reliability.\n\u2022 Clarification of Medical Tests: LLMs can identify clinical trials by analyzing the current conditions of the\npatient, medical history, and treatment plans. This will improve the efficiency and effectiveness and provide\npotential lifesaving treatments.\n\u2022 Searching for Potential Clinical Trials: LLMs can identify clinical trials by analyzing the current conditions\nof the patient, medical history, and treatment plans. This will improve the efficiency and effectiveness and\nprovide potential lifesaving treatments\n3.2 Limitations and Challenges of Using LLMs in Healthcare\n\u2022 Data Privacy and Security: Integrating LLMs into healthcare must proceed cautiously to safeguard highly\nsensitive healthcare data. Ensuring data privacy and security, along with compliance with regulations such as\nHIPAA, is paramount to prevent the potentially severe consequences of data breaches.\n\u2022 Bias and Fairness: LLMs trained on biased data may produce biased or unfair results in healthcare applications.\nThis can lead to disparities in care, misdiagnoses, or unfair allocation of resources.\n\u2022 Lack of Transparency: LLMs often operate as \"black boxes,\" making it challenging to understand their\ndecision-making processes. This lack of transparency can hinder trust among healthcare professionals and\npatients.\n\u2022 Quality Control: Ensuring the quality and accuracy of information generated or retrieved by LLMs is crucial.\nErroneous information or recommendations could harm patients or mislead healthcare providers.\n\u2022 Concern of Ethical issues : Using LLMs in healthcare raises ethical concerns, such as the potential for\ntechnology to replace human interaction in patient care, leading to depersonalized medicine.\n\u2022 Resource Intensiveness: Developing, fine-tuning, and maintaining LLMs for healthcare can be resource-\nintensive regarding computational power, data annotation, and expert oversight.\n\u2022 Generalization Challenges: LLMs may struggle with generalizing to specific healthcare domains, specialties,\nor rare conditions if not adequately fine-tuned. Customization may be necessary.\n4 Related Studies\nIn this section, we review relevant articles that have explored the integration of LLMs in healthcare applications.\nThe articles showcase the significant impact of large language models in various healthcare-related tasks, such as\nbiomedical text mining, medical image interpretation, medical question answering, and processing electronic health\nrecords. They also highlight the need for careful evaluation and consideration of limitations when applying these models\nin clinical settings. The strengths include improved task performance and potential benefits for healthcare However, the\nresource-intensive nature of such models and potential challenges in fine-tuning for specific healthcare applications\nshould be considered. N. Kang et al. Kang et al. [2013]primarily focus on evaluating the performance of MetaMap and\nPeregrine tools used for biomedical concept normalization. The study investigates the usefulness of rule-based NLP\nmodules that are used to enhance the performance of MetaMap and Peregrine, an adjunct to dictionary-based concept\nnormalization in the biomedical field, to evaluate the Corpus for Arizona Disease.\nS. A. Hasan et al. Hasan and Farri [2019] discuss the application of deep learning(DL) techniques in clinical natural\nlanguage processing (CNLP). The model emphasizes the use of DL models for various clinical applications. Deep\nlearning-driven clinical NLP applications include diagnostic inferencing, biomedical article retrieval, clinical paraphrase\ngeneration, adverse drug event detection, and medical image caption generation. J. Lee et al. lee2020biobertLee et al.\n[2020]introduced BioBERT, a pre-trained biomedical language representation model tailored for biomedical text mining.\nBioBERT\u2019s training involved a substantial biomedical text corpus. This model excelled in various tasks such as named\nentity recognition, relation extraction, and question answering, achieving state-of-the-art performance across the board.\nShin et al.Shin et al. [2020]contributed to the field with BioMegatron, a larger pre-trained biomedical language\nmodel aimed at biomedical text mining analogous to BioBERT. Differing in scale, BioMegatron was trained on an even\nmore extensive corpus of biomedical text and exhibited state-of-the-art performance in tasks such as entity recognition,\nrelation extraction, and question-answering. Additionally, X. Yang et al.Yang et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a918fb02-3a2d-48ad-9db9-4ed4e30e7d2c": {"__data__": {"id_": "a918fb02-3a2d-48ad-9db9-4ed4e30e7d2c", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "013f11cf-c5c8-4d29-8a5e-7ecd98587909", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c6265914e11897d33f6edc7f2e259de3cf708c011816813ea5c7927bf37bb4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5bdf5d1-d26f-487b-a671-1c2a3f5a4fd7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "10d7b554bd7527a33c7ea797c2c8f8222c9c4975f97fb1d192943cb0975e191e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Shin et al.Shin et al. [2020]contributed to the field with BioMegatron, a larger pre-trained biomedical language\nmodel aimed at biomedical text mining analogous to BioBERT. Differing in scale, BioMegatron was trained on an even\nmore extensive corpus of biomedical text and exhibited state-of-the-art performance in tasks such as entity recognition,\nrelation extraction, and question-answering. Additionally, X. Yang et al.Yang et al. [2022a] presented GatorTron, a\nsubstantial clinical language model created for processing and interpreting electronic health records (EHRs). With\nextensive scaling in model parameters and training data, GatorTron significantly improved performance across clinical\nNLP tasks, offering potential enhancements in healthcare-related natural language processing by evaluating it on 5\nclinical NLP tasks like clinical concept extraction, medical relation extraction, semantic textual similarity, natural\n4", "mimetype": "text/plain", "start_char_idx": 4430, "end_char_idx": 5363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "73d32840-e30b-40b3-bdd5-2a084a44186e": {"__data__": {"id_": "73d32840-e30b-40b3-bdd5-2a084a44186e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2787ee49-8958-4540-addc-80f5b6d8dde7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "28ffda9509ea5b6ec8477807fcb748d297d55cb62d30f7b4286d68e005954c8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "836e9473-7b27-4b48-8df6-dabf4d1e7aca", "node_type": "1", "metadata": {}, "hash": "178bd7f92cca4cdd489c2180ae1d25d60a04a783a4bfc86c0e95707c3295f267", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nTable 1: Some state-of-the-art approaches in using LLM and related techniques in the healthcare NLP\nAuthor Model Methodology\nN. Kang et al. Kang et al. [2013] Rule-based NLP A rule-based NLP module used to enhance the perfor-\nmance of MetaMap and Peregrine.\nS. A. Hasan et al. Hasan and Farri [2019] Deep Learning Addresses the challenges posed by clinical documents,\nincluding acronyms, nonstandard clinical jargon, incon-\nsistent document structure, and privacy concerns.\nJ. Lee et al. Lee et al. [2020] BioBERT Pre-training on large-scale biomedical corpora out-\nperforms BERT and other models in biomedical text-\nmining tasks.\nHC Shin et al. Shin et al. [2020] BioMegatron Empirical study on factors affecting domain-specific\nlanguage models, pre-training on larger domain corpus.\nX. Yang et al. Yang et al. [2022b] GatorTron Developing a large clinical language model, scaling up\nthe number of parameters and training data.\nK. Singhal et al.Singhal et al. [2022] MultiMedQA, PaLM,\nFlan-PaLM, Med-PaLM MultiMedQA benchmark, human evaluation of model\nanswers, instruction prompt tuning.\nWang et al.Wang et al. [2023b] [5] ChatGPT, CAD networks Integrating LLMs with CAD networks, enhancing out-\nput with natural language text.\nS. Reddy et al.Reddy [2023] \u2013 Discusses the potential use of Large Language Models\n(LLMs) in healthcare. Highlights concerns related to\nmisinformation and data falsification. Proposes a frame-\nwork for evaluation, including human assessments.\nH. Zhang et al. Zhang et al. [2023] HuatuoGPT Leveraging distilled data from ChatGPT and real-world\ndata from doctors for medical consultation, reward\nmodel training.\nK. Singhal et al.Singhal et al. [2023] Med-PaLM - 2 Improving upon Med-PaLM with base LLM improve-\nments, medical domain fine-tuning, and prompting\nstrategies.\nlanguage inference (NLI), and medical question answering (MQA).\nJ.Singhal et al.Singhal et al. [2022]explored the encoding of clinical knowledge using Large Language Models.\nThey demonstrated that training LLMs on extensive clinical text enabled them to accurately answer questions related\nto clinical concepts, showcasing their potential in encoding clinical knowledge. They proposed a robust framework\nfor human evaluation of model responses, incorporating factors such as factuality, precision, potential harm, and bias\ninto the assessment process. PaLM, and its instruction-tuned variant, Flan-PaLM, were evaluated using MultiMedQA.\nWang et al.Wang et al. [2023b]presented Chatcad, a large language model designed for interactive computer-aided\ndiagnosis (CAD) in medical image analysis. Trained on a dataset featuring medical images and their accompanying text\ndescriptions, Chatcad demonstrated the ability to accurately diagnose diseases from images, aiding radiologists in their\ndiagnoses.\nS. Reddy et al.Kang et al. [2013]reddy2023evaluatingintroduced a framework for evaluating the translational\nvalue of Large Language Models (LLMs) in healthcare. This framework was a comprehensive tool for assessing\nLLMs\u2019 performance in healthcare applications. It was subsequently employed to assess the NLP performance of\nLLM\u2019s on the grounds of not assessing the models\u2019 functional, utility, and ethical aspects as they apply to healthcare,\nand recommended governance aspects of LLMs in healthcare are required. Zhang et al.Zhang et al. [2023]unveiled\nHuatuoGPT, a specialized LLM tailored for medical consultation. By leveraging data from ChatGPT and real-\nworld doctors, HuatuoGPT was fine-tuned to provide clinical advice and support to patients. This unique approach\nimproved its performance, achieving state-of-the-art results in medical consultation tasks. K. Singhal et al.Singhal et al.\n[2023]introduced Med-PaLM 2, an LLM designed for expert-level medical question answering.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "836e9473-7b27-4b48-8df6-dabf4d1e7aca": {"__data__": {"id_": "836e9473-7b27-4b48-8df6-dabf4d1e7aca", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2787ee49-8958-4540-addc-80f5b6d8dde7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "28ffda9509ea5b6ec8477807fcb748d297d55cb62d30f7b4286d68e005954c8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73d32840-e30b-40b3-bdd5-2a084a44186e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "879498b4944f2a3dadbe6201ef0f98bb1463f23bbbc76504c9871c7a541e1c80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhang et al.Zhang et al. [2023]unveiled\nHuatuoGPT, a specialized LLM tailored for medical consultation. By leveraging data from ChatGPT and real-\nworld doctors, HuatuoGPT was fine-tuned to provide clinical advice and support to patients. This unique approach\nimproved its performance, achieving state-of-the-art results in medical consultation tasks. K. Singhal et al.Singhal et al.\n[2023]introduced Med-PaLM 2, an LLM designed for expert-level medical question answering. This model achieved\nremarkable scores in medical question-answering tasks with a score of 67.2% on the MedQA dataset, highlighting its\npotential for delivering high-precision performance to medical question answering.\n5 An Analysis of LLMs in Healthcare - A Case Study of BioBERT\nIn this section, we delve into the methodologies and outcomes of the aforementioned articles. We assess how LLMs\nare employed to address healthcare challenges and explore their impact on various aspects of the healthcare industry.\n5", "mimetype": "text/plain", "start_char_idx": 3399, "end_char_idx": 4384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a1da25fb-f75c-4662-b92d-1412fd58aec5": {"__data__": {"id_": "a1da25fb-f75c-4662-b92d-1412fd58aec5", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5bc688e9-54d5-4bc6-8b7d-f2f87d213a7f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "08b0609f98c457ed375f51e87648abad7e183c717cf77e4d9127aa970cde6cfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nThe paper \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining\" by J Lee\net al. Lee et al. [2020] investigates how the recently introduced pre-trained language model BERT can be adapted\nfor biomedical corpora. In this article, we explore the possibility of Fine-tuning BioBERT for the healthcare domain,\nwhich can be a valuable endeavor given its success in biomedical text-mining tasks. To adapt BioBERT for healthcare\napplications, methodology outlines the steps and considerations for fine-tuning BioBERT for healthcare-specific tasks.\nIt emphasizes the importance of domain expertise, data quality, and ethical considerations in developing robust and\nreliable healthcare language models. To adapt BioBERT for healthcare applications, the following methodology can be\nconsidered:\n\u2022 Data Collection: Gather a comprehensive and diverse dataset from healthcare and biomedical sources. This\ndataset should include electronic health records (EHRs) Yang et al. [2022a]medical literature, clinical notes,\nmedical imaging reports, and other relevant sources. And annotate the data for various healthcare-related tasks,\nsuch as medical entity recognition (e.g., disease names, medications, procedures), medical text classification\n(e.g., diagnosis prediction, disease classification), and medical question-answeringSinghal et al. [2023].\n\u2022 Pre-processing: Prepare the data by cleaning and formatting it for training. This may involve standardizing\nmedical terms, removing duplicates, removing any errors or inconsistencies in the data, and handling missing\nvalues. Then Customize tokenization to accommodate the unique vocabulary and structure of biomedical\nand clinical texts. Clinical text data often contains specialized vocabulary and structure, so it is important\nto use a customized tokenizer for this type of data. Hence specialized tokenizers may be needed to handle\nmedical terminology, abbreviations, and symbols. Some common tokenizers for biomedical and clinical text\ndata include:\n\u2013 The BioBERT tokenizer. This tokenizer is based on the BERT tokenizer but has been customized to\nhandle medical terminology and abbreviations.\n\u2013 The MedTokenizer. This tokenizer is specifically designed for biomedical text data.\n\u2013 The SciBERT tokenizer. This tokenizer is designed for scientific text data, which includes biomedical\ntext.\nFigure 1: Overall architecture for BioBERT Pre-training\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a6c95a7-906f-4554-80c4-c1a6cff2e352": {"__data__": {"id_": "9a6c95a7-906f-4554-80c4-c1a6cff2e352", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ac1ab6d-be3f-458a-8e5d-024316fa88ba", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ae3fde92cb199afc0e3c02908590433b4a7572dc25e05f48bb08b682c4478f56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nFigure 2: Overall architecture for BioBERT Finetuning\nBioBERT is a pre-trained language model trained on a massive dataset of biomedical text. The pre-trained\nweights represent the model\u2019s understanding of the general structure and semantics of biomedical text. Design a set of\ndownstream tasks specific to healthcare. Many different downstream tasks can be performed using BioBERT. Some\ncommon tasks include:\n\u2022 Medical Entity Recognition: This task involves identifying and extracting medical entities from text. These\nentities can include diseases, medications, and medical procedures.\n\u2022 Medical Text Classification: In this task, the text is categorized into different healthcare-related categories.\nExamples of categories include diagnosis, prognosis, and treatment.\n\u2022 Disease Prediction: This task involves predicting the likelihood of a patient having a particular disease.\n\u2022 Medical Question-Answering: This task involves answering questions about medical topics based on text.\nFine-tune BioBERT on these tasks using the annotated healthcare dataset. Fine-tuning is adjusting the model\u2019s weights\nto improve performance on a specific task. This is done by feeding the model the annotated healthcare dataset and\nletting it learn from it. Apply appropriate loss functions. A loss function measures the model\u2019s performance on the task.\nThe loss function is used to update the model\u2019s weights during fine-tuning. Incorporate transfer learning techniques:\nTransfer learning involves using a model trained on one task to enhance the performance of a model on a distinct task.\nThis can be achieved by initializing the new model with the pre-trained model\u2019s weights. Conduct experiments on\nhyperparameters. Hyperparameters represent the configurations of the machine learning algorithm and significantly\ninfluence the model\u2019s performance. Common hyperparameters to explore encompass:\n\u2022 Adjust the learning rate, which dictates the magnitude of weight updates in each training iteration.\n\u2022 Vary the batch size, determining the quantity of samples utilized for weight updates in each training iteration.\n\u2022 Modify the number of epochs, specifying how often the model undergoes training on the data.\n5.1 Evaluation Metrics:\nAssess the fine-tuned BioBERT model across a range of healthcare-related benchmarks and tasks. These include\nbiomedical NLP tasks, medical question-answering, clinical document classification, medical entity recognition,\ngenerating discharge summaries, interpreting medical records, and providing medical advice.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44aaa760-cc26-4fc5-873b-4684ca7933f4": {"__data__": {"id_": "44aaa760-cc26-4fc5-873b-4684ca7933f4", "embedding": null, "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebb35667-1ec2-4326-9877-959aa17912d2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9d8b7cc3a9a564f71de36b0252733f61c3b5d9a2f55bdba6995b8c4e59859d51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 F1 score: It is calculated by taking the harmonic mean of precision and recall. it is the measure of the accuracy\nand completeness of the model\u2019s predictions. The F1 score is a good metric for tasks such as medical entity\nrecognition and text classification.\n\u2022 Accuracy: Accuracy is the percentage of predictions that the model gets correct.\n\u2022 Precision: Precision is the percentage of positive predictions that are actually positive.\n\u2022 Recall: Recall is the percentage of actual positives that are predicted as positive.\n\u2022 AUC: AUC is the area under the receiver operating characteristic curve. It measures the model\u2019s ability\nto distinguish between positive and negative examples. AUC is a good metric for tasks such as medical\nquestion-answering and disease prediction.\n\u2022 C-index: The C-index measures the model\u2019s ability to predict the survival of patients.\n5.2 Model Interpretability:\nTo enhance the interpretability of a fine-tuned BioBERT model, employ the following techniques:\n\u2022 Analyze the model\u2019s predictions : Examine the model\u2019s predictions and comprehend their rationale. This\ninvolves inspecting the model\u2019s features for making predictions and scrutinizing the attention weights assigned\nto various parts of the text.\n\u2022 Utilize visualization techniques : Make the model\u2019s predictions more comprehensible through graphical\nrepresentations. Employ heat maps to visualize attention weights or other visualization methods to elucidate\nhow the model generates predictions.\n\u2022 Leverage explainability tools : Utilize various explainability tools designed to elucidate how a machine\nlearning model arrives at its predictions. These tools reveal the features employed by the model for prediction\nand provide insight into the significance of each feature.\n5.3 Validation and Testing\nTo validate the performance of a fine-tuned BioBERT model for healthcare tasks, consider the following actions.\n\u2022 Compare model\u2019s performance with that of other existing biomedical models like BioMegatronShin et al.\n[2020] GatorTron Yang et al. [2022b]and clinical language modelsSinghal et al. [2022]. Use the same\nevaluation metrics and datasets to determine the best-performing model based on these metrics.\n\u2022 Experiment with hyperparameters, recognizing that these settings can significantly influence the model\u2019s\nperformance. Conduct experiments with different hyperparameters to identify the optimal configuration for\nthe specific task.\n\u2022 Validate the model on external healthcare datasets or benchmarks to assess its generalizability and robustness.\nThe model should demonstrate strong performance on previously unseen datasets.\nWhen validating the performance of a fine-tuned BioBERT model for healthcare tasks, also consider the following\nfactors:\n\u2022 The size and quality of the training dataset.\n\u2022 The specific task for which the model is being evaluated.\n\u2022 The choice of evaluation metrics.\n\u2022 The clinical requirements that the model aims to address.\n5.4 Deployment and Integration:\nTo deploy and integrate a fine-tuned BioBERT Lee et al. [2020]model into healthcare applications and systems, take the\nfollowing actions:\n\u2022 Apply regularization techniques to prevent overfitting, a potential issue when training the model on a limited\ndataset. Overfitting occurs when the model captures noise in the data rather than the underlying patterns.\nRegularization discourages the model from learning non-generalizable patterns.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "79dc9d7b-9d55-4d7b-85c8-1533adb594f6": {"__data__": {"id_": "79dc9d7b-9d55-4d7b-85c8-1533adb594f6", "embedding": null, "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580dce2e-e952-45eb-b3c4-7f514020a4a5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95cb7e0d42f1d7efc711cd4b1d6ca715481f09b42d68259b09d645fc20a1f384", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Augment dataset by artificially increasing its size. Employ techniques such as image translation, text generation,\nand synthetic data creation to enhance the dataset. Data augmentation bolsters the model\u2019s performance by\nincreasing its resilience to data noise and variations.\n\u2022 Integrate the model into the application or system, making it accessible for making predictions or recommen-\ndations. Embed the model within the application or system or provide an API for seamless access.\n\u2022 Ensure compliance with relevant healthcare regulations and privacy standards during the model\u2019s deployment.\nThis is crucial for safeguarding patient privacy and promoting responsible model usage. Be aware that\nhealthcare regulations and privacy standards can vary between regions.\nWhile deploying and integrating a fine-tuned BioBERT model into healthcare applications, consider the following:\n\u2022 Evaluate the model\u2019s performance on a held-out dataset to ensure its effectiveness with new data.\n\u2022 Continuously monitor the model\u2019s performance to confirm it meets expectations.\n\u2022 Regularly update the model to account for changes in the data.\n5.5 Continuous Improvement:\nContinuously update and fine-tune the model in response to new healthcare data availability or evolving clinical\nrequirements.\n\u2022 Seek feedback from healthcare professionals, leveraging their expertise in the field for model improvement.\nUse their insights to identify areas where the model underperforms or to uncover new potential applications.\n\u2022 Fine-tune the model using newly acquired healthcare data, applying the same training process employed in the\nmodel\u2019s initial training phase.\n\u2022 Experiment with various hyperparameters to optimize the model\u2019s performance for the specific task.\n\u2022 Apply regularization techniques to prevent overfitting, a concern that may arise when training the model on a\nlimited dataset.\n\u2022 Enhance the model\u2019s robustness by employing data augmentation techniques, making it more resilient to noise\nand data variations.\n\u2022 Continually monitor the model\u2019s performance to ensure it meets expectations. If performance deteriorates,\nconsider fine-tuning or updating it with fresh data.\n5.6 Documentation and Accessibility:\nComprehensively document the fine-tuned BioBERT model, including pre-trained weights and code, and make it\naccessible to the healthcare and research community. Provide comprehensive documentation, code, and model\ncheckpoints in various formats like a technical paper, a blog post, and a GitHub repository. This approach will expand\naccessibility to a broader audience.\n5.7 Ethical Considerations:\nTo ensure that the fine-tuned model addresses ethical concerns related to patient privacy and data security and that it\navoids inadvertently revealing sensitive patient information in compliance with healthcare regulations like HIPAA, the\nfollowing specific ethical considerations should be incorporated when using a fine-tuned BioBERT model:\n\u2022 Respecting Patient Privacy:Users must refrain from utilizing the model to access or disclose sensitive patient\ninformation, including patient names, medical records, and insurance details.\n\u2022 Enhancing Data Security: The model should be safeguarded against unauthorized access and use. This\nentails implementing measures like encryption and access control.\n\u2022 Mitigating Bias: Efforts should be made to prevent bias against any particular group of people. This can be\nachieved by employing a balanced dataset and avoiding discriminatory features.\n\u2022 Ensuring Transparency: The model must be transparent and interpretable. Users should have the capacity to\ncomprehend how the model operates and how it generates its predictions.\n\u2022 Establishing Accountability: Developers and users of the model bear responsibility for its actions. They are\nobligated to ensure the model\u2019s safe and responsible use.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "abab35fd-92b2-437f-8719-1294f2052564": {"__data__": {"id_": "abab35fd-92b2-437f-8719-1294f2052564", "embedding": null, "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60c5a7e4-4606-4469-b240-48651f33e468", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5d337adf38851ad3c2fccbd87056cff9ebb14dec8404353a3c4ff63a8711799a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n6 Discussion\nBased on analyzing the selected works, we realized that LLMs have the potential to revolutionize healthcare. They\ncan introduce novel approaches to enhance clinical decision-making, facilitate information retrieval, and enable more\nnatural language interaction. We have explored the potential benefits and limitations of integrating these language\nmodels into healthcare applications. BioBERT\u2019s primary strength resides in its capacity to comprehend and process\nintricate biomedical and clinical texts. Its pre-training on an extensive corpus of biomedical literature provides it\nwith a robust foundation to accurately interpret medical terminologies, abbreviations, and concepts. Such capability\nproves indispensable in the healthcare context, where specialized language prevails. Moreover, BioBERT can undergo\nfine-tuning for specific applications, encompassing medical entity recognition, text classification, disease prediction,\nand question-answering. This adaptability empowers healthcare professionals to harness the model\u2019s capabilities across\na broad spectrum of clinical and administrative functions.\n6.1 Advantages of using BioBERT for healthcare applications\nBioBERT offers improved clinical decision support, representing one of its most promising applications. Healthcare\nproviders can utilize the model to swiftly access current medical knowledge, research articles, and patient records. This\nempowers them to render more informed decisions regarding diagnosis, treatment, and patient care, enhancing patient\noutcomes. BioBERT significantly enhances information retrieval efficiency from electronic health records (EHRs)\nand other clinical documents. Its ability to process and analyze extensive text data aids healthcare professionals in\npromptly accessing patient-specific information, thereby reducing the risk of overlooking critical data. The model\u2019s\nnatural language processing capabilities make it accessible to healthcare professionals, even those without technical\nbackgrounds. This promotes more effective communication between healthcare providers and technology, enhancing\nuser experience and adoption. However, we must recognize and tackle the challenges linked to deploying BioBERT in\nhealthcare:\n1. Data Privacy and Security: Healthcare data is highly sensitive and falls under stringent privacy regulations.\nTo ensure BioBERT\u2019s compliance with these regulations, such as HIPAA in the United States, it is crucial to\nprevent data breaches and safeguard patient information.\n2. Bias and Fairness: BioBERT, like other language models, can inherit biases present in the training data. This\nbias can lead to disparities in healthcare if not carefully mitigated. Developing techniques to identify and\nrectify bias in healthcare-specific contexts is essential.\n3. Lack of Transparency: Interpreting BioBERT\u2019s decisions can be challenging due to its complex architecture.\nEfforts to make the model more transparent and explainable are necessary to build trust among healthcare\nprofessionals.\n4. Quality Control: Ensuring the quality and accuracy of information generated or retrieved by BioBERT is\nparamount. Erroneous information or recommendations could have serious consequences in clinical settings.\n5. Resource Intensiveness: Developing, fine-tuning, and maintaining BioBERT for healthcare can be resource-\nintensive, requiring substantial computational power, data annotation, and expert oversight.\n6. Generalization Challenges: BioBERT may struggle with generalizing to specific healthcare domains, spe-\ncialties, or rare conditions if not adequately fine-tuned. Customization may be necessary to achieve optimal\nperformance.\nBioBERT holds immense potential for revolutionizing healthcare applications by improving information retrieval,\nclinical decision support, and natural language interaction. However, its deployment in the healthcare sector must\nbe accompanied by stringent measures to address privacy concerns, mitigate bias, ensure transparency, maintain data\nquality, allocate resources effectively, and fine-tune for specific healthcare contexts. With careful consideration and\nresponsible implementation, BioBERT can become a valuable tool for healthcare professionals, enhancing patient care\nand medical research.\n7 Conclusion and Future Work\nIn conclusion, this study offers valuable insights into how the Large Language Models can impact the healthcare sector.\nIt highlights the potential of enhancing various aspects of healthcare, applications such as improving patient care and\nstreamlining healthcare processes. However, challenges such as model performance and ethical considerations remain.\nFuture research shall focus on addressing the existing challenges and further harnessing the capabilities of LLMs by\nextending to the following dimensions.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7f21d530-74ca-466a-a186-ae71b1d17987": {"__data__": {"id_": "7f21d530-74ca-466a-a186-ae71b1d17987", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "214bf48e-035e-4d5d-8f0d-61318c7efd7d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "216b4e368ca2f3db5f00410cb0b4dd98d53b80c0326bdd6f66566fe5e080515a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ae73c34-4e7a-48db-b5ef-52e943f4c6d6", "node_type": "1", "metadata": {}, "hash": "768e3c79056fc65028d6010bd18cef16bda7bb2923e323b922d4f1cb8ca6d689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Improving model performance\n\u2022 Extending NLP for downstream tasks.\n\u2022 Harnessing the capabilities of multimodal LLMs to provide a more comprehensive understanding of patient\nhealth.\n\u2022 Developing cost-effective methods for developing and deploying LLMs.\nReferences\nJiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen\nHu, et al. Prompt engineering for healthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670,\n2023a.\nPM Lavanya and E Sasikala. Deep learning techniques on text classification using natural language processing (nlp) in\nsocial healthcare network: A comprehensive survey. In 2021 3rd international conference on signal processing and\ncommunication (ICPSC), pages 603\u2013609. IEEE, 2021.\nMoreno La Quatra and Luca Cagliero. Transformer-based highlights extraction from scientific papers.Knowledge-Based\nSystems, 252:109382, 2022.\nSandeep Reddy. Evaluating large language models for use in healthcare: A framework for translational value assessment.\nInformatics in Medicine Unlocked, page 101304, 2023.\nSheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis\non medical image using large language models. arXiv preprint arXiv:2302.07257, 2023b.\nArya Rao, John Kim, Meghana Kamineni, Michael Pang, Winston Lie, and Marc D Succi. Evaluating chatgpt as an\nadjunct for radiologic decision-making. medRxiv, pages 2023\u201302, 2023.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on\nllama model using medical domain knowledge. arXiv preprint arXiv:2303.14070, 2023.\nPartha Pratim Ray. Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics,\nlimitations and future scope. Internet of Things and Cyber-Physical Systems, 2023.\nTianyong Hao, Xieling Chen, Guozheng Li, and Jun Yan. A bibliometric analysis of text mining in medical research.\nSoft Computing, 22:7875\u20137892, 2018.\nXieling Chen, Haoran Xie, Gary Cheng, Leonard KM Poon, Mingming Leng, and Fu Lee Wang. Trends and features of\nthe applications of natural language processing techniques for clinical trials text analysis. Applied Sciences, 10(6):\n2157, 2020.\nElena-Alexandra Costea. Machine learning-based natural language processing algorithms and electronic health records\ndata. Linguistic and Philosophical Investigations, (19):93\u201399, 2020.\nDina Demner-Fushman, Wendy W Chapman, and Clement J McDonald. What can natural language processing do for\nclinical decision support? Journal of biomedical informatics, 42(5):760\u2013772, 2009.\nSuzanne Henwood and Jim Lister. NLP and coaching for health care professionals: Developing expert practice. John\nWiley & Sons, 2007.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\nNing Kang, Bharat Singh, Zubair Afzal, Erik M van Mulligen, and Jan A Kors. Using rule-based natural language\nprocessing to improve disease normalization in biomedical text. Journal of the American Medical Informatics\nAssociation, 20(5):876\u2013881, 2013.\nSadid A Hasan and Oladimeji Farri. Clinical natural language processing with deep learning. Data Science for\nHealthcare: Methodologies and Applications, pages 147\u2013171, 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ae73c34-4e7a-48db-b5ef-52e943f4c6d6": {"__data__": {"id_": "8ae73c34-4e7a-48db-b5ef-52e943f4c6d6", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "214bf48e-035e-4d5d-8f0d-61318c7efd7d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "216b4e368ca2f3db5f00410cb0b4dd98d53b80c0326bdd6f66566fe5e080515a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f21d530-74ca-466a-a186-ae71b1d17987", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "eac156bfff7ec407e951dd6f6ed6e82b1daaabd51bfbd1416304f1765ee6463c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1907.11692,\n2019.\nNing Kang, Bharat Singh, Zubair Afzal, Erik M van Mulligen, and Jan A Kors. Using rule-based natural language\nprocessing to improve disease normalization in biomedical text. Journal of the American Medical Informatics\nAssociation, 20(5):876\u2013881, 2013.\nSadid A Hasan and Oladimeji Farri. Clinical natural language processing with deep learning. Data Science for\nHealthcare: Methodologies and Applications, pages 147\u2013171, 2019.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a\npre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240,\n2020.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav\nMani. Biomegatron: Larger biomedical domain language model. arXiv preprint arXiv:2010.06060, 2020.\n11", "mimetype": "text/plain", "start_char_idx": 3025, "end_char_idx": 3923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a739065-5338-4e49-8a8f-8c832b0c56e0": {"__data__": {"id_": "6a739065-5338-4e49-8a8f-8c832b0c56e0", "embedding": null, "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd5b8dd-ca02-4ab2-a77b-c129442a2132", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "56437a353fb32fa534bf4f16d5a17dc890bf625bc13887d7ce034d7de18e0b84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas,\nCheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records. NPJ\nDigital Medicine, 5(1):194, 2022a.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\npreprint arXiv:2212.13138, 2022.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu,\nZhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint\narXiv:2305.15075, 2023.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather\nCole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models.arXiv\npreprint arXiv:2305.09617, 2023.\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas,\nCheryl Martin, Mona G Flores, Ying Zhang, et al. Gatortron: A large clinical language model to unlock patient\ninformation from unstructured electronic health records. arXiv preprint arXiv:2203.03540, 2022b.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d86a42af-29a1-41ed-943b-efe4fae1e6a3": {"__data__": {"id_": "d86a42af-29a1-41ed-943b-efe4fae1e6a3", "embedding": null, "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "078f9666-8d69-44b2-a401-ccb13079a1c9", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5495482e69f49269890b3f2f2da0b5728b0aaa3007bfbeab01c65a4ef9d3c0ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 \n \nReview article / Perspective article \nLLMs-Healthcare : Current Applications and Challenges  of Large Language Models in \nvarious Medical Specialties \nUmmara Mumtaz, Awais Ahmed, Summaya Mumtaz \nAbstract \nWe aim  to present a comprehensive overview of the latest advancements in utilizing Large \nLanguage Models (LLMs) within the healthcare sector, emphasizing their transformative impact \nacross various medical domains.  LLMs have become pivotal in supporting healthcare , including \nphysicians, healthcare providers, and patients. Our review provides insight into the applications of \nLarge Language Models (LLMs) in healthcare, specifically focusing on diagnostic and treatment-\nrelated functionalities. We shed light on how LLMs are applied in cancer care, dermatology, dental \ncare, neurodegenerative disorders, and mental health, highlighting their innovative contributions \nto medical diagnostics and patient care. Throughout our analysis, we explore the challenges and \nopportunities associated with integrating  LLMs in healthcare, recognizing their potential across \nvarious medical specialties despite existing limitations. Additionally, we offer an overview of \nhandling diverse data types within the medical field. \n \nKeywords: Large language models ; Medical Special ties; Cancer; Mental Health;  Healthcare; \nDiagnosis and Treatments; Clinical Notes; Dermatology \n \n1. Introduction \nThe field of artificial intelligence (AI) has undergone a remarkable evolution in recent years, with \nsignificant advancements, particularly noticeable in natural language processing (NLP) and the \ndevelopment of Large Language Models (LLMs). These models represent a paradigm shift in AI's \ncapability to understand, generate, and interact using human language. At their foundation, LLMs \nare complex algorithms trained on vast, text -based documents and datasets [1]. Such extensive \ntraining allows them to recognize patterns adeptly , predict subsequent words in a sentence, and", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ffb443b-66af-4698-895e-a67d879fd52f": {"__data__": {"id_": "1ffb443b-66af-4698-895e-a67d879fd52f", "embedding": null, "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb96d0fd-d6a1-43ff-a421-45a5887c7864", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "dae9220b6c447f815cacbabc7270ed2853a6d054be8592690b3b2c67f9e9a8fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 \n \ngenerate coherent, contextually relevant text for the specified inputs, often called prompts within \nthe NLP community. This ability demonstrates the technical prowess of LLMs and signifies their \npotential to revolutionize how machines understand and process human language. One of the most \nprominent features of LLMs is their proficiency in processing and analyzing large volumes of text \nrapidly and accurately, a capability that far surpasses human potential in speed and efficiency [2]. \nThis quality makes them indispensable in areas requiring the analysis of extensive data sets. They \nare also known as \u201cfew -shot\u201d learners, meaning once trained on massive datasets, they can be \nretrained for new domains utilizing a small number of domain-specific examples[3].  \nLLMs have become increasingly prevalent in the medical domain, demonstrating their versatility  \nand expanding influence. Their applications in healthcare are multifaceted, ranging from \nprocessing vast quantities of medical data  and interpreting clinical notes  to generating \ncomprehensive, human -readable reports [4]. This broad spectrum of functionalities shows how \nLLMs are not just tools for data processing but are also instrumental in providing innovative \nsolutions across various aspects of healthcare. LLMs are increasingly being utilized to tackle \ncritical challenges in patient care. This includes providing customized educational content to \npatients, assisting healthcare professionals in making complex diagnostic decisions, and easing the \nadministrative burdens often associated with healthcare provision[4,5]. \nWhile large language models have been applied across a spectrum of activities in healthcare, \nincluding medical question answering, examination, pure research -oriented tasks, and \nadministrative duties in hospitals, this review will focus exclusively on their practical applications \nin healthcare, such as diagnostics and treatment purposes. We uncover their deployment in critical \nareas such as cancer care, dermatology, dental, and mental health. This exploration is crucial, as it \nshowcases LLMs\u2019 capacity to innovate medical diagnostics and patient care, streamline treatment \ntasks, and address the challenges and opportunities in harnessing their full potential  in complex \nmedical areas . We conduct an in -depth analysis of the applications of LLMs across different \nmedical fields, aiming to present a brief yet thorough summary. We focus on the advancements \nand challenges of integrating these sophisticated models into routine healthcare practices. We offer \ninsights into the current state of progress and identify  barriers to their widespread adoption in \nclinical settings. The paper is structured to cover each medical specialty and associated challenges,", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98b29d88-7334-4e28-aa14-41ef28ed897f": {"__data__": {"id_": "98b29d88-7334-4e28-aa14-41ef28ed897f", "embedding": null, "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7909d06-ff21-486f-ad57-c14a081af40f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5d7de0cc64d9f2c0d8c3ad4fa9aa4bdcdace734dd4ed084da4585ce2a5f70c14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 \n \nfollowed by examining various data types in the medical field. The conclusion summarizes the \nfindings and implications. \n \n \n2. Cancer Care (Oncology)  \nCancer is characterized by the uncontrolled growth of abnormal cells  in the body. It is examined \nwithin oncology\u2014studying cancer types and related factors. Adopting Large Language Models \n(LLMs) such as ChatGPT in oncology has become a focal point of recent research, especially in \nsupporting decision -making processes for cancer treatment. These advanced models are being \nexplored for their capability to enhance diagnos tic accuracy, personalize therapy options, and \nstreamline patient care in oncology. By analyzing vast amounts of data, LLMs can provide insights \nthat potentially improve treatment outcomes and patient management strategies. In the subsequent \ndiscussion, we explore the studies dedicated to integrating LLMs within oncological care , \nencapsulating the innovative efforts to harness LLMs' capabilities in enhancing the diagnostic, \ntreatment, and management processes associated with cancer care. \nIn a study conducted by Vera Sorin and Eyal Klang [6], the capabilities of ChatGPT, a large \nlanguage model (LLM), were explored as a decision -support tool for breast tumor boards. The \nresearch's primary objective was determining how ChatGPT's recommendations align with expert-\nFigure 1 : Visualizing LLM Applications in different medical specialties w.r.t input data type and medical use-case", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "349c72ea-fc34-4ee8-ae7d-023fcdb1cb92": {"__data__": {"id_": "349c72ea-fc34-4ee8-ae7d-023fcdb1cb92", "embedding": null, "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1722240-690a-4587-8efe-7b9e2c036c9c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "aff4282fb1c353b9cafe690e96ea8df0eb5fa8cdf6f8313dea7ff39bd4a2b70f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 \n \ndriven decisions during breast tumor board meetings. For this purpose, clinical data from ten \npatients discussed in a breast tumor board at their institution was inputted into ChatGPT -3.5. \nSubsequently, the model's management recommendations were compared with the final decisions \nmade by the tumor board. Moreover, two senior radiologists independently evaluated ChatGPT's \nresponses, grading them on a scale from 1 (complete disagreement) to 5 (complete agreement) \nacross three categories: summarization of the case, the recommendation provided, and the \nexplanation for that recommendation. Most patients in the study, 80%, had invasive ductal \ncarcinoma, with one case each of ductal carcinoma in -situ and a phyllodes tumor with atypia. \nChatGPT's recommendations aligned with the tumor board's decisions in seven out of the ten cases, \nmarking a 70% concordance. Upon grading, the first reviewer gave mean scores of 3.7, 4.3, and \n4.6 for summarization, recommendation, and explanation , respectively, while the second \nreviewer's scores were 4.3, 4.0, and 4.3 in the same categories. As an initial exploration, the study \nsuggests that LLMs like ChatGPT could potentially be a valuable asset for breast tumor boards. \nHowever, as technology rapidly advances, medical professionals must know its advantages and \npotential limitations. \nIn a study by Stefan Lukac and Davut Dayan  in January 2023, the capabilities of ChatGPT to assist \nin the decision -making process for therapy planning in primary breast cancer cases were \ninvestigated[7]. Though the ChatGPT was able to identify specific risk factors for hereditary breast \ncancer and could discern elderly patients requiring chemotherapy assessment for cost/benefit \nevaluation, it generally offered non -specific recommendations concerning various treatment \nmodalities such as chemotherapy and radiation therapy. Notably, it made errors in patient-specific \ntherapy suggestions, misidentifying patients with Her2 1+ and 2+ (FISH negative) as candidates \nfor trastuzumab therapy and mislabeling endocrine therapy as \"hormonal treatment .\u201d The study \nconcluded that while ChatGPT demonstrates potential utility in clinical medicine, its current \nversion lacks the precision to offer specific therapy recommendations for primary breast cancer \npatients. It underscores the necessity for further refinement before it  can be a reliable adjunct in \nmultidisciplinary tumor board decisions. \nGeorges Gebrael assessed the utility of ChatGPT 4.0 to enhance triage efficiency and accuracy in \nemergency rooms for patients with metastatic prostate cancer [8]. Between May 2022 and April \n2023, clinical data of 147 patients presenting with metastatic prostate cancer were examined, of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da39ba41-fb9c-467a-848a-72db614ca487": {"__data__": {"id_": "da39ba41-fb9c-467a-848a-72db614ca487", "embedding": null, "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d5371ca-8586-44a7-880f-34157b51292a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "788bdcfd9b41c256ca17ea2ddf6da1e6b248a0748ac185f55b8176e52b263cc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 \n \nwhich 56 were selected based on inclusion criteria. ChatGPT demonstrated a high sensitivity of \n95.7% for determining patient admissions but had a low specificity of 18.2% for discharges. It \nagreed with physicians' primary diagnoses in 87.5% of cases. It outperformed physicians regarding \naccurate terminology usage (42.9% vs. 21.4%) and diagnosis comprehensiveness, having a median \ndiagnosis count of 3 compared to physicians' 2. ChatGPT was more concise in its responses but \nprovided more additional treatment r ecommendations than physicians. The data suggests that \nChatGPT could serve as a valuable tool for assisting medical professionals in emergency room \nsettings, potentially enhancing triage efficiency and the overall quality of patient care. \nA study led by Arya Rao et al.  investigated the potential of ChatGPT-3.5 and GPT-4 (OpenAI) in \naiding radiologic decision-making, specifically focusing on breast cancer screening and breast pain \nimaging services [9]. The researchers measured the models' responses against the ACR \nAppropriateness Criteria using two prompt formats: open -ended (OE) and select all that apply \n(SATA). For breast cancer screening, both versions scored an average of 1.830 (out of 2) in the \nOE format, but GPT-4 outperformed ChatGPT-3.5 in the SATA format, achieving 98.4% accuracy \ncompared to 88.9%. Regarding breast pain, GPT -4 again showed superiority, registering an \naverage OE score of 1.666 and 77.7% in SATA, while ChatGPT -3.5 scored 1.125 and 58.3%, \nrespectively. The data suggests the growing viability of large l anguage models like ChatGPT in \nenhancing radiologic decision -making processes, with potential benefits for clinical workflows \nand more efficient radiological services . However, further refinement and broader use cases are \nneeded for full validation. \nHana et al. conducted a retrospective study in February 2023 to evaluate the appropriateness of \nChatGPT's responses to common questions concerning breast cancer prevention and screening[10]. \nLeveraging methodologies from prior research that assessed ChatGPT's capacity to address \ncardiovascular disease-related inquiries, the team formulated 25 questions rooted in the BI-RADS \nAtlas and their clinical experiences within tertiary care breast imaging departments. Each question \nwas posed to ChatGPT three times, and three fellowship -trained breast radiologists critically \nassessed the responses . The radiologists categorized each response as \"appropriate,\" \n\"inappropriate,\" or \"unreliable\" based on the content's clinical relevance and consistency. Their \nevaluations considered two hypothetical scenarios: content for a hospital website and direct \nchatbot-patient interactions. The majority's opinion dictated the final determination of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "540d861a-b73c-43ef-b233-d3c5ebbda741": {"__data__": {"id_": "540d861a-b73c-43ef-b233-d3c5ebbda741", "embedding": null, "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89a6b699-7041-42b6-8056-e20e7bb12875", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "43b3513c646f83338cd442203f8a0172896c719c21087908edabaa0e45e33377", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \n \nappropriateness. Results revealed that ChatGPT provided suitable answers for 88% (22 out of 25) \nof the questions in both contexts. However, one question  pertained to mammography scheduling \nin light of COVID-19 vaccination, which elicited an inappropriate response.  \nAdditionally, there were inconsistencies in answers related to breast cancer prevention and \nscreening location queries. While ChatGPT frequently referenced guidelines from the American \nCancer Society in its responses, it omitted those from the American College of Radiology and the \nUS Preventive Services Task Force. These findings aligned with earlier research by Sarraju et \nal.[11], where 84% of ChatGPT's cardiovascular disease prevention responses were deemed \nappropriate. Despite showing considerable potential as an automated tool for patient education on \nbreast cancer, ChatGPT exhibited certain limitations, emphasizing the essenti al role of physician \noversight and the ongoing need for further refinement and research into large language models in \nhealthcare education. \nBrian Schulte , in 2023, explored the ability of ChatGPT to identify suitable treatments for \nadvanced solid cancers[12]. Through a structured approach, the study assessed ChatGPT's capacity \nto list appropriate systemic therapies for newly diagnosed advanced solid malignancies and then \ncompared the treatments ChatGPT suggested with those recommended by the National \nComprehensive Cancer Network (NCCN) guidelines. This comparison resulted in the valid \ntherapy quotient (VTQ) measure . The research encompassed 51 diagnoses and found that \nChatGPT could identify 91 unique medications related to advanced solid tumors. On average, the \nVTQ was 0.77, suggesting a reasonably high agreement between ChatGPT's suggestions and the \nNCCN guidelines. Furthermore, ChatGPT always mentioned at least one systemic therapy aligned \nwith NCCN's suggestions. However, there was a minimal correlation between the frequency of \neach cancer type and the VTQ. In conclusion, while ChatGPT displays promise in aligning w ith \nestablished oncological guidelines, its current role in assisting medical professionals and patients \nin making treatment decisions still needs to be defined. As the model evolves, it is hoped that its \naccuracy in this area will be enhanced, but continued research is essential to fully understand and \nharness its potential. \nIn a study led by Julien Haemmerli  et al., the capability of ChatGPT  was explored in the context \nof CNS tumor decision-making, specifically for glioma management [13]. Using clinical, surgical, \nimaging, and immunopathological data from ten randomly chosen glioma patients discussed in a", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5907c7ca-640b-4971-86f0-0d2d62955da2": {"__data__": {"id_": "5907c7ca-640b-4971-86f0-0d2d62955da2", "embedding": null, "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e68eec0-45d7-4ca0-880b-8c3ef5d5ab48", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b13f2f9f5551025424c065532362300532fc20d284345535a85e53c39052e5b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 \n \nTumor Board, ChatGPT's recommendations were compared with those of seven CNS tumor \nexperts. While most patients had glioblastomas, findings revealed that ChatGPT's diagnostic \naccuracy was limited, with a notable discrepancy in glioma classifications. However, it \ndemonstrated competence in recommending adjuvant treatments, aligning closely with expert \nopinions. Despite its limitations, ChatGPT shows potential as a supplementary tool in oncological \ndecision-making, particularly in settings with constrained expert resources. \nIn Shan Chen et al.\u2019s research on the effectiveness of ChatGPT in offering cancer treatment advice, \nthe study scrutinized the model's alignment with the National Comprehensive Cancer Network \n(NCCN) guidelines for breast, prostate, and lung cancer treatments [14]. Through four diverse \nprompt templates, the study assessed if the mode of questioning influenced the model's responses. \nWhile ChatGPT's recommendations aligned with NCCN's guidelines in 98% of the prompts, 34.3% \nof these recommendations also presented inf ormation that needed to be more  in sync with the \nNCCN guidelines. The study concluded that, despite its potential, ChatGPT's performance in \nconsistently delivering reliable cancer treatment advice was unsatisfactory. Consequently, patients \nand medical professionals must  exercise caution when relying on ChatGPT and similar tools for \neducational purposes. \n2.1. Challenges associated with LLMs as a decision-support tool in Cancer Care: \nWhile integrating Large Language Models (LLMs) like ChatGPT into oncology shows promise, \nparticularly in decision support for cancer treatment, it also presents several critical challenges, as \ndiscussed in the previous section . These challenges must be addressed to ensure LLMs' safe and \neffective use in high-stakes medical environments. Firstly, the issue of accuracy and precision in \nLLMs is a significant concern. For instance, in Julien Haemmerli's [13] study on glioma therapy, \nChatGPT demonstrated limitations in accurately classifying glioma types. Similarly, the study by \nStefan Lukac and Davut Dayan [7] revealed errors in patient-specific therapy suggestions, such as \nmisidentifying patients for trastuzumab therapy. These inaccuracies highlight the risk of potential \nmisdiagnoses or inappropriate treatment recommendations, which could have profound \nimplications for patient care. \nAnother challenge is the capacity of LLMs to consider the comprehensive clinical picture, \nincluding patient functional status, which is often a nuanced judgment call made by experienced \nphysicians. ChatGPT's moderate performance in this area, as seen in Ha emmerli's study [13],", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e1981ef-40bc-4931-95bc-27c9ecdb3efd": {"__data__": {"id_": "0e1981ef-40bc-4931-95bc-27c9ecdb3efd", "embedding": null, "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e78cdb2-a88a-4550-a0c1-14f85ce32f63", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "342aba6e97818a38e99e0d37bc57733531fcf08891b9213443bfa88cd8482a31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 \n \nindicates a gap between current LLM capabilities and the complex decision -making processes in \nmedical practice. Furthermore, the integration of LLMs into existing medical workflows raises \nconcerns. For example, Georges Gebrael's[8] study on triage in metastatic prostate cancer showed \nthat while ChatGPT had high sensitivity, its low specificity for discharges could lead to operational \ninefficiencies. Integrating LLMs within healthcare systems also poses challenges in data privacy, \ninteroperability, and the need for robust IT infrastructure. \nLastly, the role of LLMs in patient education and communication is not without limitations. Hana \nL Haver et al.[10] studies demonstrated inconsistencies in ChatGPT's responses to breast cancer \nprevention and screening questions . This inconsistency highlights the importance of human \noversight in verifying the information provided by LLMs, ensuring it aligns with established \nmedical guidelines and practices. In summary, while LLMs present exciting opportunities for \nenhancing cancer care, their current limitatio ns in accuracy, comprehensive clinical assessment, \nintegration into existing systems, and patient education necessitate a cautious and critical approach. \nThese models should be viewed as supplementary tools that augment, rath er than replace, the \nexpertise of medical professionals. Continuous evaluation, refinement, and ethical consideration \nare essential to harness the full potential of LLMs in oncology. \n3. Skin Care: Dermatology \nOur skin is a barrier against external threats such as viruses, bacteria, and other harmful organisms. \nDermatology is the branch of medicine dealing with skin diseases.  There has been a surge in cases \nrelated to skin diseases in the past years , affecting people of all ages [15].  Common skin-related \ndiseases include acne, alopecia, bacterial skin infections, decubitus ulcers, fungal skin diseases, \npruritus, and psoriasis[16]. Traditional dermatology diagnosis is based on a visual inspection of skin \nfeatures and subjective evaluation by a dermatologist[17]. The realm of dermatology diagnosis faces \nseveral significant challenges. Firstly, accurately interpreting skin disease imagery is complex due \nto the wide variety of skin conditions and their subtle visual differences. This task requires a high \nlevel of exp ertise, leading to the second challenge: a noticeable shortage of dermatologists, \nespecially in remote or underserved areas. Lastly, creating patient -friendly diagnostic reports is \nanother hurdle. These reports need to be detailed yet understandable to non -specialists, making \ntheir production time-consuming and labor-intensive for dermatologists.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b871956-1ff5-4f57-8cce-e8a3be0c9fbf": {"__data__": {"id_": "1b871956-1ff5-4f57-8cce-e8a3be0c9fbf", "embedding": null, "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b32f56c2-7db7-4166-83b0-8d0c79598ca2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "46962a740c63428f21ee71cd71d916f8905852634d4bdad17018e7165e0d26cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 \n \nIn addressing the above challenges in dermatological diagnostics, Zhou et al. introduced SkinGPT-\n4, an innovative interactive dermatology diagnostic system underpinned by an advanced visual \nLarge Language Model [18]. This study was mainly focused on tackling the prevalent issues in \ndermatology, such as the shortage of specialized medical professionals in remote areas, the \nintricacies involved in interpreting skin disease images accurately, and the demanding nature of \ncreating patient-friendly diagnostic reports. SkinGPT-4, utilizing a refined version of MiniGPT-4, \ntrained on an extensive dataset that included 52,929 images of skin diseases, both from public \ndomains and proprietary sources, along with detailed clinical concepts and doctors' notes. This \ncomprehensive training on skin -related disease images enabled SkinGPT -4 to articulate medical \nfeatures in skin disease images using natural language  and make  precise diagnoses. The \nfunctionality of SkinGPT-4 allows users to upload images of their skin conditions, after which the \nsystem autonomously analyzes these images. It identifies the characteristics and categorizes the \nskin conditions, performs an in -depth analysis, and provides interactive treatment \nrecommendations. A notable aspect of SkinGPT-4 is its local deployment feature, combined with \na solid commitment to maintaining user privacy, making it a viable option for patients seeking \naccurate dermatological assessments. To ascertain the efficacy of SkinGPT-4, the study conducted \na series of quantitative evaluations on 150 real-life dermatological cases. Certified dermatologists \nindependently reviewed these cases to validate the diagnoses provided by SkinGPT-4. Among the \n150 cases, a commendable 78.76% of the diagnoses rendered by SkinGPT -4 were validated as \neither accurate or relevant by the dermatologists, breaking down into 73.13% that firmly aligned \nand another 5.63% that agreed. The outcomes of this evaluation underscored the accuracy of \nSkinGPT-4 in diagnosing skin diseases. While SkinG PT-4 is not positioned as a replacement for \nprofessional medical consultation, its contribution to enhancing patient comprehension of medical \nconditions, improving communication between patients and doctors, expediting dermatologists' \ndiagnostic processes,  and potentially fostering human -centered care and healthcare equity in \nunderdeveloped regions is significant. \n3.2. Challenges associated with utilizing LLMs in Dermatology: \nThe introduction of SkinGPT-4 by Zhou et al. marks a significant advancement in dermatological \ndiagnostics, addressing challenges like the dermatologist shortage and the complexities of skin \ndisease image interpretation and patient -friendly report generation [18]. Despite its innovative", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03fe14a5-e7c7-4ad0-8b63-7995f7257c41": {"__data__": {"id_": "03fe14a5-e7c7-4ad0-8b63-7995f7257c41", "embedding": null, "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a3ba3cc-f88c-4265-9514-ce5c189ed928", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "790a45c0dbfee8755e0c23544c350e2103ef7fce1c033a4172ef8706b241b84f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 \n \napproach and the training on an extensive dataset to articulate medical features in skin images, \nthere are inherent challenges. Some challenges associated with deploying SkinGPT-4 include \nensuring consistent diagnostic accuracy across various skin conditions, safeguarding patient \nprivacy while managing sensitive health data, and integrating the technology seamlessly into \nexisting healthcare systems. Additionally, despite SkinGPT -4's high diagnostic accuracy, \ncontinuous human oversight in medic al diagnosis a nd treatment planning remains critical to \ncomplement the AI's capabilities with professional medical judgment and ensure optimal patient \ncare outcomes. Additionally, advancements might focus on developing models that can adapt to \nnew, emerging skin conditions and leveraging telemedicine to extend dermatological care to \nremote areas, thus promoting healthcare equity. \n \n4. Neurodegenerative Disorders: Dementia & Alzheimer's  \nNeurodegenerative disorders involve the gradual deterioration of specific neuron groups, differing \nfrom the non -progressive neuron loss seen in metabolic or toxic conditions. These diseases are \ncategorized by their primary symptoms (such as dementia, parkinsonism, or motor neuron disease), \nthe location of neurodegeneration within the brain (including frontotemporal degenerations, \nextrapyramidal disorders, or spinocerebellar degenerations), or by the underlying molecular \nabnormalities[19]. Dementia is a broad category of brain diseases that cause a long -term and often \ngradual decrease in the ability to think and remember, affecting daily functioning. Alzheimer's \ndisease (AD) is the most common cause of dementia, characterized by memory loss, language \nproblems, and unpredictable behavior. \nLLMs such as Google Bard and ChatGPT have emerged as valuable tools for predicting \nneurodegenerative disorders. A study by Koga et al. evaluated these models' predictive accuracy \nusing cases from Mayo Clinic conferences [20]. Using the Mayo Clinic brain clinicopathological \nconferences as their sample pool, the researchers extracted 25 cases of neurodegenerative disorders. \nThese clinical summaries were then utilized for training and  testing the models. The diagnoses \noffered by each model were compared against the official diagnosis provided by medical \nprofessionals. Findings from the study highlighted that  ChatGPT-3.5 aligned with 32% of all the \nphysician-made diagnoses, Google Bard with 40%, and ChatGPT -4 with 52%. When assessing \nthe accuracy of these diagnostic predictions, ChatGPT -3.5 and Google Bard both achieved a", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2611, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "89852993-9801-434b-8b2b-c49e34af79af": {"__data__": {"id_": "89852993-9801-434b-8b2b-c49e34af79af", "embedding": null, "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "814a2403-6dfc-4673-9d31-b232a98267b4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0eac3227e5c1e41f8db631838e581653da7e98abec7c6b5367815dea4d7c4ad2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 \n \ncommendable score of 76%, while ChatGPT -4 led the pack with an impressive accuracy rate of \n84%. The evident proficiency exhibited by LLMs, specifically ChatGPT and Google Bard, \nhighlights their considerable potential in revolutionizing diagnostic processes in \nneurodegenerative disorders. \nThis study conducted by Agbavor and Liagn (2022)  explored the use of GPT -3-generated text \nembeddings to predict dementia, utilizing data from the ADReSSo Challenge  (Alzheimer\u2019s \nDementia Recognition through Spontaneous Speech  only challenge[22]), which focuses on \nidentifying cognitive impairment through spontaneous speech [21]. The author proposed using the \nmodel to identify individuals with dementia against healthy individuals as controls. Using the 237 \nspeech recordings  derived from the ADReSSO  (Alzheimer\u2019s Dementia Recognition through \nSpontaneous Speech only challenge), the author used a 70/30 split and obtained 71 data s amples \nas the testing set and 166 as the training set. In the training set, 87 individuals had AD, and 79 \nwere healthy controls. GPT-3 was innovatively used for embedding the transcribed speech texts. \nThen, the model extracts the acoustic features such as temporal analysis (periodicity of speech, \npause rate, phonation rate, etc.) and speech production (vocal quality, articulation, prosody, etc.). \nThese features serve as the input for the classification model used in AD prediction . GPT-3 \nembeddings are then compared with BERT and traditional acoustic features. The findings reveal \nthat text embeddings outperform traditional acoustic methods and compare well with fine -tuned \nmodels such as BERT. This suggests that GPT-3's text embeddings offer a promising approach for \nearly dementia diagnosis. \nAnother study conducted by Mao and colleagues [23] outlines developing and applying  a deep \nlearning framework utilizing the BERT model for predicting the progression from Mild Cognitive \nImpairment (MCI) to Alzheimer's Disease (AD) using unstructured EHR notes.  The study  \ncataloged 3,657 MCI -diagnosed patients and their clinical notes from  Northwestern Medicine \nEnterprise Data Warehouse (NMEDW) between 2000 and 2020, using only their initial MCI \ndiagnosis notes for analysis. These notes underwent de-identification, cleaning, and segmentation \nbefore training an AD -specific BERT model (AD -BERT). AD-BERT transformed patient note \nsections into vector forms, which a fully connected network analyzed to predict MCI -to-AD \nprogression. For validation, a similar methodology was applied to 2,563 MCI patients from  Weill", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9531aa2-aeac-4360-aee8-d53f280b9b9e": {"__data__": {"id_": "a9531aa2-aeac-4360-aee8-d53f280b9b9e", "embedding": null, "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dd2e20e-d2c0-463d-a5ff-9d9ab53370db", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "72bc7609f560eda7304e94ae9bd8b88b8e62b19bec15717aadbd811cc1f33210", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 \n \nCornell Medicine (WCM) . AD -BERT outperformed seven baseline models, showing superior \naccuracy in both patient groups, evidenced by its AUC and F1 scores.  \nIn the diagnosis of complex conditions like Alzheimer's disease, medical professionals use a \nvariety of data such as images, patient demographics, genetic profiles, medication history, \ncognitive assessments, and speech data. Some of the recent studies have proposed multi -modal \nAD diagnosis or prediction methods lever aging the popular pre -trained large language model \n(LLM) to add text data sources, in addition to images and other data types[24,25-26].  \n4.1. Challenges associated with LLMs in Neurodegenerative disorders \nUtilizing LLMs in diagnosing and managing neurodegenerative disorders like dementia and \nAlzheimer's disease presents several challenges. Firstly, the complexity and variability of these \nconditions require highly accurate and deep understanding, which LLMs may not always provide \ndue to limitations in their training data. The ethical and privacy concerns about handling sensitive \npatient data pose significant hurdles. Furthermore, integrating these models into clinical workflows \ndemands substantial validation to ensure they  complement, rather than complicate, healthcare \nprofessionals' decision -making processes. Lastly, there's a need for continuous updates and \nimprovements in these models to keep pace with the latest medical research and clinical practices \n5. Dentistry  \nThe World Health Organization reports that oral diseases impact approximately 3.5 billion \nindividuals globally, with dental caries, periodontal diseases, and tooth loss being the most \nprevalent. These conditions, largely preventable and manageable with ear ly diagnosis, have seen \nthe application of AI methodologies in recent years, including the diagnosis of dental caries [27, 28]  \nand periodontitis[29]. Despite this, exploring Large Language Models (LLMs) in dentistry remains \nnotably scarce, with limited studies demonstrating their practical application.  \nHuang et al. stand out by proposing LLM -based deployment strategies within dentistry, marking \nthe emerging area of research with significant potential for advancement [29]. To showcase the \neffectiveness and potential of applying Large Language Models (LLMs) in dentistry, this work \nintroduced a framework for an automated diagnostic system utilizing Multi -Modal LLMs. This \ninnovative system incorporate d three distinct input modules: visual, auditory, and textual data, \nenabling comprehensive analysis. Visual inputs, such as dental X-rays and CT scans, are evaluated", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3f4f7563-31ca-42c3-95c5-6d132d346934": {"__data__": {"id_": "3f4f7563-31ca-42c3-95c5-6d132d346934", "embedding": null, "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d93fe54-1262-42f9-9dc3-af81f426b019", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ad682fdd4e78762c4ea6d06fdad20d2498e1b03064f33e0bd3b1a109ab8407c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 \n \nfor anomalies using vision -language models, facilitating precise diagnostics. Audio inputs serve \ndual purposes: detecting voice anomalies and understanding patient narratives, which are \nconverted to text for further analysis  by LLM. To illustrate the capabilities of the multi -modal \nLLM AI system in dental practice, Huang et al. proposed its application in diagnosing and planning \ntreatment for dental caries. The process begins with inputting a tooth's X-ray into the system, where \nvision-language modeling is employed to detect any decay on the tooth. Once identified, the \nsystem utilizes LLM to propose a comprehensive treatment plan, articulated through seven detailed \nsteps. These steps range from initial patient communication to scheduling follow-up appointments, \nhighlighting a thorough approach to patient care. Despite its advanced diagnostics, the system's \nlimitations, such as failing to detect potential bone loss, are acknowledged, suggestin g areas for \nfurther research and development to enhance its effectiveness in dental diagnostics. \n5.1.  Challenges associated with dental care:  \nThe accuracy of LLMs like ChatGPT depends on the availability of high -quality, relevant dental \ndata. A significant hurdle in designing and training LLMs for dental care is limited access to the \ndental records owned by private dental clinics and concerns over patient privacy, which restricts \naccess to comprehensive and current datasets. LLMs\u2019 development and effectiveness in dentistry \nmust navigate these challenges, ensuring access to extensive, up -to-date information while \naddressing privacy and ownership issues to avoid biases and maintain data integrity. \nThe potential of LLMs in dental healthcare seems promising and can revolutionize how dental \nprofessionals diagnose, treat, and manage patient care today. LLMs could significantly improve \ndiagnostic precision by leveraging the vast amounts of data available in patient records and \nimaging, allowing for early detection and intervention in dental conditions. Furthermore, the \nability of LLMs to generate personalized treatment plans and educational materials tailored to \nindividual patient needs could enhance the effectiveness of patient care. This personalization and \nthe model\u2019s ability to process and analyze data swiftly  could lead to more efficient and patient -\ncentered dental healthcare practices. As LLMs continue to evolve, their integration into dental \nhealthcare is expected to deepen, offering innovative solutions to longstanding challenges and \nimproving patient outcomes worldwide. \n6. Mental Health: Psychiatry and Psychology", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa96261a-2441-4f9a-ba76-18d0604d177c": {"__data__": {"id_": "fa96261a-2441-4f9a-ba76-18d0604d177c", "embedding": null, "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c924199-a612-4a3f-a1f6-8165449807ae", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5dec5bad92d3ed5c9057413d101a7bb4107e7f00d86859fdb8a6698fcb8424f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 \n \nMental health disorders, which affect millions globally, significantly reduce the life quality of \nindividuals and their families. In psychiatry, LLMs have the potential to refine diagnostic precision, \noptimize treatment outcomes, and enable more tailored patient care, moving beyond traditional, \nsubjective diagnostic approaches prone to inaccuracies. By leveraging AI to analyze extensive \npatient data, it's possible to uncover patterns not easily detectable by humans, thereby improving \ndiagnosis[28,29]. \nGalatzer-Levy and colleagues, in 2023, delved into exploring the potential role of large language \nmodels (LLM) in psychiatry [30]. Their primary investigation tool  was Med -PALM 2, an LLM \nequipped with comprehensive medical knowledge. The model was trained and tested using a blend \nof clinical narratives and patient interview transcripts. The dataset encompassed expert evaluations \nusing instruments like the 8 -item Patient Health Questionnaire and the PTSD Checklist -Civilian \nVersion (PCL -C). The study  intended to gauge the severity of PTSD using the PCL -C while \nemploying the PHQ -8 to assess depression and anxiety levels. The evaluation process involved \nextracting from Med -PALM 2 clinical scores, the rationale for such scores, and the model's \nconfidence in its derived results. The gold standard for this evaluation was the DSM 5 (Diagnostic \nand Statistical Manual of Mental Disorders, Fifth Edition). The researchers' rigorous testing \nprocess involved the analysis of 46 clinical case studies, 115 PTSD evaluations, and 145 \ndepression instances. These were probed using prompts to tease out diagnosti c information and \nclinical scores. The rigorous assessment also saw Med -PaLM 2 fine-tuned through many natural \nlanguage applications and a substantial textual database. Notably , research -quality clinical \ninterview transcripts were employed as inputs when assessing the model's efficacy. Med-PaLM 2 \ndemonstrated its prowess in evaluating psychiatric states across various psychiatric conditions. \nRemarkably, when tasked with predicting psychiatric risk from clinician and patient narratives, \nthe model showcased an impressive accuracy rate ranging between 80% and 84%. \nAnother study  evaluated the performance of various LLMs, including Alpaca and its variants, \nFLAN-T5, GPT-3.5, and GPT -4, across different mental health prediction tasks such as mental \nstate (depressed, stressed or risk actions like suicide) using online text [31]. Through extensive \nexperimentation, including zero-shot, few-shot, and instruction fine-tuning methods, it was found \nthat instruction fine -tuning notably enhances LLMs' effectiveness across all tasks. Notably, the", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc27b76d-beba-403a-9021-7a1b79b1948f": {"__data__": {"id_": "dc27b76d-beba-403a-9021-7a1b79b1948f", "embedding": null, "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e3770db-8a36-478c-a0e0-620b9c3a2bc1", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a0d2dd47bac8d9acf0415a6b2e00380b03a9deadb7ed64f125058e3fb78410d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 \n \nfine-tuned models, Mental -Alpaca and Mental -FLAN-T5, demonstrated superior performance \nover larger models like GPT-3.5 and GPT-4 and matched the accuracy of task-specific models.  \nThe use of conversational agents based on LLMs for mental well-being support is growing, yet the \neffects of such applications still need to be  fully understood. A qualitative study by Ma et al. of \n120 Reddit posts and 2,917 comments from a subreddit dedicated to mental health support apps \nlike Replika reveals mixed outcomes [32]. While Replika offers accessible, unbiased support that \ncan enhance confidence and self -exploration, it struggles with content moderation, consistent \ninteractions, memory retention, and preventing user dependency, potentially increasing social \nisolation. \nFollowing the advancements with ChatGPT, research into automated therapy using AI's latest \ntechnologies is gaining momentum. This new direction aims to shift mental health assessments \nfrom traditional rating scales to a more natural, language -based communication. The emergence \nof large language models, like those powering ChatGPT and BERT, marks a significant shift in \nartificial intelligence, potentially revolutionizing standardized psychological assessments. This \nevidence points towards AI's capacity to transform mental health evaluations into interactions that \nmirror natural human communication, pending comprehensive validation in specific application \nscenarios[33]. \n6.1. Challenges associated with applications of LLMs for Mental Health \nIn mental health applications, LLMs face challenges like ensuring content sensitivity and safety to \navoid harmful advice, maintaining accuracy and reliability to prevent misdiagnoses, and offering \npersonalized, empathetic responses for adequate support. Data privacy and security are paramount \ndue to the personal nature of discussions. There's also a need to prevent user over -reliance on \nLLMs, potentially deterring professional help. Ethical considerations include the impact of \nreplacing human interactions w ith AI and avoiding biases. Additionally, navigating regulatory \ncompliance within mental health laws and guidelines is crucial for lawful operation. \n8 Other Medical Specialties: Nephrology, Gastroenterology, Allergy and immunology \nThe integration of Large Language Models into medical specialties like nephrology and \ngastroenterology remains in the early stages, with their full potential yet to be realized. Current \napplications in these areas are sparse, highlighting opportunities for future exploration and", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5c0bcc2-2964-45a4-9e51-a56592124c02": {"__data__": {"id_": "f5c0bcc2-2964-45a4-9e51-a56592124c02", "embedding": null, "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26d91884-20ca-4606-8c54-4e246a86ff03", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "72e8efd29878ea65f7f15f81c0a2cb1e2c6c578d685b5bbfc685b614fc2ff9ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 \n \nimplementation. This brief overview aims to shed light on the existing implementations of LLMs \nwithin these specific fields, indicating the nascent but promising role of advanced AI technologies \nin enhancing diagnostic and treatment methodologies in nephrology and gastroenterology. \n8.1.Nephrology \nWithin the domain of nephrology, LLMs are being utilized to assist in diagnosing kidney diseases, \nproviding treatment guidance, and monitoring  renal function, as noted by Wu and colleagues[34]. \nThese LLMs facilitate the evaluation of crucial data such as laboratory results, clinical data, and a \npatient's medical history during the diagnostic phase. As such, the LLMs chosen for nephrological \napplications are often preferred to possess a sophisti cated medical knowledge capability, \nespecially in  multiple-choice medicine test -taking. Various LLMs, including Orca Mini 13B, \nStable Vicuna 13B, Falcon 7B, Koala 7B, Claude 2, and GPT-4, have found applications in treating \nand diagnosing kidney diseases. However, owing to their unique zero-shot reasoning capabilities, \nGPT-4 and Claude 2 are particularly suitable for this intricate medical specialty. Currently, these \nmodels are employed to respond to multiple -choice questions about nephrology. Wu et al.  \nincorporated questions from clinical backgrounds linked to 858 nephSAP multiple-choice queries \ncollated between 2016 and 2023. When evaluating the proficiency of Claude 2 and GPT -4, \nperformance was gauged based on the proportion of correctly answered neph rology-related \nnephSAP multiple-choice questions. GPT-4 demonstrated superior performance, garnering a score \nof 73.3%, in contrast to Claude 2, which achieved a score of 54.4%. When individual nephrology \ntopics were examined, GPT -4 consistently outperforme d its counterparts, including Claude 2, \nVuna, Kaola, Orca-mini, and Falcon. \n8.2. Gastroenterology \nLahat et al. explored the capabilities of large language models, specifically OpenAI's ChatGPT, in \nresponding to queries within the realm of gastrointestinal health [35]. Their evaluation employed \n110 real -world questions, benchmarking ChatGPT's responses against the expert consensus of \nseasoned gastroenterologists. These queries spanned a spectrum of topics, from diagnostic tests \nand prevalent symptoms to treatments for a range of gastrointestinal issues. The source of these \nquestions was public internet platforms. The researchers evaluated the outputs of ChatGPT on \nmetrics such as accuracy, clarity, up-to-dateness, and efficacy, rating them on a scale from 1 to 5. \nThese outputs were then categorized into symptoms, diagnostic tests, and treatments. ChatGPT", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68284171-9e3c-457c-b5f3-1306147d2a47": {"__data__": {"id_": "68284171-9e3c-457c-b5f3-1306147d2a47", "embedding": null, "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ea65f56-0a8f-4a7d-9cbe-c4bcb1777595", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ab96e175e78b42866ba38791643d7185892b7a2392c0b2ed246d3b0286355607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17 \n \naveraged scores of 3.7 for clarity, 3.4 for accuracy, and 3.2 for efficacy in the symptom category . \nDiagnostic test-related queries resulted in scores of 3.7 for clarity, 3.7 for accuracy, and 3.5 for \nefficacy. As for treatment-related questions, the model achieved 3.9 for clarity, 3.9 for accuracy, \nand 3.3 for efficacy. The results indicated the subst antial potential of ChatGPT in providing \nvaluable insights within the gastrointestinal specialty. \n8.3. Allergy and immunology \nIn allergy and immunology, LLMs akin to their applications in dermatology, have shown \npromising potential. According to a study by Goktas et al., LLMs, specifically models like GPT-4 \nand Google Med -PaLM2, significantly enhance  the diagnostic process within allergy and \nimmunology disciplines[36]. These advanced models elevate the precision of diagnosis and  can \ntailor treatment plans to suit individual patient needs. Beyond the clinical realm, they also play a \npivotal role in fostering patient engagement, ensuring patients are actively involved and informed \nin their healthcare journey. As a result, the integration of LLMs in allergy and immunology \nrepresents a paradigm shift towards more accurate, personalized, and patient-centric medical care. \nSection 9: Handling different types of data in the medical industry \nThis section provides  an overview of how different data formats and types are handled in the \nmedical industry when used as training data or inputs for a large language model. \n9.1. Clinical Notes  \nClinical notes, an integral component of patient health records, have increasingly been utilized in \nmedicine as input to large language models (LLMs). These notes, typically generated by healthcare \nprofessionals, serve as rich patient information repositories, including  their medical history, \npresent symptoms, diagnoses, treatments, and more. Clinical notes are fed into LLMs to extract \nmeaningful patterns, predictions, and insights. Before using these notes, they are often \npreprocessed to ensure they are in  a format that's easily digestible for the models. This \npreprocessing can involve converting handwritten notes into digital formats, anonymizing patient \ndata to maintain privacy, and structuring the data in a consistent format. LLMs can directly process \nthese notes and produce a range of tools suited for activities like condensing medical data, assisting \nin clinical decisions, and creating medical reports [37]. To utilize clinical notes in LLMs, prompts \ncontaining questions, scenarios, or comments about the note are used, such as \"Assume the role of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d782f8fc-1f6a-4b1a-8887-82ebea70c671": {"__data__": {"id_": "d782f8fc-1f6a-4b1a-8887-82ebea70c671", "embedding": null, "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc16011d-df63-4c64-956b-cf60dfe6a02c", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "de60a238cc299b5808d8c677e9aa58145c1025bbf07cf6345cff9a2351d519b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 \n \na neurologist at the Mayo Clinic brain bank clinicopathological conference.\" Based on this, the \nmodel provides an output that aids in evaluation or diagnosis across different medical fields[37]. \n9.2.X-rays/ Images  \n  X-rays are medical imaging that utilizes ionizing radiation to produce images of internal body \norgans. This data type may include CT scans (tomography), chest X -rays, and bone X-rays. In \nmedicine, X-ray images can be processed by a computer -aided detection (CAD) model, which is \npre-trained to derive the outputs in tensor form. These tensors are then translated into natural \nlanguage, where they can be used as LLM input to generate  summaries or descriptions of the X -\nray images. Wang et al. illustrated how the X-rays of exam images are handled for utilizing them \nwith the LLMs[38]. They established that the model is fed into pre -trained CAD models to derive \nthe output. Then, translate the tensor (output) into natural language. Lastly, the language models \nare used to make conclusions and summarize the results. They establish that X-ray images can be \nused as input in the LLM and fed into the model with prompts to generate the image summarization \nor descriptive caption. The LLM supports visual question answering, where the x -ray images of \nthe patients are fed into an image encoder (BLIP -2), where the natural language presentation is \ngenerated and embedded based on the image understanding. \nBazi and colleagues proposed a transformer encoder-decoder architecture to handle the visual data \nwhen using the LLM [39]. They extracted the image features using the vision transformer (ViT) \nmodel and then used the textual encoder transformer to embed the questions. It is then fed to the \nresulting textual and visual representations into a multi-modal decoder to generate the answers. To \ndemonstrate how LLM handles the visual data, they used VQA datasets for radiology images, \ntermed PathVQA and VQA-RAD. In decoding the radiology images, the proposed model achieved \n72.97% and 8.99% for the VQA-RAD and 62.37% or 83.86% for PathVQA. \n9.3.Radiological reports \nRadiological reports are documents from radiologists that present the findings or interpretation of \nmedical imaging studies such as MRIs, X -rays, and CT scans. These data are processed as texts \nwithin the report to be input for LLMs in medicine. After data augmentation, the radiological \nreports are used as inputs in the LLM model. Tan and colleagues collected 10,602 CT scan reports \nfrom patients with cancer at a single facility[40]. These reports were categorized into four response", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eda27de0-e840-42b9-8ec7-5f122017c53e": {"__data__": {"id_": "eda27de0-e840-42b9-8ec7-5f122017c53e", "embedding": null, "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2c1e965-199a-45a8-bb6d-6ab683cb8acc", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "213e64d73b0244e05ead4c53f07d699e1b295b0636859482be4623f87aa83926", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19 \n \ntypes: no evidence of disease, partial response, stable disease, or progressive disease. To analyze \nthese reports, we utilized various models , including transformer models, a bidirectional LSTM \nmodel, a CNN model, and traditional machine learning approaches. Techniques such as data \naugmentation through sentence shuffling with consistency loss and prompt-based fine-tuning were \napplied to enhance the performance of the most effective models. \n9.4.Speech data \nSpeech data, encompassing medical interviews, consultations, and patient audio interactions, \nserves as a valuable reservoir of information. Before its use in Large Language Models (LLMs), \nthis data is converted into a textual format through automatic speec h recognition (ASR) systems. \nNotably, converting audio data into text is accomplished using pre-trained models, with Wav2vec \n2.0 emerging as a leading contender in speech recognition technology. In their groundbreaking \nwork, Agbavor and Liang [21] employed the Wav2vec2 -base-960 base model, an advanced tool \nfine-tuned on an extensive 960 -hour dataset of 16 kHz speech audio. Their methodology \nincorporated Librosa for audio file loading and Wav2Vec2Tokenizer for the crucial task of \nwaveform audio token ization. These tokenized audio segments are inputted into the \nWav2Vec2ForCTC model depending on memory capacities . This model decodes the tokens, \nresulting in the generation of text transcripts. Furthermore, an alternative approach to leveraging \nspeech data in LLMs involves using open MILE, an open -source toolkit. Open MILE  offers \nfunctionalities like speech classification and facilitates extracting audio features from speech or \nmusical signals, proving its versatility in handling audio data for various applications.  \n9.5.Tabular Data \nIn the medical domain, tabular data typically encompasses clinical measurements, patient records, \nand lab outcomes, arranged methodically in a matrix of rows and columns. A transformation via \ntabular modeling is requisite for this structured data to be effectively utilized by Large Language \nModels (LLMs). The ubiquity of this tabular format in clinical and physician databases has often \nled to the use of tree-based models like bagging and boosting. However, these models come with \ntheir share of limitations. Highlighting an innovative approach to this challenge, Chen et al.  \npresented a study employing a data set of 1479 patients undergoing immune checkpoint blockade \n(ICB) treatments for various cancer types[41]. Segmenting the dataset, with 295 patients for testing \nand 1184 for training, they unveiled how LLMs process tabular data. Crucial to this process is", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e5e2d97-bbad-4179-af0a-fb1ec121817c": {"__data__": {"id_": "6e5e2d97-bbad-4179-af0a-fb1ec121817c", "embedding": null, "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6503fd34-ae7b-4d44-a271-95e6cec6d187", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "050d26c1ecdac9e850d959dd32fdc14dc1dec115325d990456a10e32afa17162", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 \n \nserializing the feature columns into coherent sequences of natural language tokens that the LLM \ncan interpret. This serialization can be achieved through various methods, be it the prompting -\nbased regeneration approach, using {attribute} is {value} functions , or manual serialization \ntemplates.  \nFurthermore, Chen and his team introduced an advanced tabular model, ClinTaT, augmented from \nits original design. This refined model incorporates a continuous embedding layer harmonized \nwith multiple distinct layers that mirror the table's continuous feature count. Continuous variables \nare melded with embedded categorical data for the final processing step, which is then channeled \ninto the transformer for analysis. \n10. Conclusion  \nLarge Language Models (LLMs) applications have carved out a transformative niche in the \nhealthcare sector. From patient engagement and education to diagnostic assistance, administrative \nsupport, and medical research, the multifaceted applications of LLMs have demonstrated their \npotential to optimize  various facets of the medical landscape. Their expansive knowledge \nrepositories and adeptness at understanding context and generating human -like textual responses \nhave positioned LLMs as invaluable assets within the healthcare domain. Their integration with \nchatbots offers a more personalized and efficient patient experience, aiding in tasks ranging from \nmedication clarification to mental health support. On the diagnostic front, incorporating LLMs \nwith electronic health systems and medical imaging promises to enhance the accuracy and \nefficiency of diagnosis and treatment plans. LLMs' capability to assist in clinical documentation, \nmedical language translation, and medical education for patients highlights their adaptability and \nrelevance in varied healthcare scenarios. \nHowever, while the benefits of LLMs are numerous, their practical application in the healthcare \nsector also underscores the importance of precision, context awareness, and ethical considerations, \ngiven the critical nature of medical decision-making. While LLMs like ChatGPT and Med-PaLM \nhave shown significant potential, there's an imperative for ongoing refinement, especially when \nhandling complex or rare medic al cases. As LLMs become more integrated into patient care, \nresearch addressing the ethical implica tions, including data privacy, the balance between \nautomation and human intervention, and informed patient consent, will be paramount. \nCollaborative research exploring the fusion of LLMs with other emerging technologies, such as", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6aa0204d-04e6-4332-8fd0-44edb6a7d135": {"__data__": {"id_": "6aa0204d-04e6-4332-8fd0-44edb6a7d135", "embedding": null, "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e02d8246-f470-4045-9f61-b6ada5eb6f78", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4d1332087a3725e251065f178d2da5976450a56572de598982e8e823f73baecf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21 \n \naugmented reality or wearable health devices, can open new avenues for patient care and remote \nmonitoring. Enhancing the LLMs' contextual understanding is crucial. Future work should focus \non the model's ability to consider a patient's medical history and present conditions before offering \nrecommendations. In sum, the horizon of LLMs in healthcare is expansive and promising. As we \ncontinue to witness the convergence of technology and medicine, the collaboration of \nmultidisciplinary teams\u2014combining expertise from AI, medicine, ethics, and other domains\u2014will \nbe integral to harnessing the full potential of LLMs in healthcare. \nReferences \n1. Min B, Ross H, Sulem E, et al., 2023, Recent advances in natural language processing via \nlarge pre-trained language models: A survey. ACM Computing Surveys, 56: 1-40. \n2. Wei J, Tay Y, Bommasani R, et al., 2022, Emergent abilities of large language  models. \narXiv preprint arXiv:2206.07682. \n3. Brown T, Mann  B, Ryder  N, et al. , 2020,  Language models are few -shot \nlearners. Advances in Neural Information Processing Systems, 33: 1877-1901. \n4. Thirunavukarasu AJ, Ting  DSJ, Elangovan  K, et al. , 2023, Large language  models in \nmedicine. Nature medicine, 29: 1930-1940. \n5. Cascella M, Montomoli  J, Bellini  V, Bignami E., 2023, Evaluating the feasibility  of \nChatGPT in healthcare: an analysis of multiple clinical and research scenarios . Journal of \nMedical Systems, 47: 33. \n6. Sorin V, Klang E, Sklair-Levy, et al.,  2023,  Large language model (ChatGPT) as a support \ntool for breast tumor board. NPJ Breast Cancer, 9:44. https://doi.org/10.1038/s41523-023-\n00557-8. \n7. Lukac S, Dayan D, Fink V, et al. , 2023, Evaluating ChatGPT as an adjunct for the  \nmultidisciplinary tumor board decision -making in primary breast cancer cases. Arch \nGynecol Obstet, 308:1831-1844. doi: 10.1007/s00404-023-07130-5 \n8. Gebrael G, Sahu KK, Chigarira B, et al., 2023, Enhancing Triage Efficiency and Accuracy \nin Emergency Rooms for Patients with Metastatic Prostate Cancer: A Retrospective  \nAnalysis of Artificial Intelligence -Assisted Triage Using ChatGPT 4.0. Cancers (Basel), \n5:3717. doi: 10.3390/cancers15143717.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "efe62d02-f379-4502-9789-83173321422c": {"__data__": {"id_": "efe62d02-f379-4502-9789-83173321422c", "embedding": null, "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c47c95e-b57d-4ca6-ac29-aa5cdf666e43", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d0ea1db3f1bcbf38d4766a562f1ea5a02175c847e1b7710fb6e26da2b5ab9917", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 \n \n9. Arya Rao, John Kim, Meghana Kamineni et al.,  2023, Evaluating GPT as an Adjunct for \nRadiologic Decision Making: GPT -4 Versus GPT-3.5 in a Breast Imaging Pilot . Journal \nof the American College of Radiology, 20. https://doi.org/10.1016/j.jacr.2023.05.003. \n10. Haver HL, Ambinder EB, Bahl M, et al. , 2023, Appropriateness of Breast Cancer  \nPrevention and Screening Recommendations Provided by ChatGPT. Radiology, 307. doi: \n10.1148/radiol.230424. \n11. Sarraju A, Bruemmer D, Van Iterson E, et al., 2023, Appropriateness of Cardiovascular \nDisease Prevention Recommendations Obtained From a Popular Online Chat -Based \nArtificial Intelligence Model. JAMA,329:842-844. doi: 10.1001/jama.2023.1044.  \n12. Schulte B , 2023, Capacity of ChatGPT to Identify Guideline -Based Treatments for \nAdvanced Solid Tumors. Cureus, 15:e37938. doi: 10.7759/cureus.37938.  \n13. Haemmerli J, Sveikata L, Nouri A, et al. , 2023, ChatGPT in glioma adjuvant therapy  \ndecision making: ready to assume the role of a doctor in the tumour board? BMJ Health \nCare Inform., 30. doi: 10.1136/bmjhci-2023-100775.  \n14. Chen S, Kann BH, Foote MB, et al. , 2023, Use of Artificial Intelligence Chatbots for \nCancer Treatment Information. JAMA Oncol. , 9:1459\u20131462. \ndoi:10.1001/jamaoncol.2023.2954 \n15. Yakupu A, Aimaier R, Yuan, B, et al., 2023, The burden of skin and subcutaneous diseases: \nfindings from the global burden of disease study 2019. Front Public Health, 11:1145513. \ndoi: 10.3389/fpubh.2023.1145513.  \n16. Urban K, Chu S, Giesey RL, et al ., 2020, Burden of skin disease and associated  \nsocioeconomic status in Asia: a cross-sectional analysis from the Global Burden of Disease \nStudy 1990-2017. JAAD Int., 2:40\u201350. 10.1016/j.jdin.2020.10.006 \n17. Burlando M, Muracchioli A, Cozzani E, et al., 2021, Biologic Therapy: Case Report and \nNarrative Review. Case Rep. Dermatol., 13, 372\u2013378. \n18. Zhou J, He X, Sun L, et al., 2023, SkinGPT-4: An Interactive Dermatology  Diagnostic \nSystem with Visual Large Language Model. Electrical Engineering and Systems Science, \n1-12. https://arxiv.org/abs/2304.10691 \n19. Dugger, BN., Dickson, DW, 2017, Pathology of Neurodegenerative Disease. Cold Spring \nHarb Perspect Biol., 9:a028035. doi: 10.1101/cshperspect.a028035.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "33f3ede6-5df6-417d-abd6-ecb79d3e27a6": {"__data__": {"id_": "33f3ede6-5df6-417d-abd6-ecb79d3e27a6", "embedding": null, "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4d6db63-83d1-4fb2-9c94-5d7fe7bcedeb", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "491b8358875708cbaa635a428f5c18898a99be4faf63e14c1dd838e7869d5179", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "23 \n \n20. Koga S, Martin NB, Dickson DW., 2023, Evaluating the performance of large  language \nmodels: ChatGPT and Google bard in generating differential diagnoses in  \nclinicopathological conferences of neurodegenerative disorders. Brain Pathology , \nhttps://doi.org/10.1111/bpa.13207 \n21. Agbavor F, Liang H., 2022, Predicting dementia from spontaneous speech using  large \nlanguage models. PLOS Digital Health , 1(12), e0000168.  \nhttps://doi.org/10.1371/journal.pdig.0000168 \n22. Luz S, Haider F, de la Fuente S, et al., 2021, Detecting cognitive decline using speech only: \nThe ADReSSo Challenge. ArXiv Prepr ArXiv210409356. \n23. Mao C, Xu J, Rasmussen L, et al., 2023, AD-BERT: Using pre-trained language model to \npredict the progression from mild cognitive impairment to Alzheimer\u2019s disease. Journal of \nBiomedical Informatics, 144, 104442. \n24. Cai H, Huang X, Liu Z, et al., 2023, Exploring Multimodal Approaches for  Alzheimer\u2019s \nDisease Detection Using Patient Speech Transcript and Audio Data.  arXiv preprint \narXiv:2307.02514. \n25. Feng Y, Wang J, Gu X, et al., 2023, Large language models improve Alzheimer\u2019s disease \ndiagnosis using multi-modality data. arXiv preprint arXiv:2305.19280. \n26. Ying Y, Yang  T, Zhou H, 2023, Multimodal fusion for alzheimer\u2019s disease  \nrecognition. Applied Intelligence, 53: 16029-16040. \n27. Mohammad-Rahimi H, Motamedian SR, Rohban MH , et al. , 2022,  Deep learning for \ncaries detection: A systematic review. J Dent , 122:104115. doi: \n10.1016/j.jdent.2022.104115. Epub 2022 Mar 30. PMID: 35367318. \n28. Urban R, et al. , 2023, AI-assisted CBCT data management in modern dental practice: \nbenefits, limitations and innovations. Electronics 12, 1710. \n29. Huang H, Zheng O, Wang D, et al., 2023, ChatGPT for shaping the future of dentistry: The \npotential of multi -modal large language model. International Journal of Oral Science , \n15(1). https://doi.org/10.1038/s41368-023-00239-y \n30. Galatzer-Levy IR, McDuff DN, Karthikesalingam A, Malgaroli M, 2023, The Capability \nof Large Language Models to Measure Psychiatric Functioning.  Computation and \nLanguage, 1-15. https://arxiv.org/abs/2308.01834", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55b3c148-de24-4729-969a-9b82d4afd343": {"__data__": {"id_": "55b3c148-de24-4729-969a-9b82d4afd343", "embedding": null, "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c0d2782-15c6-4d1f-b7f3-2e486a589ec7", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4458e6447c5ddbbe7fff18fa1b6f6cb717897f1ff5d85bcfb1c32ffd97bc2134", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 \n \n31. Xu X, Yao B, Dong Y, et al., 2023, Leveraging large language models for mental  health \nprediction via online text data. arXiv preprint arXiv:2307.14385. \n32. Ma Z, Mei Y, Su Z. , 2024, Understanding the Benefits and Challenges of Using Large  \nLanguage Model -based Conversational Agents for Mental Well -being Support . AMIA  \nAnnu Symp Proc. 11;2023:1105-1114. \n33. Kjell, O., Kjell, K., &amp; Schwartz, H. A. , 2023, AI-based large language models are \nready to transform psychological health assessment. \n34. Wu S, Koo  M, Blum , et al. , 2023, A comparative study of open -source large  language \nmodels, GPT -4 and Claude 2: Multiple -choice test taking in nephrology.  arXiv.org. \nhttps://arxiv.org/abs/2308.04709 \n35. Lahat A, Shachar E, Avidan B, et al., 2023,  Evaluating the utility of a large language  \nmodel in answering common patients\u2019 gastrointestinal health -related questions: Are we  \nthere yet? Diagnostics, 13:1950. https://doi.org/10.3390/diagnostics13111950 \n36. Goktas P, Karakaya G, Kalyoncu, et al., 2023, Artificial intelligence chatbots in  allergy \nand immunology practice: Where have we been and where are we going? The  Journal of \nAllergy and Clinical Immunology: In Practice , 11 :2697-2700. \nhttps://doi.org/10.1016/j.jaip.2023.05.042 \n37. Singhal K, Azizi S, Tu T, et al., 2023, Large language models encode clinical knowledge. \nhttps://www.nature.com/articles/s41586-023-06291-2. \n38. Wang S, Zhao  Z., Ouyang, X., et al. , 2023, ChatCAD: Interactive Computer -Aided \nDiagnosis on Medical Image using Large Language Models. Computer Science , 1 -11. \nhttps://arxiv.org/abs/2302.07257 \n39. Bazi Y, Rahhal  MM, Bashmal  L, Zuair M, 2023, Vision\u2013language model for  visual \nquestion answering in medical imagery. Bioengineering, 10(3), 380.  \nhttps://doi.org/10.3390/bioengineering10030380 \n40. Tan RS, Lin Q, Low GH, et al., 2023, Inferring cancer disease response from  radiology \nreports using large language models with data augmentation and prompting. Journal of the \nAmerican Medical Informatics Association, 1-8. https://doi.org/10.1093/jamia/ocad133. \n41. Chen, Z., Balan, M. M., &amp; Brown, K. , 2023, Language models are few-shot learners \nfor prognostic prediction. arXiv.org. https://arxiv.org/abs/2302.12692", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "302f76de-3c44-475e-8a9e-822d1e210eb2": {"__data__": {"id_": "302f76de-3c44-475e-8a9e-822d1e210eb2", "embedding": null, "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfcf7971-6e71-4a2b-aa91-fa46305049b6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "785184cf540be1efb11445e4bca35ff27293635ed1f395dbe7024a01be1db79d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LARGE LANGUAGE MODELS IN HEALTHCARE AND MEDICAL\nDOMAIN : A R EVIEW\nZabir Al Nazi\nUniversity of California, Riverside\nRiverside, CA\nznazi002@ucr.edu\nWei Peng\nStanford University\nPalo Alto, CA\nwepeng@stanford.edu\nABSTRACT\nThe deployment of large language models (LLMs) within the healthcare sector has sparked both\nenthusiasm and apprehension. These models exhibit the remarkable capability to provide profi-\ncient responses to free-text queries, demonstrating a nuanced understanding of professional medical\nknowledge. This comprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development, starting from traditional\nPretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we\nexplore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare appli-\ncations, particularly focusing on clinical language understanding tasks. These tasks encompass a wide\nspectrum, ranging from named entity recognition and relation extraction to natural language inference,\nmulti-modal medical applications, document classification, and question-answering. Additionally, we\nconduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain,\nwhile also assessing the utilization of various open-source LLMs and highlighting their significance\nin healthcare applications. Furthermore, we present the essential performance metrics employed\nto evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large language models\nin the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings.\nThis review provides a comprehensive exploration of the current landscape of LLMs in healthcare,\naddressing their role in transforming medical applications and the areas that warrant further research\nand development.\nKeywords Large Language Model \u00b7 Healthcare \u00b7 Medicine \u00b7 Natural Language Generation \u00b7 Natural Language\nProcessing \u00b7 Machine Learning Applications \u00b7 ChatGPT \u00b7 Generative AI \u00b7 Medical AI\n1 Introduction\nDeep Learning provides an intelligent way to understand human behaviors, emotions and human healthcare [1, 2, 3, 4].\nRecent developments in clinical language understanding have ushered in the potential for a paradigm shift in the\nhealthcare sector. These advancements hold the promise of ushering in a new era characterized by the deployment\nof intelligent systems designed to bolster decision-making, expedite diagnostic processes, and elevate the quality of\npatient care. In essence, these systems have the capacity to serve as indispensable aids to healthcare professionals as\nthey grapple with the ever-expanding body of medical knowledge, decipher intricate patient records, and formulate\nhighly tailored treatment plans. This transformative potential has ignited considerable enthusiasm within the healthcare\ncommunity [5, 6, 7].\nThe immense value of lage language models (LLMs) lies in their ability to process and synthesize colossal volumes of\nmedical literature, patient records, and the ever-expanding body of clinical research. Healthcare data [8, 9] is inherently\ncomplex, heterogeneous, and often overwhelming in scale. LLMs act as a powerful force multiplier, aiding healthcare\nprofessionals struggling with information overload. By automating the analysis of medical texts, extracting crucial\ninsights, and applying that knowledge, LLMs are poised to drive groundbreaking research and enhance patient care,\nsignificantly improving and contributing to the progression of the healthcare and medical domain.\narXiv:2401.06775v2  [cs.CL]  8 Jul 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0ba989b9-230f-47b7-ad8c-095b25a9f3c8": {"__data__": {"id_": "0ba989b9-230f-47b7-ad8c-095b25a9f3c8", "embedding": null, "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d8fca3-4372-4dd2-ae91-4d18833a262e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "224d659c81211c845989094cddd60b9bc18a1f7821cfd7aded32a177fa2958df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3980cd5b-4d4e-4e2e-99ec-d59472cf180c", "node_type": "1", "metadata": {}, "hash": "20b712836b90a0a07eda6f071b3ebbe2942421a80a753330c90d29e641e10c0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nNotably, this surge of enthusiasm is attributable, in part, to the exceptional performance of state-of-the-art large\nlanguage models (LLMs) such as OpenAI\u2019s GPT-3.5, GPT-4 [10, 11], and Google\u2019s Bard. These models have exhibited\nremarkable proficiency in a wide spectrum of natural language understanding tasks, highlighting their pivotal role\nin healthcare. Their ability to comprehend and generate human-like text is poised to play a transformative role in\nhealthcare practices, where effective communication and information processing are of paramount importance [12].\nThe trajectory of natural language processing (NLP) has been characterized by a series of noteworthy milestones,\nwith each development building upon the strengths and limitations of its predecessors. In its nascent stages, recurrent\nneural networks (RNNs) laid the foundation for contextual information retention in NLP tasks. However, their inherent\nlimitations in capturing long-range dependencies became evident, thus necessitating a shift in the NLP paradigm.\nThe pivotal moment in NLP\u2019s evolution came with the introduction of Transformers, a groundbreaking architecture\nthat addressed the challenge of capturing distant word relationships effectively. This innovation was a turning point,\nenabling more advanced NLP models. These advancements provided the impetus for the emergence of sophisticated\nlanguage models like Llama 2 [13] and GPT-4, which, underpinned by extensive training data, have elevated NLP to a\nlevel of understanding and text generation that closely approximates human-like language.\nWithin the healthcare domain, tailored adaptations of models like BERT, including BioBERT and ClinicalBERT [14, 15],\nwere introduced to tackle the intricacies of clinical language. The introduction of these models addressed the unique\nchallenges posed by medical text, which frequently features complex medical terminology, lexical ambiguity, and\nvariable usage. However, introducing LLMs into the highly sensitive and regulated domain of healthcare demands\ncareful consideration of ethics, privacy, and security. Patient data must be rigorously protected, while ensuring that\nLLMs don\u2019t perpetuate existing biases or lead to unintended harm. Nevertheless, the potential for LLMs to enhance\nhealthcare practices, better patient outcomes, and spearhead innovative research avenues continues to stimulate ongoing\ninvestigation and growth in this rapidly evolving field.\nAs we navigate this dynamic field, our review aims to function as a comprehensive guide, offering insights to medical\nresearchers and healthcare professionals seeking to optimize their research endeavors and clinical practices. We seek\nto provide a valuable resource for the judicious selection of LLMs tailored to specific clinical requirements. Our\nexamination encompasses a detailed exploration of LLMs within the healthcare domain, elucidating their underlying\ntechnology, diverse healthcare applications, and facilitating discussions on critical topics such as fairness, bias mitigation,\nprivacy, transparency, and ethical considerations. By highlighting these critical aspects, this review aims to illustrate the\nimportance of integrating LLMs into healthcare in a manner that is not only effective but also ethical, fair, and equitable,\nultimately fostering benefits for both patients and healthcare providers.\nThis review paper is organized into distinct sections that systematically address the integration, impact, and limitations\nof large language models (LLMs) in healthcare:\n\u2022 Section 2 provides a foundational understanding of LLMs, covering their key architectures such as Transform-\ners, foundational models, and multi-modal capabilities.\n\u2022 In section 3, the focus shifts to the application of LLMs in healthcare, discussing their use cases and the\nmetrics for assessing their performance within clinical settings.\n\u2022 Section 4 critically examines the challenges associated with LLMs in healthcare, including issues related to\nexplainability, security, bias, and ethical considerations.\n\u2022 The paper concludes by summarizing the findings, highlighting the transformative potential of LLMs while\nacknowledging the need for careful implementation to navigate their limitations and ethical implications.\n2 Review of Large Language Models\nLarge language models have emerged as a notable advancement in the field of natural language processing (NLP)\nand have attracted considerable interest in recent times [ 16, 10]. These models exhibit notable attributes such as\ntheir considerable number of parameters, pre-training on vast collections of textual data, and fine-tuning for specific\ndownstream objectives [17, 18, 13]. By leveraging these key characteristics, large language models demonstrate\nexceptional performance across a wide range of NLP tasks. This section presents a comprehensive discussion of the\nconcept, architecture, and pioneering examples of large language models. Furthermore, we explore the pre-training\nmethodology and the significance of transfer learning in facilitating these models to achieve exceptional performance\nacross diverse tasks [19].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3980cd5b-4d4e-4e2e-99ec-d59472cf180c": {"__data__": {"id_": "3980cd5b-4d4e-4e2e-99ec-d59472cf180c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61d8fca3-4372-4dd2-ae91-4d18833a262e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "224d659c81211c845989094cddd60b9bc18a1f7821cfd7aded32a177fa2958df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ba989b9-230f-47b7-ad8c-095b25a9f3c8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0e97444cdb841fedbd88cef3b6a787e2916e7a3e457fa771abc0ab45c2ea5d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These models exhibit notable attributes such as\ntheir considerable number of parameters, pre-training on vast collections of textual data, and fine-tuning for specific\ndownstream objectives [17, 18, 13]. By leveraging these key characteristics, large language models demonstrate\nexceptional performance across a wide range of NLP tasks. This section presents a comprehensive discussion of the\nconcept, architecture, and pioneering examples of large language models. Furthermore, we explore the pre-training\nmethodology and the significance of transfer learning in facilitating these models to achieve exceptional performance\nacross diverse tasks [19].\nLarge Language models, built upon the Transformer architecture, have been specifically engineered to enhance the\nefficiency of natural language data processing in comparison to earlier iterations. The Transformer architecture, as\nproposed by [20], utilizes a self-attention mechanism to capture the contextual relationships between words in a sentence.\n2", "mimetype": "text/plain", "start_char_idx": 4542, "end_char_idx": 5548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "29b8c972-e312-466c-aedc-55f224d6377b": {"__data__": {"id_": "29b8c972-e312-466c-aedc-55f224d6377b", "embedding": null, "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fc0fac2-b8d3-44b6-a807-f45a4bf151e6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f6f2d2c760526e9fa5c9da5bc2eec306adc9c85bdef37998709a3977b0f192a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5334ad4f-35f8-4668-a4cd-9a7588c99ffc", "node_type": "1", "metadata": {}, "hash": "b705047d5afd43e217665c9a5365deb60a38299a88e6224268d969bb6a7295d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nThis mechanism facilitates the model\u2019s ability to assign varying degrees of significance to distinct words during the\nprediction process, rendering it especially suitable for handling long-range dependencies in language.\nThe key aspects of large language models encompass their substantial magnitude [ 21, 22], pre-training on vast text\ncorpora [23, 13], and subsequent fine-tuning tailored towards specific tasks [24]. These models possess a substantial\nnumber of parameters, ranging from hundreds of millions to billions, which allows them to effectively capture intricate\npatterns and nuances within language. Pre-training is commonly conducted on diverse datasets devoid of task-specific\nannotations, enabling the model to acquire knowledge from a broad spectrum of linguistic instances and develop a\ncomprehensive grasp of language. Following pre-training, the model undergoes a further fine-tuning process using\nsmaller datasets that are appropriate to the task at hand. This allows the model to successfully adapt to and perform\nwell on specific natural language processing (NLP) tasks.\nThe progression of natural language processing (NLP) has been characterized by a series of significant advancements.\nAt the outset, recurrent neural networks (RNNs) facilitated the retention of context in natural language processing\n(NLP) tasks. Nevertheless, recurrent neural networks (RNNs) were found to have several shortcomings when it comes\nto effectively capturing long-range dependencies. The advent of Transformers has had a transformative impact by\neffectively addressing the challenge of capturing distant word relationships. Subsequently, large language models like\nLlama 2 [13], GPT-4 [11] emerged, powered by extensive training data, significantly advancing NLP capabilities in\nunderstanding and generating human-like text. This progression signifies a continuous cycle of innovation, with each\nstage building upon the strengths and limitations of its predecessor. In the subsequent section, we delineate significant\nphases of development within the continuum of progress in the landscape of natural language processing (NLP).\nIn the domain of healthcare, specialized adaptations of BERT, namely BioBERT [14] and ClinicalBERT [15], were\nintroduced to address a variety of challenges in comprehending clinical language. GPT-3 (Generative Pre-trained\nTransformer 3), developed by OpenAI, is one of the largest language models to date, with 175 billion parameters [10].\nRecently, OpenAI introduced the GPT-3.5 and its successor, GPT-4 (OpenAI, 2023) [11], alongside Google AI\u2019s Bard,\nboth of which have emerged as cutting-edge Large Language Models (LLMs), displaying remarkable capabilities across\ndiverse applications, including healthcare and medicine [6].\n2.1 Transformers\nThe Transformers architecture, introduced in \"Attention is All You Need,\" [20] has revolutionized natural language\nprocessing. The primary novelty of this model is its utilization of the self-attention mechanism, which allows for the\nassessment of the importance of input tokens by considering their relevance to the given task. In this setup, multiple\nattention heads work in parallel, allowing the model to focus on various aspects of the input whereas positional encoding\nconveys relative token positions. Given an input sequenceX of length N, the self-attention mechanism [25, 26, 27]\ncomputes attention scores A(i, j) between all token pairs (i, j). Three learned matrices, Query (Q), Key (K), and\nValue (V ), are obtained by linear projections of X.\nAttention(Q, K, V) =softmax( QKT\n\u221adk\n)V\nHere, dk represents the dimension of key vectors. The softmax function normalizes scores. The output for each token is\nthen computed as a weighted sum of value vectors for all tokens j. Multi-Head Attention extends this mechanism by\ncomputing multiple attention sets in parallel, concatenated and linearly transformed to form the final output.\nTransformers consist of stacked encoder-decoder blocks, adapting to diverse tasks. The training occurs via unsupervised\nor semi-supervised learning on vast text corpora, using gradient-based optimization. Transformers have become\nfoundational in natural language processing due to their capacity to handle sequential data, capture long-range\ndependencies, and adapt to various tasks with minimal fine-tuning. They extend beyond text, finding applications in\nhealthcare, recommendation systems, image generation, and other domains.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5334ad4f-35f8-4668-a4cd-9a7588c99ffc": {"__data__": {"id_": "5334ad4f-35f8-4668-a4cd-9a7588c99ffc", "embedding": null, "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fc0fac2-b8d3-44b6-a807-f45a4bf151e6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f6f2d2c760526e9fa5c9da5bc2eec306adc9c85bdef37998709a3977b0f192a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29b8c972-e312-466c-aedc-55f224d6377b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4b940c2421ce678939cd66fbdd5c7849bbe8fdc1a43198d6caf9b38377db8fac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Transformers consist of stacked encoder-decoder blocks, adapting to diverse tasks. The training occurs via unsupervised\nor semi-supervised learning on vast text corpora, using gradient-based optimization. Transformers have become\nfoundational in natural language processing due to their capacity to handle sequential data, capture long-range\ndependencies, and adapt to various tasks with minimal fine-tuning. They extend beyond text, finding applications in\nhealthcare, recommendation systems, image generation, and other domains.\n2.2 Large Foundational Models\nThe advent of Large Foundational Models, exemplified by GPT-3 (Brown et al., 2020) [ 10] and Stable Diffusion\n(Rombach et al., 2022) [28], ushers in a transformative era in the field of machine learning and generative artificial\nintelligence. Researchers have introduced the term \"foundation model\" to delineate machine learning models that\nundergo training on extensive, diverse, and unlabeled datasets, endowing them with the ability to adeptly tackle a broad\nspectrum of general tasks. These encompass tasks related to language comprehension, text and image generation, and\nnatural language dialogue.\n3", "mimetype": "text/plain", "start_char_idx": 3987, "end_char_idx": 5153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e26f2687-c7a5-42cd-b3fa-938e7e694189": {"__data__": {"id_": "e26f2687-c7a5-42cd-b3fa-938e7e694189", "embedding": null, "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db02c5ee-cc26-4afa-9bf3-303b6d5a813f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cafa242508d2bc9c0408b86cd6c1f7c4d8068e2e31a524ff78ab88e7f9cc3bdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 1: Scale of Medical Language Models: A Size Comparison\nLarge foundational models are massive AI architectures trained on extensive quantities of unlabeled data, predominantly\nemploying self-supervised learning methods. This approach to training yields models of exceptional versatility, enabling\nthem to excel across a wide spectrum of tasks, ranging from image classification and natural language processing to\nquestion-answering, consistently delivering outstanding levels of accuracy.\nThese models particularly shine in tasks demanding generative capabilities and human interaction, including the creation\nof marketing content or intricate artwork based on minimal prompts. Nevertheless, adapting and integrating these\nmodels into enterprise applications may present specific challenges [29].\n2.3 Multi-modal Language Models\nA Multi-Modal Large Language Model (MLLM) represents a groundbreaking advancement in the fields of artificial\nintelligence (AI) and natural language processing (NLP). In contrast to conventional language models focused solely on\ntextual data, MLLMs possess the unique ability to process and generate content across multiple modalities, including\ntext, images, audio, and video. This novel approach significantly expands the capabilities of AI applications, allowing\nmachines not only to comprehend and generate text but also to interpret and integrate information from various sensory\ninputs. The integration of multiple modalities enables MLLMs to bridge the gap between human communication and\nmachine understanding, making them versatile tools with the potential to transform diverse fields. This theoretical\nintroduction highlights the transformative potential of MLLMs and their central role in pushing the boundaries of\nartificial intelligence, affecting areas such as image and speech recognition, content generation, and interactive AI\napplications [30].\nMulti-modal large language models (MLLMs) are designed to process and integrate information from multiple data\nsources, such as text, images, and audio, to perform a variety of tasks. These models leverage deep learning techniques\nto understand and generate content across different modalities, enhancing their applicability in real-world scenarios. For\ninstance, Visual ChatGPT combines text and visual inputs to address complex queries [31], while systems like BLIP-2\nutilize a Qformer to integrate visual features with textual data for enhanced image-text interactions [32]. MLLMs are\nparticularly effective in tasks like visual question answering (VQA), where they can interpret and respond to queries\nbased on visual content. The integration of modalities allows these models to offer more comprehensive responses and\nhandle a broader range of interactions than single-modality models. The iterative training processes, often involving\nstages of freezing certain components while fine-tuning others, enable these models to maintain robust language\ncapabilities while adapting to new modalities and tasks.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e629bc8e-da09-43f8-b064-8e59531dc214": {"__data__": {"id_": "e629bc8e-da09-43f8-b064-8e59531dc214", "embedding": null, "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9345d441-1a96-472d-9f8d-694c31af0438", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "41ca496f9ecd7bd33f3f4670592c76f86bbfc47fdf0f7344ef36a2e2eba24c39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 2: Schematic Representation of a Standard Multimodal Large Language Model (MLLM) Architecture\nFigure 2 displays a typical MLLM architecture, comprising an encoder EM , a connector C, and a Large Language\nModel (LLM). Additionally, a generator G can be integrated with the LLM to produce outputs beyond text, such\nas other modalities. The encoder processes inputs like images, audio, or videos into features, which the connector\nrefines to enhance the LLM\u2019s comprehension capabilities. Connectors in these systems come in three main varieties:\nprojection-based, query-based, and fusion-based. The first two types utilize token-level fusion, converting features into\ntokens that are combined with text tokens, whereas the fusion-based connector performs a feature-level fusion directly\nwithin the LLM [30].\nRecently, the integration of the Mixture of Experts (MoE) architecture into multi-modal large language models (MLLMs)\nhas significantly advanced their capabilities. This approach employs multiple specialized sub-models, each fine-tuned\nfor specific types of data or tasks such as image recognition or language processing. By selectively activating the\nmost relevant experts based on the input and task, MoE allows MLLMs to dynamically adapt to the demands of\nmultimodal data integration. This enhances the precision of the model in handling complex multimodal interactions and\noptimizes computational resources. Models like MoV A [33] and MoE-LLaV A [34] leverage MoE strategies effectively,\nimproving performance while maintaining manageable computational costs during both training and inference phases.\nThe adaptability and efficiency of MoE within MLLMs thus contribute significantly to their scalability and efficacy in\nreal-world applications across varied tasks and data types [35].\n3 Large Language Models in Healthcare and Medical Domain\nLanguage models have become a revolutionary force in the constantly changing world of healthcare and medicine,\nrevolutionising how medical researchers and practitioners engage with data, patients, and huge corpus of medical\nknowledge [36]. The use of language models in the medical field has undergone a significant metamorphosis, from\nthe early days of simple rule-based systems, feature extraction, and keyword matching to the arrival of cutting-edge\ntechnologies like Transformers, and Large Language Models (LLMs) such as GPT-v4 [11]. These language models\nhave overcome the constraints of conventional methods, enabling more complex natural language generation and\ninterpretation.\nSeveral pioneering large language models have significantly influenced the landscape of NLP. The emergence of the\nTransformer architecture [20] marked a significant milestone in the realm of natural language processing, leading to the\nemergence of expansive pre-trained language models like the BERT [37] and RoBERTa [38].\nBERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. (2018) [37], revolution-\nized NLP by pre-training a deep bidirectional model on a large corpus and outperforming previous models on various\ntasks. RoBERTa (A Robustly Optimized BERT Pretraining Approach) by Liu et al. (2019) [ 38] demonstrated that\nfurther pre-training improvements and optimization could significantly enhance the performance of BERT.\nIn this section, we will first talk about the current large language models specifically for medical applications, in\nsection 3.1. Then, in section 3.2 we will talk about the use cases of various LLMs that designed mainly for patients,\nexperts, and medical materials.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ba8adce-8f30-4121-8787-7069020dd9d1": {"__data__": {"id_": "2ba8adce-8f30-4121-8787-7069020dd9d1", "embedding": null, "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f71088fc-52c6-480e-80a3-15d093574264", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cc8bd0e15d713fd4fd09e4ccf93d28d2f9f7409707500079a397d99ec614073a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 1: Summary of Large Language Models in the Healthcare Space\nMethod Year Task Institution Source\nCode\nBioMistral [39] 2024 Medical Question Answer-\ning\nAvignon Universit\u00e9,\nNantes Universit\u00e9\nmodel\nMed-PaLM 2\n[40]\n2023 Medical Question Answer-\ning\nGoogle Research, Deep-\nMind\nRadiology-\nLlama2 [41]\n2023 Radiology University of Georgia\nDeID-GPT [42] 2023 De-identification University of Georgia code\nMed-HALT\n[43]\n2023 Hallucination test Saama AI Research code\nChatCAD [44] 2023 Computer-aided diagnosis ShanghaiTech University code\nBioGPT [45] 2023 Classification, relation ex-\ntraction, question answer-\ning, etc.\nMicrosoft Research code\nGatorTron [46] 2022 Semantic textual similarity,\nnatural language inference,\nand medical question an-\nswering\nUniversity of Florida code\nBioMedLM 2022 Biomedical question an-\nswering\nStanford CRFM, Mo-\nsaicML\ncode\nBioBART [47] 2022 Dialogue, summarization,\nentity linking, and NER\nTsinghua University, In-\nternational Digital Econ-\nomy Academy\ncode\nClinicalT5 [48] 2022 Classification, NER University of Oregon,\nBaidu Research\nmodel\nKeBioLM [49] 2021 Biomedical pre-training,\nNER, and relation extrac-\ntion\nTsinghua University, Al-\nibaba Group\ncode\nCRNN [50] 2017 Relation classification Indian Institute of Tech-\nnology\ncode\nLSTM RNN\n[51]\n2017 Named entity recognition Wuhan University code\n3.1 Large Language Models for Medical and Healthcare Applications\nFigure 1 provides a comprehensive overview of the progression in biomedical language model (LM) development\nfrom 2019 to 2023, emphasizing a logarithmic growth in model complexity and parameter count. It describes the\nevolutionary trajectories of various domain-specific adaptations of prominent models such as BioBERT, and GPT-2,\nalong with the inception of more advanced systems like MedPaLM. The sizes of the illustrated models are proportional\nto their parameter volumes, showcasing a consistent trend towards larger, more capable models. This is culminated\nin the emergence of Large Language Models (LLMs) by 2023, which signifies a pivotal shift towards architectures\nwith substantially heightened computational requirements and potential performance in biomedical text analysis and\ngeneration tasks.\nOn the other hand, table 1 provides an insightful overview of leading large language models within the healthcare domain.\nRecently, \"BioMistral\" was published as a a collection of open-source pre-trained large language models for medical\ndomains. In 2023, \"Med-PaLM 2\" and \"Radiology-Llama2\" emerged as key players, addressing medical question\nanswering and radiology tasks, respectively. The \"DeID-GPT\" model extends its capabilities to de-identification, while\n\"Med-HALT\" specializes in hallucination testing. Simultaneously, \"ChatCAD\" offers valuable support in the realm of\ncomputer-aided diagnosis. \"BioGPT\" showcases versatility by handling classification, relation extraction, and question\nanswering. \"GatorTron\" excels in semantic textual similarity and medical question answering, whereas \"BioMedLM\"\nnarrows its focus to biomedical question answering. \"BioBART\" demonstrates prowess in dialogue, summarization,\nentity linking, and NER. \"ClinicalT5\" tackles classification and NER, while \"KeBioLM\" specializes in biomedical\npre-training, NER, and relation extraction. Before the advent of language models or transformers, convolutional and\nrecurrent neural networks represented the state of the art in the field. These models collectively represent remarkable\nstrides in healthcare NLP, providing accessible source code or models for further exploration and practical application.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e885adc5-dd7a-4633-a5e5-455b09c26c0b": {"__data__": {"id_": "e885adc5-dd7a-4633-a5e5-455b09c26c0b", "embedding": null, "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "080b97e4-1712-490e-a0ab-552a8bdef02b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bcb68eb3b2bb30202ab272f03d6a2321b19f35cbab9612539a1ff98bb8d8f9c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 3: Applications of Large Language Models in Healthcare\n3.2 Use Cases of Large Language Models in Healthcare\nIn recent years, the emergence of large language models has catalyzed a transformative shift in the healthcare landscape,\noffering unprecedented opportunities for innovation and advancement. The capability of comprehending and generating\ntext that resembles that of humans has demonstrated remarkable potential across a wide range of healthcare applications\n[52]. The applications of large language models in the healthcare sector are experiencing rapid growth. These models\nare being utilized for clinical decision support, medical record analysis, patient engagement, health information\ndissemination, etc. Their implementation holds the prospect to improve diagnostic accuracy, streamline administrative\nprocedures, and ultimately enhance the efficiency, personalization, and comprehensiveness of healthcare delivery. This\nsection delves into a comprehensive exploration of the multifaceted applications of large language models in healthcare,\nshedding light on their profound implications these applications bear on the trajectory of medical practices and the\neventual outcomes experienced by patients.\n\u2022 Medical Diagnosis:Certain clinical procedures may depend on the use of data analysis, clinical research, and\nrecommendations [53, 54]. LLMs may potentially contribute to medical diagnosis by conducting analyses\non patient symptoms, medical records, and pertinent data, potentially aiding in the identification of potential\nillnesses or conditions with a certain degree of accuracy. Large language models have the potential to contribute\nto several aspects such as clinical decision assistance, clinical trial recruiting, clinical data administration,\nresearch support, patient education, and other related areas [55, 56]. Corroborating this perspective, authors\nintroduce a methodology that utilizes transformer models, namely BERT, RoBERTa, and DistilBERT, for the\npurpose of predicting COVID-19 diagnosis based on textual descriptions of acute alterations in chemosensation\n[57]. Similarly, a number of alternative investigations have been undertaken within the literature, proposing\nstrategies using large language models for the diagnosis of Alzheimer\u2019s disease [58] and dementia [59]. Fur-\nthermore, a corpus of literature has emerged, advocating the integration of large language model chatbots to\ncater to analogous objective [60, 61, 62, 63].\n\u2022 Patient Care:Large Language Models have emerged as transformative tools with the capacity to significantly\nenhance the realm of patient care [64]. Through the provision of personalised recommendations [ 65], cus-\ntomised treatment strategies, and continual monitoring of patients\u2019 advancements throughout their medical\njourneys [66], LLMs offer the promise of revolutionizing healthcare delivery. By harnessing the capabilities\nof LLMs, healthcare providers can ensure a more personalized and patient-centric approach to care. This\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1328a3e8-8820-4b15-b9eb-fa661608bd93": {"__data__": {"id_": "1328a3e8-8820-4b15-b9eb-fa661608bd93", "embedding": null, "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adcddc03-9e4b-4388-89fa-04aae2a9ccb9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c43ad7dc58753252077fe835f802b0dc45e7ca81a911bf1c3c11e253ee33c9bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6280781-059c-41c8-96b4-d70fb657b03f", "node_type": "1", "metadata": {}, "hash": "0f553faabde95c5effbf0946d47b97ee6f9cff13938c798eb02f1ceeb0f0d3d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\ntechnology enables the delivery of precise and well-informed medical guidance [67], aligning interventions\nwith patients\u2019 distinct requirements and circumstances.\nThe effective use of LLMs within clinical practise not only enhances patient outcomes but also enables\nhealthcare professionals to make data-driven decisions, leading to enhanced patient care. As LLMs continue\nto advance, the potential for augmenting patient care through personalized recommendations and ongoing\nmonitoring remains a promising trajectory in modern medicine [68]. In essence, LLMs represent a pivotal\nleap forward, holding the capacity to reshape the landscape of patient care by fostering precision, adaptability,\nand patient-centeredness [69].\n\u2022 Clinical Decision Support: Language models (LMs) have evolved into crucial decision support tools for\nhealthcare professionals. By analyzing extensive medical data, LMs can provide evidence-based recommenda-\ntions, enhancing diagnostic accuracy, treatment selection, and overall patient care. This fusion of artificial\nintelligence with healthcare expertise holds immense promise for improved medical decision-making. A body\nof existing research has illuminated promising prospects for the application of language models within clinical\ndecision support, particularly within the domains of radiology [70], oncology [71] and dermatology [72].\n\u2022 Medical Literature Analysis: Large language models (LLMs) exhibit remarkable efficiency in comprehen-\nsively reviewing and succinctly summarizing extensive volumes of medical literature. This capability aids\nboth researchers and clinicians in maintaining topicality with cutting-edge developments and evidence-based\nmethodologies, ultimately fostering informed and optimized healthcare practices. In a fast-evolving field like\nhealthcare, the ability to maintain currency with the latest advancements is paramount, and LLMs can play a\npivotal role in ensuring that healthcare remains at the forefront of innovation and evidence-based care delivery\n[73, 74].\n\u2022 Drug Discovery: Large Language Models, have a significant impact in facilitating drug discovery through\ntheir capacity to scrutinize intricate molecular structures, discern promising compounds with therapeutic\npotential, and forecast the efficacy and safety profiles of these candidates [75, 76]. Chemical language models\nhave exhibited notable achievements in the domain of de novo drug design [77]. In this corresponding study,\nauthors explore the utilization of pre-trained biochemical language models to initialize targeted molecule\ngeneration models, comparing one-stage and two-stage warm start strategies, as well as evaluating compound\ngeneration using beam search and sampling, ultimately demonstrating that warm-started models outperform\nbaseline models and the one-stage strategy exhibits superior generalization in terms of docking evaluation and\nbenchmark metrics, while beam search proves more effective than sampling for assessing compound quality\n[78].\n\u2022 Virtual Medical Assistants and Health Chatbots: LLMs may also serve as the underlying intelligence for\nhealth chatbots, revolutionizing the healthcare landscape by delivering continuous and personalized health-\nrelated support. These chatbots can offer medical advice, monitor health conditions, and even extend their\nservices to encompass mental health support, a particularly pertinent aspect of healthcare given the growing\nawareness of mental well-being [63, 60].\n\u2022 Radiology and Imaging:Multi-modal visual-language models, through their integration of visual and textual\ndata, hold significant promise for augmenting medical imaging analysis. Radiologists can benefit from these\nmodels as they facilitate the early identification of abnormalities in medical images and contribute to the\ngeneration of more precise and comprehensive diagnostic interpretations, ultimately advancing the accuracy\nand efficiency of diagnostic processes in the field of medical imaging [79, 70, 80, 81, 82, 83, 84].\n\u2022 Automated Medical Report Synthesis from Imaging Data:Automated medical report generation from\nimages is crucial for streamlining the time-consuming and error-prone task faced by pathologists and radiol-\nogists. This emerging field at the intersection of healthcare and artificial intelligence (AI) aims to alleviate\nthe burden on experienced medical practitioners and enhance the accuracy of less-experienced ones. The\nintegration of AI with medical imaging facilitates the automatic drafting of reports, encompassing abnormal\nfindings, relevant normal observations, and patient history. Early efforts employed data-driven neural networks,\ncombining convolutional and recurrent models for single-sentence reports, but limitations arose in capturing\nthe complexity of real medical scenarios [5]. Recent advances leverage large language models (LLMs) such as\nChatCAD [70], enabling more sophisticated applications. ChatCAD enhances medical-image Computer-Aided\nDiagnosis networks, yielding significant improvements in report generation. ChatCAD+ further addresses\nwriting style mismatches, ensuring universality and reliability across diverse medical domains, incorporating a\ntemplate retrieval system for consistency with human expertise [44].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6280781-059c-41c8-96b4-d70fb657b03f": {"__data__": {"id_": "f6280781-059c-41c8-96b4-d70fb657b03f", "embedding": null, "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adcddc03-9e4b-4388-89fa-04aae2a9ccb9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c43ad7dc58753252077fe835f802b0dc45e7ca81a911bf1c3c11e253ee33c9bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1328a3e8-8820-4b15-b9eb-fa661608bd93", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "484d5b7f4e92a9b22ccf0f4fe828e68ccd69c86a82c17654d0a80d0200a385db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Early efforts employed data-driven neural networks,\ncombining convolutional and recurrent models for single-sentence reports, but limitations arose in capturing\nthe complexity of real medical scenarios [5]. Recent advances leverage large language models (LLMs) such as\nChatCAD [70], enabling more sophisticated applications. ChatCAD enhances medical-image Computer-Aided\nDiagnosis networks, yielding significant improvements in report generation. ChatCAD+ further addresses\nwriting style mismatches, ensuring universality and reliability across diverse medical domains, incorporating a\ntemplate retrieval system for consistency with human expertise [44]. In [85], authors use pre-trained language\nmodel (PLM) and in-context learning (ICL) to generate clinical note from doctor patient conversation. These\nintegrated systems signify a pivotal advancement in automating medical report generation through the strategic\nutilization of LLMs.\n8", "mimetype": "text/plain", "start_char_idx": 4657, "end_char_idx": 5595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "78485156-54a6-4e4b-950c-8630b10c039e": {"__data__": {"id_": "78485156-54a6-4e4b-950c-8630b10c039e", "embedding": null, "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2d6b0b5-edf4-4e67-9363-f35aedd4721d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "76c78cf32de25bd77d1bdacfe9eaa3356888ac274636cc53e5e1180a3637b98b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 2: Summary of Recent XIAI Methods for LLMs in Healthcare\nMethod Year Task XIAI Attributes XIAI Evaluation\nMetric\nMentaLLaMA\n[code] [94]\n2024 Mental health analysis Prompt-based (ChatGPT w/ task-\nspecific instructions)\nBART-score, Human\nEval\nArgMed-\nAgents [93]\n2024 Clinical decision reasoning Prompt-based (Self-argumentation iter-\nations + symbolic solver)\nPred. accuracy with\nLLM evaluator\nDiagnostic rea-\nsoning prompts\n[95]\n2024 Medical Question Answering\n(MedQA)\nPrompt-based (Bayesian, differential di-\nagnosis, analytical, and intuitive reason-\ning)\nExpert Evaluation,\nInter-rater agreement\nSkinGEN [96] 2024 Dermatological diagnosis Visual explanations (Stable Diffusion),\ninteractive framework\nPerceived explainabil-\nity ratings\nDR. KNOWS\n[91]\n2023 Automated diagnosis generationKnowledge Graph (explainable diagnos-\ntic pathway)\n-\nHuman-AI Col-\nlaboration [97]\n2023 Clinical decision making Salient features, counterfactual explana-\ntions\nAgreement Level,\nUsability Question-\nnaires\nChatGPT [92] 2023 Mental health analysis Prompt-based (emotional cues and\nexpert-written few-shot examples)\nBART-score, Human\nEval\nCHiLL [98] 2023 Clinical predictive tasks, Chest\nX-ray report classification\nInterpretable features, linear models Expert Evaluation,\nClinical Judgement\nAlignment\nTrap-VQA [99] 2022 Pathology Visual Question An-\nswering (PathVQA)\nGrad-CAM, SHapley Additive exPlana-\ntions\nQualitative Evalua-\ntion\nVision Trans-\nformer [100]\n2021 Covid-19 diagnosis Saliency maps Visualisation\nClinicalBERT\n[code] [15]\n2019 Predicting hospital readmissionAttention weights Visualisation\n3.3 Explainable AI Methods for Interpreting Healthcare LLMs\nLarge Language Models (LLMs) have significantly advanced the healthcare domain, enhancing tasks such as med-\nical diagnosis and patient monitoring. However, the complexity of these models necessitates interpretability for\nreliable decision-making [86]. This section discusses \"eXplainable and Interpretable Artificial Intelligence\" (XIAI)\nand examines recent XIAI methods by their functionality and scope. Despite challenges, such as the difficulty in\nquantifying interpretability and the lack of standardized evaluation metrics, opportunities exist in integrating XIAI\nto add interpretability for LLMs in healthcare. Notable XIAI methods include SHAP [ 87], which quantifies feature\ncontributions, LIME [88, 89], which generates interpretable models through input perturbations, t-SNE for visualizing\nhigh-dimensional data [90], attention mechanisms that highlight key features [15], and knowledge graphs that structure\ncontextual relationships [91], all of which provide crucial insights into model decision-making processes.\nExisting research delves into explainability for LLMs in the healthcare domain. For instance, Yang et al. (2023) [92]\ninvestigate different prompting strategies using emotional cues and expert-written examples for mental health analysis\nwith LLMs. This study shows that models like ChatGPT can generate near-human-level explanations, enhancing\ninterpretability and performance. Additionally, ArgMedAgents (Hong et al., 2024) [93] is a multi-agent framework\ndesigned for explainable clinical decision reasoning through interaction, utilizing the Argumentation Scheme for\nClinical Discussion and a symbolic solver to provide clear decision explanations. Furthermore, Gao et al. (2023)\npropose enhancing LLM explainability for automated diagnosis by integrating a medical knowledge graph (KG) from\nthe Unified Medical Language System (UMLS), using the DR.KNOWS model to interpret complex medical concepts.\nTheir experiments with real-world hospital data demonstrate a transparent diagnostic pathway. Similarly, TraP-VQA\n[91], a novel vision-language transformer for Pathology Visual Question Answering (PathVQA), employs Grad-CAM\nand SHAP methods to offer visual and textual explanations, ensuring transparency and fostering user trust.\nWe have compiled a list in table 2, detailing XIAI attributes, summarizing recent research works focused on explainability\nmethods for LLMs in the healthcare domain. This table includes evaluations of various models, highlighting their\nunique contributions to enhancing interpretability and reliability in medical applications. Each entry outlines the task,\nmethod, XAI attributes, and evaluation metrics, offering a clear overview of the advancements and effectiveness of\nXIAI techniques in improving decision-making processes in healthcare.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2a5bfc1-c413-414a-86e4-e3ccbfc5aee7": {"__data__": {"id_": "b2a5bfc1-c413-414a-86e4-e3ccbfc5aee7", "embedding": null, "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e401de1-7d6d-4811-95c8-4eab0d7a3f5e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8d5d01256c22faae2495eeefad480a16262d1fce9db8c801c82db73a5c841827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e0e837-8ebc-4278-a7d8-013fe6d8fe4b", "node_type": "1", "metadata": {}, "hash": "47ca8b4def1a1257a965b17b0db949a4f816f210d421729c69e9cffa70d3c014", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n3.4 Future Trajectories of Large Language Models in Healthcare\nAs large language models (LLMs) continue to integrate into the healthcare sector, future developments promise to\nrevolutionize patient care and medical research. A particularly promising avenue involves enhancing LLMs\u2019 capabilities\nto interpret and generate not only textual but also biomolecular data [101]. This advancement could significantly improve\napplications in genomics and personalized medicine, enabling these models to predict individual responses to treatments\nbased on genetic profiles, thereby advancing the precision of medical interventions. Furthermore, incorporating adaptive\nlearning capabilities in real-time could transform LLMs into dynamic aids during surgical procedures or emergencies,\nwhere they might analyze data from medical devices on-the-fly [102] to offer critical decision support.\nAnother innovative trajectory for LLMs in healthcare is the development of federated learning systems [103]. Such\nsystems could facilitate the secure, privacy-preserving propagation of medical knowledge across institutions, improving\nmodel robustness and applicability across varied demographic groups without direct data sharing. This approach will\nnot only enhance the privacy and security of patient data but will also enable a collective intelligence that could lead to\nmore generalized healthcare solutions.\nThe potential of large language models (LLMs) in healthcare extends into the realms of explainable medical AI [104]\nand the utilization of multi-modal models incorporating sensor data. By integrating LLMs with wearable technologies\n[105], these advanced models can serve as continuous health monitors in non-clinical settings.\nTo further advance explainable medical AI, LLMs can be instrumental in deciphering the complexities of medical\nconditions and treatment outcomes. By processing and interpreting multi-modal data, including sensor readings, these\nmodels can contribute to a deeper understanding of patient health on a granular level. This may aid in the development\nof precise, targeted therapies, improving patient outcomes and enhancing the transparency of medical decisions.\nLarge Language Models (LLMs) are poised to revolutionize the healthcare domain by enhancing diagnostic accuracy,\npersonalizing treatment plans, and optimizing operational efficiencies. By integrating LLMs into electronic health\nrecord systems, healthcare providers can more accurately diagnose conditions through natural language processing\ntechniques that analyze clinical notes and patient histories. Moreover, LLMs assist in generating personalized treatment\nrecommendations by analyzing vast datasets that include genetic information, clinical outcomes, and patient preferences.\nFurthermore, these models streamline administrative tasks by automating documentation, coding, and billing processes,\nthus reducing operational costs and allowing medical staff to focus more on patient care. As generative AI advances,\nits transformative impact on the healthcare sector is becoming increasingly significant. This technology is poised to\nrevolutionize areas such as clinical trials, personalized medicine, and drug discovery. Additionally, its applications\nextend to enhancing natural language processing and understanding, improving medical imaging, and supporting\nvirtual assistants in patient care. Generative AI also plays a crucial role in illness detection and screening, facilitating\nmore accurate diagnostics. Moreover, it is being integrated into medical conversation tasks, voice generation, video\ngeneration, and image synthesis and manipulation within healthcare settings [ 106]. These innovations are not only\nimproving the efficiency of medical services but are also paving the way for new methods of patient interaction and\ntreatment planning. As these applications continue to mature, LLMs will become integral in transforming healthcare\nservices into more efficient, accurate, and personalized systems.\n3.5 Performance Evaluation and Benchmarks\nThe medicine and healthcare industries largely acknowledge the potential of artificial intelligence (AI) to drive\nsubstantial progress in the delivery of healthcare. However, empirical evaluations have demonstrated that numerous\nartificial intelligence (AI) systems do not successfully achieve their desired translation goals, primarily because of\nintrinsic deficiencies that become evident only after implementation [107, 108]. In order to optimize the utilization of\nlanguage models (LLMs) within healthcare settings, it is imperative to develop evaluation frameworks that possess\nthe capacity to thoroughly evaluate their safety and quality. It is important to note that certain highly effective models,\nsuch as ChatGPT and PaLM 2 [109], are now not publicly available. The absence of accessibility gives rise to notable\nproblems pertaining to transparency, which is a crucial factor in the medical domain and hinders the capacity to\nthoroughly examine the structure and results of the model. Consequently, this impedes endeavors to recognize and\naddress biases and hallucinations. Thorough research is necessary to understand the specific performance characteristics\nand ramifications of utilizing publicly accessible, pre-trained language models in addressing the challenges in the\nhealthcare and medical domains. Language models that have been pre-trained using medical data also encounter\ncomparable difficulties.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62e0e837-8ebc-4278-a7d8-013fe6d8fe4b": {"__data__": {"id_": "62e0e837-8ebc-4278-a7d8-013fe6d8fe4b", "embedding": null, "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e401de1-7d6d-4811-95c8-4eab0d7a3f5e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8d5d01256c22faae2495eeefad480a16262d1fce9db8c801c82db73a5c841827", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2a5bfc1-c413-414a-86e4-e3ccbfc5aee7", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "881ecd6696f01c36fe0880d69eb2f6f9b77bdc1ca8da02be6a865c54451b4a34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The absence of accessibility gives rise to notable\nproblems pertaining to transparency, which is a crucial factor in the medical domain and hinders the capacity to\nthoroughly examine the structure and results of the model. Consequently, this impedes endeavors to recognize and\naddress biases and hallucinations. Thorough research is necessary to understand the specific performance characteristics\nand ramifications of utilizing publicly accessible, pre-trained language models in addressing the challenges in the\nhealthcare and medical domains. Language models that have been pre-trained using medical data also encounter\ncomparable difficulties. Therefore, the careful choice and implementation of suitable performance metrics to evaluate\nthe language model assume great significance.\nIn table 4, we present a comprehensive catalog of performance metrics, including but not limited to the F1 score,\nBLEU, GLUE, and ROGUE, which constitute the standard evaluative criteria employed for the rigorous assessment of\nlarge language models operating within the healthcare and medical domain. This compendium of metrics serves as a\n10", "mimetype": "text/plain", "start_char_idx": 4876, "end_char_idx": 6005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cefd4005-86b5-4faf-a469-78c5a433cf62": {"__data__": {"id_": "cefd4005-86b5-4faf-a469-78c5a433cf62", "embedding": null, "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "434cf912-7cf4-4349-8ed5-215c3ee4ee56", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7b58f645948932809bfe9fb5b4070f30c5cd6525df74ef0c7132b7d1c86cd57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 4: Comparative Performance of Healthcare LLMs\nTable 3: LLM Performance Benchmark\nOrganization Model MMLU Score Coding (HumanEval) Release Date\nOpenAI GPT-4 Opus 88.7 - May 2024\nAnthropic Claude 3.5 Sonnet 88.7 92.0 June 2024\nAnthropic Claude 3 Opus 86.8 - March 2024\nOpenAI GPT-4 Turbo 86.4 85.4 April 2024\nOpenAI GPT-4 86.4 90.2 April 2023\nMeta Llama 3 400B 86.1 - -\nGoogle Gemini 1.5 Pro 85.9 84.1 May 2024\nGoogle Gemini Ultra 83.7 - December 2023\nOpenAI GPT-3.5 Turbo - 73.2 -\nMeta Llama 3 (70B) - 81.7 -\nMeta Llama 3 (8B) - 62.2 -\nGoogle Gemini 1.5 Flash - 74.3 -\nvaluable reference, encapsulating the quantitative and qualitative measures utilized to gauge the efficacy, proficiency,\nand suitability of these models in diverse healthcare applications [108].\n3.6 Quantitative Performance Comparison of LLMs in Healthcare Domain\nRecent advancements in language models have been benchmarked against diverse datasets to evaluate their capabilities\nacross various domains. One such comprehensive benchmark is the MMLU (Massive Multitask Language Under-\nstanding) [110], designed to assess the understanding and problem-solving abilities of language models. The MMLU\ncomprises 57 tasks spanning topics such as elementary mathematics, US history, computer science, and law, requiring\nmodels to demonstrate a broad knowledge base and problem-solving skills. This benchmark provides a standardized\nmethod to test and compare various language models, including OpenAI GPT-4o, Mistral 7b, Google Gemini, and\nAnthropic Claude 3, among others.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "486e2214-3f7c-417a-8c8b-4e87b5841238": {"__data__": {"id_": "486e2214-3f7c-417a-8c8b-4e87b5841238", "embedding": null, "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39ff8ed8-5b2c-47d3-a1ab-c9292850e48f", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9f0ce6b65b8fa7a82c951d6f5893d7304c8fb1791eb7f158ce80595131e57342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59917371-2612-4d68-85f1-f869cebb51de", "node_type": "1", "metadata": {}, "hash": "de93ff987a61896e56776e66122dc944b282b7c8d4ae19a339a153f106b24244", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nThe HumanEval benchmark is used to measure the functional correctness of code generated by LLMs from docstrings.\nThis benchmark evaluates models based on their ability to generate code that passes provided unit tests, using the\npass@k metric. If any of the \u2019k\u2019 solutions generated by the model pass all unit tests, the model is considered successful\nin solving the problem [111]. Table 3 provides a concise summary of the performance of various LLMs on the MMLU\nand HumanEval (Coding) datasets [112].\nIn the healthcare domain, a variety of LLMs have been developed and evaluated on specific datasets such as MedQA,\nMedNLI [113], Tox21 [114], and PubMedQA [115]. The GPT-4 (2024) model stands out in the MedQA dataset with\nan impressive accuracy of 93.06%, significantly outperforming other models like Med-PaLM 2 (CoT + SC) (2023),\nwhich achieves 83.7%, and Meerkat-7B (Ensemble) (2024), with 74.3%. In the MedNLI dataset, BioELECTRA-Base\n(2021) achieves the highest accuracy of 86.34%, closely followed by CharacterBERT (base, medical) (2020) at 84.95%.\nThe Tox21 dataset highlights elEmBERT-V1 (2023) with an outstanding AUC of 0.961, making it the most effective\nin predicting chemical properties and toxicity. For the PubMedQA dataset, Meditron-70B (CoT + SC) (2023) and\nBioGPT-Large (1.5B) (2023) exhibit strong performance with accuracies of 81.6% and 81.0%, respectively [116]. These\nfindings underscore the variability in performance across different healthcare tasks, emphasizing the need for careful\nselection of models based on specific application requirements [117]. Figure 4 presents a comparative performance\nanalysis of various healthcare LLMs, highlighting their accuracy and AUC metrics across different datasets including\nMedQA, MedNLI, Tox21, and PubMedQA.\n4 Limitations and Open Challenges\nThe integration of large language models (LLMs) in healthcare presents complex challenges, including the need\nfor explainability in model decision-making, robust security and privacy measures to protect sensitive patient data,\naddressing biases and ensuring fairness in medical AI applications, mitigating the issue of hallucinations where models\ngenerate erroneous information, and establishing clear legal frameworks for the responsible use of LLMs in healthcare,\nall of which demand careful scrutiny and resolution to harness the full potential of these models for improving healthcare\noutcomes while upholding ethical and legal standards.\n4.1 Model Explainability and Transparency\nLarge language models face notable challenges when applied to healthcare. Their recommendations often lack\ntransparency due to their opaque nature, which can hinder acceptance among healthcare professionals who prioritize\nexplainability in medical decision-making. Moreover, the presence of biases in the training data may compromise the\naccuracy of these models, potentially leading to incorrect diagnoses or treatment recommendations. It is therefore\ncrucial for medical professionals to exercise caution and thoroughly review and validate the recommendations provided\nby large language models before integrating them into their clinical decision-making processes [127]. In healthcare,\nthe importance of interpretability and explainability for AI models utilized in medical imaging analysis and clinical\nrisk prediction cannot be overstated. Inadequate transparency and explainability have the potential to undermine\ntrustworthiness and hinder the validation of clinical recommendations. Consequently, effective governance underscores\nthe continuous pursuit of transparency and interpretable frameworks, aiming to augment the decision-making process in\nthe realm of healthcare [108]. Large language models (LLMs) often function as \"blackboxes\", rendering it challenging\nto discern the underlying processes leading to specific conclusions or suggestions. In the healthcare context, where the\nrepercussions of decisions are profound, it becomes imperative for practitioners to grasp the logic behind AI-generated\noutputs. The persistent endeavor to create models that are more interpretable and transparent remains an enduring\nchallenge within the healthcare domain [128, 129, 130].\n4.2 Security and Privacy Considerations\nLarge Language Models (LLMs) are used in medical research, which necessitates careful consideration of data privacy\nand security issues. Researchers are entrusted with the duty of managing extremely private patient data while enforcing\nrigorous compliance with current privacy laws.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "59917371-2612-4d68-85f1-f869cebb51de": {"__data__": {"id_": "59917371-2612-4d68-85f1-f869cebb51de", "embedding": null, "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39ff8ed8-5b2c-47d3-a1ab-c9292850e48f", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9f0ce6b65b8fa7a82c951d6f5893d7304c8fb1791eb7f158ce80595131e57342", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "486e2214-3f7c-417a-8c8b-4e87b5841238", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7b88cd1a7bbbe3222f36cf6916064f51d9abc2dee3d86b7b1dbd532626618b0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the healthcare context, where the\nrepercussions of decisions are profound, it becomes imperative for practitioners to grasp the logic behind AI-generated\noutputs. The persistent endeavor to create models that are more interpretable and transparent remains an enduring\nchallenge within the healthcare domain [128, 129, 130].\n4.2 Security and Privacy Considerations\nLarge Language Models (LLMs) are used in medical research, which necessitates careful consideration of data privacy\nand security issues. Researchers are entrusted with the duty of managing extremely private patient data while enforcing\nrigorous compliance with current privacy laws. The use of LLMs in this setting raises concerns about a number of\naspects of data processing, including as data protection, the possibility of re-identification, and the moral application\nof patient data. One notable issue is the inadvertent inclusion of personally identifiable information (PII) within\npre-training datasets, which can compromise patient confidentiality. Additionally, LLMs can make privacy-invading\ninferences by deducing sensitive personal attributes from seemingly innocuous data, potentially violating individual\nprivacy [131]. Implementing strong measures like data anonymization, safe data storage procedures, and steadfast\nadherence to ethical standards are essential to addressing these issues. Together, these steps make up crucial safeguards\nmeant to protect research participants\u2019 trust, maintain the integrity of research processes, and protect patient privacy.\nThe importance of these factors is underscored by the necessity of balancing the significant contributions of LLMs\n12", "mimetype": "text/plain", "start_char_idx": 3917, "end_char_idx": 5576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7388d728-d29b-4b71-bff9-335b66cb0ee6": {"__data__": {"id_": "7388d728-d29b-4b71-bff9-335b66cb0ee6", "embedding": null, "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a3c2390-9797-41c6-b4ed-c32c4783bbf9", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25f13644f1850113b71dc24fcb1b88f9f497b0170cac5a44f686001236b451c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 4: Evaluation Metrics for Language Models in Healthcare Domain\nEval. Metric Description References Key Highlights\nPerplexity\nPerplexity, a probabilistic metric, quan-\ntifies the uncertainty in the predictions\nof a language model. Lower values indi-\ncate higher prediction accuracy and co-\nherence.\n[118] -\n[119] The federated learning model\nachieved a best perplexity\nvalue of 3.41 for English.\n[120] The Transformer model\nachieved a test perplexity of\n15.6 on the PSVG dataset,\nsignificantly outperforming the\nLSTM\u2019s perplexity of 20.7.\n[91] The lowest perplexity achieved\nwas 3.86e-13 with manually de-\nsigned prompts.\nBLEU The BLEU score assesses the quality of\nmachine translation by comparing it to\nreference translations.\n[121] The best BLEU-1 score\nachieved was 13.9 by the\nClinicalGPT model.\n[122] T-5 (fine-tuned) model\nachieved the best BLEU-1\nscore of 26.63.\nGLEU GLEU score computes mean scores of\nvarious n-grams to assess text generation\nquality.\n[121] The best GLEU score achieved\nwas 2.2 by the Bloom-7B\nmodel.\n[122] T-5 (fine-tuned) model\nachieved the best GLEU score\nof 11.38.\nROUGE ROUGE score evaluates summarization\nand translation by measuring overlap with\nreference summaries.\n[121] The best ROGUE-L score\nachieved was 21.3 by the Clini-\ncalGPT model.\n[122] T-5 (fine-tuned) model\nachieved the best ROGUE-L\nscore of 24.85.\nDistinct n-grams Measures the diversity of generated re-\nsponses by counting unique n-grams.\n[122] On the Huatuo-26M dataset,\nthe fine-tuned T5 model\nachieved Distinct-1 and\nDistinct-2 scores of 0.51 and\n0.68, respectively.\nF1 Score\nThe F1 score balances precision and re-\ncall, measuring a model\u2019s accuracy in\nidentifying positive instances and mini-\nmizing false results.\n[123] The GatorTron-large model\nachieved the best F1 score of\n0.9627 for medical relation ex-\ntraction.\n[46] The GatorTron-large model\nachieved the best F1 score of\n0.9000 for clinical concept ex-\ntraction and 0.9627 for medical\nrelation extraction.\n[124] The multicenter Transformers-\nbased model achieved an over-\nall F1 score of 84.77% on the\nPsyNIT dataset.\n[76] The BERT-D2 model achieved\nan F1 score of 81.97% on the\nDDI Extraction 2013 corpus.\nBERTScore BERTScore calculates similarity scores\nbetween tokens in candidate and refer-\nence sentences, using contextual embed-\ndings.\n[125] -\n[85] The Longformer-Encoder-\nDecoder (LED large-PubMed)\nmodel achieved the best\nBERTScore F1 of 70.7.\nHuman Evaluation Involves expert human assessors rating\nthe quality of model-generated content,\nproviding qualitative insights into its per-\nformance.\n[126] The median performance\nfor all human SCORE users\nwas 65%, whereas ChatGPT\ncorrectly answered 71%\nof multiple-choice SCORE\nquestions and 68% of Data-B\nquestions.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "177ea331-8ba9-454e-bcd0-1ea58b71d4e3": {"__data__": {"id_": "177ea331-8ba9-454e-bcd0-1ea58b71d4e3", "embedding": null, "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d99b3ec8-f5fc-4838-bd99-20bcfbc2f0ad", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5ada16f09d82194d6c25d48829ae8c7a3187c93a1f33e02e3eb907a2ac40ba38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 5: Challenges of Large Language Models in Healthcare\nin medical research with the critical requirement to protect private patient information [ 132]. LLMs\u2019 ability to find\npotentially revealing patterns in large amounts of health data, even when anonymized, poses a serious privacy risk. This\nnecessitates strict regulations and technical protections. Anonymizing data more effectively is crucial, as are algorithms\ndesigned to spot and prevent the re-identification of individuals. Ongoing monitoring of what LLMs produce is vital\nto ensure privacy isn\u2019t accidentally compromised. Implementing these measures helps guarantee responsible use of\nsensitive data, allowing LLMs to be used ethically in healthcare while still respecting patient privacy. To ensure the\nethical use of LLMs in healthcare, strong governance frameworks must extend beyond basic privacy laws. Proactive\npolicies should anticipate challenges, and experts need to verify LLMs meet ethical guidelines. Engaging patients and\nhealthcare providers in the development process promotes transparency and maintains trust in how health data is used\nwithin these systems.\n4.3 Bias and Fairness\nResearching ways to tackle and reduce biases in language models, while also comprehending their ethical ramifications,\nrepresents a pivotal research domain. It is imperative to create techniques for identifying, alleviating, and forestalling\nbiases in large language models. A primary concern associated with Large Language Models (LLMs) pertains to\nthe risk of producing misinformation or biased outputs. These models, drawing from extensive text data, encompass\nboth dependable and unreliable sources, which can inadvertently result in the generation of inaccurate or misleading\ninformation. Furthermore, if the training data incorporates biases, such as gender or racial biases prevalent within\nscientific literature, LLMs can perpetuate and magnify these biases in their generated content.\nTo ensure the reliability and accuracy of information derived from LLMs, researchers must exercise caution and\nimplement rigorous validation and verification processes. LLMs have the potential to amplify pre-existing biases\ninherent in their training data, particularly those linked to demographics, disease prevalence, or treatment outcomes.\nConsequently, the generated outputs may inadvertently reflect and perpetuate these biases, posing considerable\nchallenges in achieving equitable and unbiased healthcare outcomes.\nTo address these challenges, researchers must remain vigilant in recognizing and mitigating biases within both the\ntraining data and the outputs generated by LLMs. This diligence is crucial for promoting fairness and inclusivity within\nthe realm of biomedical research and healthcare applications, ultimately enhancing the ethical and equitable utility\nof LLMs in these domains [ 132]. Prioritizing bias mitigation in LLMs is essential. Researchers should curate and\npreprocess training data diligently to reduce inherent biases and address sources of inequality. Routine audits and\nevaluations are necessary to identify and correct biases in model training and deployment. Collaborative efforts between\ndomain experts, ethicists, and data scientists can establish guidelines and best practices for unbiased LLM development,\nfostering fairness and inclusivity in biomedical research and healthcare.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d2c2217-4101-4644-9c02-3a894a47a2c9": {"__data__": {"id_": "3d2c2217-4101-4644-9c02-3a894a47a2c9", "embedding": null, "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "981f68e3-8d13-49b9-a94f-e77afaf948cd", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5a9391ebe71b8830d1c692a2f5596f93e5499085ce4340ae80b9e212734c99e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n4.4 Hallucinations and Fabricated Information\nLanguage models exhibit a proclivity for generating erroneous content, commonly referred to as hallucinations.\nThis phenomenon is characterized by the production of text that appears plausible but lacks factual accuracy. This\ninherent trait poses a substantial risk when such generated content is employed for critical purposes, such as furnishing\nmedical guidance or contributing to clinical decision-making processes. The consequences of relying on hallucinatory\ninformation in healthcare contexts can be profoundly detrimental, potentially leading to harmful or even catastrophic\noutcomes [133].\nThe gravity of this issue is exacerbated by the continuous advancement of Large Language Models (LLMs), which\ncontinually enhance their capacity to generate increasingly persuasive and believable hallucinations. Moreover, LLMs\nare often critiqued for their opacity, as they provide no discernible link to the original source of information, thereby\ncreating a formidable barrier to the verification of the content they produce. To mitigate these risks, healthcare\nprofessionals must exercise extreme caution when utilizing LLMs to inform their decision-making processes, rigorously\nvalidating the accuracy and reliability of the generated information.\nCurrent research endeavors are dedicated to addressing hallucination issues within Large Language Models (LLMs) in\nthe healthcare and medical domain. The introduction of Med-HALT, a novel benchmark dataset, serves the purpose of\nevaluating hallucination phenomena in LLMs in medical contexts. Med-HALT encompasses two distinct test categories:\nreasoning-based and memory-based hallucination assessments. These tests have been meticulously designed to gauge\nthe problem-solving and information retrieval capabilities of LLMs when operating within the medical domain [43].\n4.5 Legal and Ethical Reasons\nEthical concerns extend to the generation of potentially harmful content by LLMs, especially when delivering distressing\nmedical diagnoses without providing adequate emotional support. Moreover, the blurring line between LLM-generated\nand human-written text poses a risk of misinformation dissemination, plagiarism, and impersonation.\nTo address these challenges, rigorous auditing and evaluation of LLMs are essential, along with the development of\nregulations for their medical use. Thoughtful selection of training datasets, particularly within the medical domain,\nis crucial to ensure the responsible handling of sensitive data. These measures collectively strive to strike a balance\nbetween harnessing LLMs\u2019 potential and safeguarding patient privacy and ethical standards [131].\nThe European Union\u2019s AI Act and the United States\u2019 Health Insurance Portability and Accountability Act (HIPAA)\nare two significant regulatory frameworks impacting the deployment of AI in healthcare. The AI Act introduces\ncomprehensive regulations, including the Artificial Intelligence Liability Directive (AILD), which addresses liability for\nAI-related damages. This directive ensures that victims are compensated and that preventive measures are cost-effective.\nThe AI Act classifies General Purpose AI (GPAI) models and imposes specific obligations on providers, including\ntechnical documentation, risk assessments, and transparency about training data [134].\nIn the United States, HIPAA sets stringent standards for the protection of patient data, impacting how LLMs handle\nsensitive information. Compliance with HIPAA requires robust data encryption, regular security assessments, and strict\naccess controls to protect patient information. These regulations ensure that LLMs used in healthcare settings adhere to\nhigh standards of privacy and security, mitigating risks associated with data breaches and unauthorized access.\nOther relevant laws and compliance frameworks include the General Data Protection Regulation (GDPR) in the EU,\nwhich emphasizes data protection and privacy, and the Medical Device Regulation (MDR) that ensures the safety\nand efficacy of AI-driven medical devices. These regulations collectively impact the deployment of generative AI in\nhealthcare by ensuring legal accountability, protecting patient data, and promoting ethical standards in AI development\nand application.\nThe implementation of regulatory frameworks such as the EU\u2019s AI Act, HIPAA, GDPR, and MDR significantly\nimpacts the deployment of LLMs and generative AI in healthcare by ensuring transparency, data protection, and patient\nsafety. These regulations necessitate detailed documentation of AI models, advanced data encryption, strict access\ncontrols, and rigorous clinical testing, thereby increasing development costs and timelines. However, they also promote\nreliability, legal accountability, and ethical standards in AI development, fostering trust among users and stakeholders\nand encouraging the responsible and wider adoption of AI technologies in healthcare [135].\n5 Conclusion\nIn conclusion, the integration of large language models (LLMs) in healthcare showcases immense potential for enhancing\nclinical language understanding and medical applications. These models offer versatility and sophistication, from\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7fd858dc-feea-46fc-82ae-fe386f056826": {"__data__": {"id_": "7fd858dc-feea-46fc-82ae-fe386f056826", "embedding": null, "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68120e16-101f-4af5-af63-aa3db48f0ecb", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7dbae05a2f662f7885c63eeaf9665c07acf83cb0a044161fedc49a0462a0df7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "014eae43-cb2b-41ca-a68f-ae5fd3588488", "node_type": "1", "metadata": {}, "hash": "df9c18b437500e970109c2ab1c876909bc436f4ef3cef6c5654223a361b989fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nnamed entity recognition to question-answering, bolstering decision support and information retrieval. Comparative\nanalyses of state-of-the-art LLMs and open-source options emphasize their significance in healthcare, promoting\ninnovation and collaboration. Performance metrics drive continuous improvement but call for rigorous evaluation\nstandards, considering potential biases and ethical concerns. However, challenges persist, including the need for robust\ntraining data, bias mitigation, and data privacy. LLMs in healthcare necessitate further research and interdisciplinary\ncooperation. LLMs promise transformative benefits, but their full potential hinges on addressing these challenges and\nupholding ethical standards. The ongoing journey of LLMs in healthcare demands collective efforts to harness their\npower for improved patient care while ensuring ethical and responsible application.\nReferences\n[1] Henglin Shi, Wei Peng, Haoyu Chen, Xin Liu, and Guoying Zhao. Multiscale 3d-shift graph convolution network\nfor emotion recognition from human actions. IEEE Intelligent Systems, 37(4):103\u2013110, 2022.\n[2] Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, and Guoying Zhao. Modality unifying network for visible-infrared\nperson re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n11185\u201311195, 2023.\n[3] Yante Li, Wei Peng, and Guoying Zhao. Micro-expression action unit detection with dual-view attentive\nsimilarity-preserving knowledge distillation. In 2021 16th IEEE International Conference on Automatic Face\nand Gesture Recognition (FG 2021), pages 01\u201308. IEEE, 2021.\n[4] Xiaopeng Hong, Wei Peng, Mehrtash Harandi, Ziheng Zhou, Matti Pietik\u00e4inen, and Guoying Zhao. Char-\nacterizing subtle facial movements via riemannian manifold. ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), 15(3s):1\u201324, 2019.\n[5] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A survey of large\nlanguage models for healthcare: from data, technology, and applications to accountability and ethics. arXiv\npreprint arXiv:2310.05694, 2023.\n[6] Yuqing Wang, Yun Zhao, and Linda Petzold. Are large language models ready for healthcare? a comparative\nstudy on clinical language understanding. arXiv preprint arXiv:2304.05368, 2023.\n[7] Ping Yu, Hua Xu, Xia Hu, and Chao Deng. Leveraging generative ai and large language models: a comprehensive\nroadmap for healthcare integration. In Healthcare, volume 11, page 2776. MDPI, 2023.\n[8] Wei Peng, Li Feng, Guoying Zhao, and Fang Liu. Learning optimal k-space acquisition and reconstruction using\nphysics-informed neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20794\u201320803, 2022.\n[9] Wei Peng, Ehsan Adeli, Tomas Bosschieter, Sang Hyun Park, Qingyu Zhao, and Kilian M Pohl. Generating\nrealistic brain mris via a conditional diffusion probabilistic model. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages 14\u201324. Springer, 2023.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "014eae43-cb2b-41ca-a68f-ae5fd3588488": {"__data__": {"id_": "014eae43-cb2b-41ca-a68f-ae5fd3588488", "embedding": null, "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68120e16-101f-4af5-af63-aa3db48f0ecb", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7dbae05a2f662f7885c63eeaf9665c07acf83cb0a044161fedc49a0462a0df7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fd858dc-feea-46fc-82ae-fe386f056826", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "83a95bfd89223a7f70a7ab3a3242be10857422c138c88267155a353b5b70b623", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488, 2023.\n[13] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[14] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36(4):1234\u20131240, 2020.\n[15] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342, 2019.\n[16] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian\nRiedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n16", "mimetype": "text/plain", "start_char_idx": 3342, "end_char_idx": 4614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "440d6004-7e26-4559-a775-99aab1416d29": {"__data__": {"id_": "440d6004-7e26-4559-a775-99aab1416d29", "embedding": null, "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c43deeb1-bc0a-4597-83ec-89fa05206ba8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25aa34720d599191c7347ee0424c03a4528691bdc11628fd4324117e88c5a253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec3edb3f-19d8-4df3-8b26-ef27f56b8c74", "node_type": "1", "metadata": {}, "hash": "8ce819f4026a6fff56d8ff78de6704f896cc0e75572ab23b25bfcd7c8b35a34d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[17] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[21] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\n[22] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In\nInternational Conference on Machine Learning, pages 5547\u20135569. PMLR, 2022.\n[23] Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, and Yu Sun. Pre-trained language models and their applications.\nEngineering, 2022.\n[24] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n[25] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and Jiashi\nFeng. Refiner: Refining self-attention for vision transformers. arXiv preprint arXiv:2106.03714, 2021.\n[26] Zabir Al Nazi, Fazla Rabbi Mashrur, Md Amirul Islam, and Shumit Saha. Fibro-cosanet: pulmonary fibrosis\nprognosis prediction using a convolutional self attention network.Physics in Medicine & Biology, 66(22):225013,\n2021.\n[27] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside\ntransformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963\u201312971,\n2021.\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10684\u201310695, 2022.\n[29] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models. arXiv\npreprint arXiv:2309.05922, 2023.\n[30] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal\nlarge language models. arXiv preprint arXiv:2306.13549, 2023.\n[31] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec3edb3f-19d8-4df3-8b26-ef27f56b8c74": {"__data__": {"id_": "ec3edb3f-19d8-4df3-8b26-ef27f56b8c74", "embedding": null, "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c43deeb1-bc0a-4597-83ec-89fa05206ba8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25aa34720d599191c7347ee0424c03a4528691bdc11628fd4324117e88c5a253", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "440d6004-7e26-4559-a775-99aab1416d29", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6dd8621ae36febb58d051b196dc7bcbf852bee500a84335c9f38a86877c7c8f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal\nlarge language models. arXiv preprint arXiv:2306.13549, 2023.\n[31] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning, pages\n19730\u201319742. PMLR, 2023.\n[33] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu.\nMova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024.\n[34] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.\nMoe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024.\n[35] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin\nWen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts. arXiv preprint arXiv:2405.05949,\n2024.\n[36] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and\nDaniel Shu Wei Ting. Large language models in medicine. Nature Medicine, pages 1\u201311, 2023.\n[37] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[38] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n17", "mimetype": "text/plain", "start_char_idx": 2779, "end_char_idx": 4667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5141534c-1abd-445a-af2b-c22a2e89f5bb": {"__data__": {"id_": "5141534c-1abd-445a-af2b-c22a2e89f5bb", "embedding": null, "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39743898-7152-4110-9bbe-f1b04afaf30e", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2652d48c1ac68dc06dcc9e8af6e7ba21d61746e6a779fae3618c69353b2eacf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab3c3480-844f-4e77-9c1e-907ba0fafbed", "node_type": "1", "metadata": {}, "hash": "12a6848378b81cb29485068324590b274b74eb25ff65e0dae62f10181b17c79b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[39] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard\nDufour. Biomistral: A collection of open-source pretrained large language models for medical domains. arXiv\npreprint arXiv:2402.10373, 2024.\n[40] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language\nmodels. arXiv preprint arXiv:2305.09617, 2023.\n[41] Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie\nLuo, Cheng Chen, et al. Radiology-llama2: Best-in-class large language model for radiology. arXiv preprint\narXiv:2309.06419, 2023.\n[42] Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Wei Liu, Dinggang\nShen, Quanzheng Li, et al. Deid-gpt: Zero-shot medical text de-identification by gpt-4. arXiv preprint\narXiv:2303.11032, 2023.\n[43] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. Med-halt: Medical domain hallucination\ntest for large language models. arXiv preprint arXiv:2307.15343, 2023.\n[44] Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang,\nand Dinggang Shen. Chatcad+: Towards a universal and reliable interactive cad using llms. arXiv preprint\narXiv:2305.15964, 2023.\n[45] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative\npre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):bbac409,\n2022.\n[46] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin\nCompas, Cheryl Martin, Mona G Flores, Ying Zhang, et al. Gatortron: A large clinical language model to unlock\npatient information from unstructured electronic health records. arXiv preprint arXiv:2203.03540, 2022.\n[47] Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, and Sheng Yu. Biobart: Pretraining and\nevaluation of a biomedical generative language model. arXiv preprint arXiv:2204.03905, 2022.\n[48] Qiuhao Lu, Dejing Dou, and Thien Nguyen. Clinicalt5: A generative language model for clinical text. In\nFindings of the Association for Computational Linguistics: EMNLP 2022, pages 5436\u20135443, 2022.\n[49] Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang, and Fei Huang. Improving biomedical pretrained\nlanguage models with knowledge. arXiv preprint arXiv:2104.10344, 2021.\n[50] Desh Raj, Sunil Sahu, and Ashish Anand. Learning local and global contexts using a convolutional recurrent net-\nwork model for relation classification in biomedical text. In Proceedings of the 21st conference on computational\nnatural language learning (CoNLL 2017), pages 311\u2013321, 2017.\n[51] Chen Lyu, Bo Chen, Yafeng Ren, and Donghong Ji. Long short-term memory rnn for biomedical named entity\nrecognition. BMC bioinformatics, 18:1\u201311, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab3c3480-844f-4e77-9c1e-907ba0fafbed": {"__data__": {"id_": "ab3c3480-844f-4e77-9c1e-907ba0fafbed", "embedding": null, "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39743898-7152-4110-9bbe-f1b04afaf30e", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2652d48c1ac68dc06dcc9e8af6e7ba21d61746e6a779fae3618c69353b2eacf2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5141534c-1abd-445a-af2b-c22a2e89f5bb", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "23374ba6bb4cd372fe5498d4385e262355432ea7e626e130010bca09f0f1680a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[50] Desh Raj, Sunil Sahu, and Ashish Anand. Learning local and global contexts using a convolutional recurrent net-\nwork model for relation classification in biomedical text. In Proceedings of the 21st conference on computational\nnatural language learning (CoNLL 2017), pages 311\u2013321, 2017.\n[51] Chen Lyu, Bo Chen, Yafeng Ren, and Donghong Ji. Long short-term memory rnn for biomedical named entity\nrecognition. BMC bioinformatics, 18:1\u201311, 2017.\n[52] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L\nMcClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint\narXiv:2207.07051, 2022.\n[53] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature,\npages 1\u20139, 2023.\n[54] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Language models are few-shot learners for prognostic\nprediction. arXiv e-prints, pages arXiv\u20132302, 2023.\n[55] Vivian Weiwen Xue, Pinggui Lei, and William C Cho. The potential impact of chatgpt in clinical and translational\nmedicine. Clinical and Translational Medicine, 13(3), 2023.\n[56] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Boosting transformers and language models for clinical\nprediction in immunotherapy. arXiv preprint arXiv:2302.12692, 2023.\n[57] Hongyang Li, Richard C Gerkin, Alyssa Bakke, Raquel Norel, Guillermo Cecchi, Christophe Laudamiel,\nMasha Y Niv, Kathrin Ohla, John E Hayes, Valentina Parma, et al. Text-based predictions of covid-19 diagnosis\nfrom self-reported chemosensory descriptions. Communications Medicine, 3(1):104, 2023.\n[58] Chengsheng Mao, Jie Xu, Luke Rasmussen, Yikuan Li, Prakash Adekkanattu, Jennifer Pacheco, Borna Bonakdar-\npour, Robert Vassar, Li Shen, Guoqian Jiang, et al. Ad-bert: Using pre-trained language model to predict\nthe progression from mild cognitive impairment to alzheimer\u2019s disease. Journal of Biomedical Informatics,\n144:104442, 2023.\n18", "mimetype": "text/plain", "start_char_idx": 2572, "end_char_idx": 4652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "466e8572-6fdf-41f3-959b-f5c74172ff62": {"__data__": {"id_": "466e8572-6fdf-41f3-959b-f5c74172ff62", "embedding": null, "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae3a5731-6c14-4b81-a9d4-a4ed212cefbf", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e7d913ed31eba9bfaf0987c9e073cf10e58ee9f0d2b77e99b04dea63da60e71c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd8ceb3f-df48-44a4-aa83-b31cd4e2a76b", "node_type": "1", "metadata": {}, "hash": "89efb4ae692b68956c502972b2bb94666768d24f8ea0d456244407a53424ed08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[59] Felix Agbavor and Hualou Liang. Predicting dementia from spontaneous speech using large language models.\nPLOS Digital Health, 1(12):e0000168, 2022.\n[60] Desir\u00e9e Bill and Theodor Eriksson. Fine-tuning a llm using reinforcement learning from human feedback for a\ntherapy chatbot application, 2023.\n[61] Michael Balas and Edsel B Ing. Conversational ai models for ophthalmic diagnosis: Comparison of chatgpt and\nthe isabel pro differential diagnosis generator. JFO Open Ophthalmology, 1:100005, 2023.\n[62] Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, and Ziqi Wang. Psy-llm: Scaling up global\nmental health psychological services with ai-based large language models. arXiv preprint arXiv:2307.11991,\n2023.\n[63] Maham Bilal, Yumna Jamil, Dua Rana, and Hussain Haider Shah. Enhancing awareness and self-diagnosis of\nobstructive sleep apnea using ai-powered chatbots: The role of chatgpt in revolutionizing healthcare. Annals of\nBiomedical Engineering, pages 1\u20133, 2023.\n[64] Mohd Javaid, Abid Haleem, and Ravi Pratap Singh. Chatgpt for healthcare services: An emerging stage for an\ninnovative perspective. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 3(1):100105,\n2023.\n[65] Stephen R Ali, Thomas D Dobbs, Hayley A Hutchings, and Iain S Whitaker. Using chatgpt to write patient clinic\nletters. The Lancet Digital Health, 5(4):e179\u2013e181, 2023.\n[66] Josh Nguyen and Christopher A Pepping. The application of chatgpt in healthcare progress notes: A commentary\nfrom a clinical and research perspective. Clinical and Translational Medicine, 13(7), 2023.\n[67] Harriet Louise Walker, Shahi Ghani, Christoph Kuemmerli, Christian Andreas Nebiker, Beat Peter M\u00fcller,\nDimitri Aristotle Raptis, and Sebastian Manuel Staubli. Reliability of medical information provided by chatgpt:\nAssessment against clinical guidelines and patient information quality instrument. Journal of Medical Internet\nResearch, 25:e47479, 2023.\n[68] Linta Iftikhar et al. Docgpt: Impact of chatgpt-3 on health services as a virtual doctor. EC Paediatrics,\n12(1):45\u201355, 2023.\n[69] Hao Yang, Jiaxi Li, Siru Liu, Lei Du, Xiali Liu, Yong Huang, Qingke Shi, and Jialin Liu. Exploring the potential\nof large language models in personalized diabetes treatment strategies. medRxiv, pages 2023\u201306, 2023.\n[70] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided\ndiagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257, 2023.\n[71] Vera Sorin, Yiftach Barash, Eli Konen, and Eyal Klang. Large language models for oncological applications.\nJournal of Cancer Research and Clinical Oncology, pages 1\u20134, 2023.\n[72] Rubeta N Matin, Eleni Linos, and Neil Rajan. Leveraging large language models in dermatology, 2023.\n[73] Malik Sallam. The utility of chatgpt as an example of large language models in healthcare education, research\nand practice: Systematic review on the future perspectives and potential limitations. medRxiv, pages 2023\u201302,\n2023.\n[74] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin F Rousseau, et al. Evaluating large language models on medical evidence summarization.\nnpj Digital Medicine, 6(1):158, 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd8ceb3f-df48-44a4-aa83-b31cd4e2a76b": {"__data__": {"id_": "bd8ceb3f-df48-44a4-aa83-b31cd4e2a76b", "embedding": null, "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae3a5731-6c14-4b81-a9d4-a4ed212cefbf", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e7d913ed31eba9bfaf0987c9e073cf10e58ee9f0d2b77e99b04dea63da60e71c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "466e8572-6fdf-41f3-959b-f5c74172ff62", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "266f0c199bb3e796b586c8a09f354673ce85dbe0a78df41c56ac449ac546a4af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "medRxiv, pages 2023\u201302,\n2023.\n[74] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin F Rousseau, et al. Evaluating large language models on medical evidence summarization.\nnpj Digital Medicine, 6(1):158, 2023.\n[75] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Ruili Huang, and Weida Tong. Ai-based language\nmodels powering drug discovery and development. Drug Discovery Today, 26(11):2593\u20132607, 2021.\n[76] Tanmoy Tapos Datta, Pintu Chandra Shill, and Zabir Al Nazi. Bert-d2: Drug-drug interaction extraction using\nbert. In 2022 International Conference for Advancement in Technology (ICONAT), pages 1\u20136. IEEE, 2022.\n[77] Francesca Grisoni. Chemical language models for de novo drug design: Challenges and opportunities. Current\nOpinion in Structural Biology, 79:102527, 2023.\n[78] G\u00f6k\u00e7e Uludo\u02d8gan, Elif Ozkirimli, Kutlu O Ulgen, Nilg\u00fcn Karal\u0131, and Arzucan \u00d6zg\u00fcr. Exploiting pretrained\nbiochemical language models for targeted drug design. Bioinformatics, 38(Supplement_2):ii155\u2013ii161, 2022.\n[79] Lei Ma, Jincong Han, Zhaoxin Wang, and Dian Zhang. Cephgpt-4: An interactive multimodal cephalometric\nmeasurement and diagnostic system with visual large language model. arXiv preprint arXiv:2307.07518, 2023.\n[80] Firas Khader, Gustav Mueller-Franzes, Tianci Wang, Tianyu Han, Soroosh Tayebi Arasteh, Christoph Haarburger,\nJohannes Stegmaier, Keno Bressem, Christiane Kuhl, Sven Nebelung, et al. Medical diagnosis with large scale\nmultimodal transformers\u2013leveraging diverse data for more accurate diagnosis. arXiv preprint arXiv:2212.09162,\n2022.\n19", "mimetype": "text/plain", "start_char_idx": 3047, "end_char_idx": 4677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6761b260-4c9d-4627-8e00-76d7357d5b00": {"__data__": {"id_": "6761b260-4c9d-4627-8e00-76d7357d5b00", "embedding": null, "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6abc588-e112-4631-8d8e-215751aee4cf", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "159919ed9222f14e04e8dde84e60fab7453445ed0b249bd2195dfec7b2179093", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8748485-e96e-46d9-afc5-877eb3e6ff1e", "node_type": "1", "metadata": {}, "hash": "3b2c0c0682b60e68648cc97de34c073c1c091e0bfbe3de794f76c4287297ad3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[81] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer,\nSalman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using\nmedical vision-language models. arXiv preprint arXiv:2306.07971, 2023.\n[82] Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, and Zuozhu Liu. A chatgpt aided explainable\nframework for zero-shot medical image diagnosis. arXiv preprint arXiv:2307.01981, 2023.\n[83] Masoud Monajatipoor, Mozhdeh Rouhsedaghat, Liunian Harold Li, C-C Jay Kuo, Aichi Chien, and Kai-Wei\nChang. Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 725\u2013734. Springer, 2022.\n[84] Alireza Roshanzamir, Hamid Aghajan, and Mahdieh Soleymani Baghshah. Transformer-based deep neural\nnetwork language models for alzheimer\u2019s disease risk assessment from targeted speech.BMC Medical Informatics\nand Decision Making, 21:1\u201314, 2021.\n[85] John Giorgi, Augustin Toma, Ronald Xie, Sondra Chen, Kevin An, Grace Zheng, and Bo Wang. Wanglab at\nmediqa-chat 2023: Clinical note generation from doctor-patient conversations using large language models. In\nProceedings of the 5th Clinical Natural Language Processing Workshop, pages 323\u2013334, 2023.\n[86] Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, and Giorgos Papanastasiou. From explainable to\ninterpretable deep learning for natural language processing in healthcare: How far from reality? Computational\nand Structural Biotechnology Journal, 2024.\n[87] Hans-Christian Thorsen-Meyer, Davide Placido, Benjamin Skov Kaas-Hansen, Anna P Nielsen, Theis Lange,\nAnnelaura B Nielsen, Palle Toft, Jens Schierbeck, Thomas Str\u00f8m, Piotr J Chmura, et al. Discrete-time survival\nanalysis in the critically ill: a deep learning approach using heterogeneous data. NPJ digital medicine, 5(1):142,\n2022.\n[88] Alwin Yaoxian Zhang, Sean Shao Wei Lam, Marcus Eng Hock Ong, Phua Hwee Tang, and Ling Ling Chan.\nExplainable ai: classification of mri brain scans orders for quality improvement. In Proceedings of the 6th\nIEEE/ACM international conference on big data computing, applications and technologies, pages 95\u2013102, 2019.\n[89] Ozan Ozyegen, Devika Kabe, and Mucahit Cevik. Word-level text highlighting of medical texts for telehealth\nservices. Artificial Intelligence in Medicine, 127:102284, 2022.\n[90] Adam Gabriel Dobrakowski, Agnieszka Mykowiecka, Ma\u0142gorzata Marciniak, Wojciech Jaworski, and Prze-\nmys\u0142aw Biecek. Interpretable segmentation of medical free-text records based on word embeddings. Journal of\nIntelligent Information Systems, 57:447\u2013465, 2021.\n[91] Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M Churpek, and Majid Afshar.\nLeveraging a medical knowledge graph into large language models for diagnosis prediction. arXiv preprint\narXiv:2308.14321, 2023.\n[92] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards\ninterpretable mental health analysis with large language models. arXiv preprint arXiv:2304.03347, 2023.\n[93] Shengxin Hong, Liang Xiao, Xin Zhang, and Jianxia Chen.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8748485-e96e-46d9-afc5-877eb3e6ff1e": {"__data__": {"id_": "e8748485-e96e-46d9-afc5-877eb3e6ff1e", "embedding": null, "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6abc588-e112-4631-8d8e-215751aee4cf", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "159919ed9222f14e04e8dde84e60fab7453445ed0b249bd2195dfec7b2179093", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6761b260-4c9d-4627-8e00-76d7357d5b00", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "78d187aff6f8c91a0a89a4604da0b462abaf609ea522c8b4364bb5ee62b30ffe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Leveraging a medical knowledge graph into large language models for diagnosis prediction. arXiv preprint\narXiv:2308.14321, 2023.\n[92] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards\ninterpretable mental health analysis with large language models. arXiv preprint arXiv:2304.03347, 2023.\n[93] Shengxin Hong, Liang Xiao, Xin Zhang, and Jianxia Chen. Argmed-agents: Explainable clinical decision\nreasoning with large language models via argumentation schemes. arXiv preprint arXiv:2403.06294, 2024.\n[94] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Mentallama:\ninterpretable mental health analysis on social media with large language models. In Proceedings of the ACM on\nWeb Conference 2024, pages 4489\u20134500, 2024.\n[95] Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H Chen. Diagnostic reasoning\nprompts reveal the potential for large language model interpretability in medicine.NPJ Digital Medicine, 7(1):20,\n2024.\n[96] Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zuyong Zhang, Zhouyang Wang, Jie Zhang, Shuiguang Deng,\nand Jianwei Yin. Skingen: An explainable dermatology diagnosis-to-generation framework with interactive\nvision-language models. arXiv preprint arXiv:2404.14755, 2024.\n[97] Min Hun Lee and Chong Jun Chew. Understanding the effect of counterfactual explanations on trust and\nreliance on ai for human-ai collaborative clinical decision making. Proceedings of the ACM on Human-Computer\nInteraction, 7(CSCW2):1\u201322, 2023.\n[98] Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, and Byron C Wallace. Chill: zero-\nshot custom interpretable feature extraction from clinical notes with large language models. arXiv preprint\narXiv:2302.12343, 2023.\n[99] Usman Naseem, Matloob Khushi, and Jinman Kim. Vision-language transformer for interpretable pathology\nvisual question answering. IEEE Journal of Biomedical and Health Informatics, 27(4):1681\u20131690, 2022.\n20", "mimetype": "text/plain", "start_char_idx": 2881, "end_char_idx": 4881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56f96d72-fa67-410c-a3b4-d505986798af": {"__data__": {"id_": "56f96d72-fa67-410c-a3b4-d505986798af", "embedding": null, "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea3d537f-0394-4e79-b93f-be94bddbc93f", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f1cb8a7162d38b6b09b864658ece9c58c4dafa4528216b45e76ce7475cab08a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbfe7dda-ac00-4bd7-945d-86eab268758c", "node_type": "1", "metadata": {}, "hash": "75456f669499c901351deab2d04c48e6ce4e742f42b1d08d4cb8be70a99baecb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[100] S Park, G Kim, Y Oh, JB Seo, SM Lee, JH Kim, S Moon, JK Lim, and JC Ye. Vision transformer for covid-19\ncxr diagnosis using chest x-ray feature corpus. arxiv 2021. arXiv preprint arXiv:2103.07055.\n[101] Jie Pan. Large language model for molecular chemistry. Nature Computational Science, 3(1):5\u20135, 2023.\n[102] Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang. Online\ntraining of large language models: Learn while chatting. arXiv preprint arXiv:2403.04790, 2024.\n[103] Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S Sheng, Huaiyu Dai, and Dejing Dou.\nFederated learning of large language models with parameter-efficient prompt tuning and adaptive optimization.\narXiv preprint arXiv:2310.15080, 2023.\n[104] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and\nMengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and\nTechnology, 15(2):1\u201338, 2024.\n[105] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm: Large language\nmodels for health prediction via wearable sensor data. arXiv preprint arXiv:2401.06866, 2024.\n[106] Saurabh Pahune and Noopur Rewatkar. Large language models and generative ai\u2019s expanding role in healthcare.\n2024.\n[107] Sandeep Reddy, Wendy Rogers, Ville-Petteri Makinen, Enrico Coiera, Pieta Brown, Markus Wenzel, Eva\nWeicken, Saba Ansari, Piyush Mathur, Aaron Casey, et al. Evaluation framework to guide implementation of ai\nsystems into healthcare settings. BMJ health & care informatics, 28(1), 2021.\n[108] Sandeep Reddy. Evaluating large language models for use in healthcare: A framework for translational value\nassessment. Informatics in Medicine Unlocked, page 101304, 2023.\n[109] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403,\n2023.\n[110] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n[111] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[112] Klu AI. Mmlu benchmark (massive multi-task language understanding), 2024. Accessed: 2024-07-08.\n[113] Qiao Jin, Bhuwan Dhingra, William W Cohen, and Xinghua Lu. Probing biomedical embeddings from language\nmodels. arXiv preprint arXiv:1904.02181, 2019.\n[114] Andreas Mayr, G\u00fcnter Klambauer, Thomas Unterthiner, and Sepp Hochreiter. Deeptox: toxicity prediction using\ndeep learning. Frontiers in Environmental Science, 3:80, 2016.\n[115] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbfe7dda-ac00-4bd7-945d-86eab268758c": {"__data__": {"id_": "fbfe7dda-ac00-4bd7-945d-86eab268758c", "embedding": null, "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea3d537f-0394-4e79-b93f-be94bddbc93f", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f1cb8a7162d38b6b09b864658ece9c58c4dafa4528216b45e76ce7475cab08a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56f96d72-fa67-410c-a3b4-d505986798af", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4ee02bd5907fb75d900046b956a151f7e8c302322ec91d3d76311827d62ac23b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Probing biomedical embeddings from language\nmodels. arXiv preprint arXiv:1904.02181, 2019.\n[114] Andreas Mayr, G\u00fcnter Klambauer, Thomas Unterthiner, and Sepp Hochreiter. Deeptox: toxicity prediction using\ndeep learning. Frontiers in Environmental Science, 3:80, 2016.\n[115] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A dataset for\nbiomedical research question answering. arXiv preprint arXiv:1909.06146, 2019.\n[116] Papers with Code. Medical papers with code, 2024. Accessed: 2024-07-08.\n[117] Jonghyun Lee, In-Soo Myeong, and Yun Kim. The drug-like molecule pre-training strategy for drug discovery.\nIEEE Access, 11:61680\u201361687, 2023.\n[118] Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang\nZhu, Hongmin Cai, Tianming Liu, et al. Differentiate chatgpt-generated and human-written medical texts. arXiv\npreprint arXiv:2304.11567, 2023.\n[119] Andrea Manoel, Mirian del Carmen Hipolito Garcia, Tal Baumel, Shize Su, Jialei Chen, Robert Sim, Dan Miller,\nDanny Karmon, and Dimitrios Dimitriadis. Federated multilingual models for medical transcript analysis. In\nConference on Health, Inference, and Learning, pages 147\u2013162. PMLR, 2023.\n[120] Yuhui Zhang, Allen Nie, Ashley Zehnder, Rodney L Page, and James Zou. Vettag: improving automated\nveterinary diagnosis coding via large-scale language modeling. NPJ digital medicine, 2(1):35, 2019.\n[121] Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. Clinicalgpt: Large language models\nfinetuned with diverse medical data and comprehensive evaluation. arXiv preprint arXiv:2306.09968, 2023.\n[122] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and\nBenyou Wang. Huatuo-26m, a large-scale chinese medical qa dataset. arXiv preprint arXiv:2305.01526, 2023.\n21", "mimetype": "text/plain", "start_char_idx": 2724, "end_char_idx": 4583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3c99cd2-cbbc-40b0-99f6-827b49d746af": {"__data__": {"id_": "f3c99cd2-cbbc-40b0-99f6-827b49d746af", "embedding": null, "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfcb46db-f2d8-4946-ade8-08c13a3def5c", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1cd9615f78ec2e6b081dbee5b313f6360b74db233cb3490389b51fedebe7daff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[123] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin\nCompas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health\nrecords. NPJ Digital Medicine, 5(1):194, 2022.\n[124] Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundar\u00f2,\nMarina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, et al. Advancing italian biomedical\ninformation extraction with large language models: Methodological insights and multicenter practical application.\narXiv preprint arXiv:2306.05323, 2023.\n[125] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675, 2019.\n[126] Brendin R Beaulieu-Jones, Sahaj Shah, Margaret T Berrigan, Jayson S Marwaha, Shuo-Lun Lai, and Gabriel A\nBrat. Evaluating capabilities of large language models: Performance of gpt4 on surgical knowledge assessments.\nmedRxiv, pages 2023\u201307, 2023.\n[127] Hazrat Ali, Junaid Qadir, Tanvir Alam, Mowafa Househ, and Zubair Shah. Chatgpt and large language models\n(llms) in healthcare: Opportunities and risks. 2023.\n[128] Giovanni Briganti. A clinician\u2019s guide to large language models. Future Medicine AI, (0):FMAI, 2023.\n[129] Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Pietro Lio, and Andrija Petrovic.\nInterpretable medical diagnostics with structured data extraction by large language models. arXiv preprint\narXiv:2306.05052, 2023.\n[130] Yan Jiang, Ruihong Qiu, Yi Zhang, and Peng-Fei Zhang. Balanced and explainable social media analysis for\npublic health with large language models. arXiv preprint arXiv:2309.05951, 2023.\n[131] Jesutofunmi A Omiye, Haiwen Gui, Shawheen J Rezaei, James Zou, and Roxana Daneshjou. Large language\nmodels in medicine: the potentials and pitfalls. arXiv preprint arXiv:2309.00087, 2023.\n[132] Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, bard, and large language models for biomedical research:\nOpportunities and pitfalls. Annals of Biomedical Engineering, pages 1\u20135, 2023.\n[133] Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen,\nWon Kim, Donald C Comeau, et al. Opportunities and challenges for chatgpt and large language models in\nbiomedicine and health. arXiv preprint arXiv:2306.10070, 2023.\n[134] Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, and Luciano Floridi. Generative ai in eu\nlaw: liability, privacy, intellectual property, and cybersecurity.arXiv preprint arXiv:2401.07348, 2024.\n[135] Philipp Hacker, Andreas Engel, and Marco Mauer. Regulating chatgpt and other large generative ai models. In\nProceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 1112\u20131123,\n2023.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77133867-0d0a-4b9c-92f0-59cab680b66b": {"__data__": {"id_": "77133867-0d0a-4b9c-92f0-59cab680b66b", "embedding": null, "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd563c1c-eb7d-4d1c-8720-86d9417a99dd", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0faaa9c20b04d83620bfba436681d4f48b0ff37dc61cc21cb2a1b95623780f03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Developing Healthcare Language Model Embedding Spaces\nNiall Taylora, Dan Schofieldb, Andrey Kormilitzina, Dan W Joycec, Alejo\nNevado-Holgadoa\naDepartment of Psychiatry, University of Oxford, Oxford, United Kingdom\nbTransformation Directorate, NHS England, Leeds, United Kingdom\ncDepartment of Primary Care and Mental Health, University of Liverpool, Liverpool, United Kingdom\nAbstract\nPre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets\nlike healthcare focused text. We explore specialized pre-training to adapt smaller LLMs\nto different healthcare datasets. Three methods are assessed: traditional masked lan-\nguage modeling, Deep Contrastive Learning for Unsupervised Textual Representations\n(DeCLUTR), and a novel pre-training objective utilizing metadata categories from the\nhealthcare settings. These schemes are evaluated on downstream document classifica-\ntion tasks for each dataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification tasks,\ndelivering strong performance from limited labeled data and with fewer model parameter\nupdates required. While metadata-based pre-training does not further improve classifica-\ntions across the datasets, it yields interesting embedding cluster separability. All domain\nadapted LLMs outperform their publicly available general base LLM, validating the\nimportance of domain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational budgets,\nan essential capability for responsible and sustainable deployment in local healthcare\nsettings. We provide pre-training guidelines for specialized healthcare LLMs, motivate\ncontinued inquiry into contrastive objectives, and demonstrates adaptation techniques to\nalign small LLMs with privacy-sensitive medical tasks.\n1. Introduction\nLarge Language Models1 (LLMs) such as the Bidirectional Encoder Representa-\ntion Transformer (BERT) models based on masked language modelling (MLM), and\nGenerative Pretrained Transformers (GPT) models based on autoregressive language\nmodelling, have changed the research landscape entirely [1, 2, 3], and are generally seen\nas offering state of the art results on a number of popular Natural Language Processing\n(NLP) benchmark datasets and tasks [1, 3, 4, 5]. The type of task different LLMs excel\nin generally depends on the architecture and pre-training objective: BERT-like LLMs,\ntypically perform very well at embedding focused tasks like text classification, clustering\n1LLMs refers to all relevant Pre-trained Language Models (PLMs) across all scales\nPreprint submitted to Elsevier April 1, 2024\narXiv:2403.19802v1  [cs.CL]  28 Mar 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2854761d-7fcd-4132-a526-e311a724336c": {"__data__": {"id_": "2854761d-7fcd-4132-a526-e311a724336c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57adcca6-efe8-4cda-9aa3-f06f3e58f3b1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "53a3d6d42b7a7ecbfcc50a123c230be73f0e68fe785e1a9eb547623963b92682", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and retrieval [5]. Generative LLMs excel in generative tasks and form foundation for\nmany chat-based APIs dominating the artificial intelligence (AI) landscape.\nOne common problem across LLMs, both embedding and generative focused LLMs,\nis a drop in performance on text data or NLP tasks from a specific domain (e.g. health-\ncare) to the one that the model was originally trained on [6, 7, 8, 9, 10, 11, 12]. Aligning\nopen LLMs with new domains and tasks remains a considerable issue, in particular\nfor private datasets which remain unseen by open LLMs, e.g. UK National Health\nService (NHS) based clinical datasets. Firstly, the LLM needs to be fine-tuned for each\nspecific domain and task, and secondly, the LLM will suffer catastrophic forgetting,\nwhere it loses performance on other data and tasks in which it was originally trained.\nAn intermediate approach to avoid these problems is to pre-train language models to\nproducing good text embeddings that align well with possible downstream tasks to\nreduce the need for further fine-tuning, and thus improve training efficiency. Whilst\ncertain approaches, such as Retrieval Augmented Generation (RAG), are becoming\npopular in attempting to imbue generally trained LLMs with external knowledge sources\nthrough embeddings, and thus reduce the need for domain adaption, the ability of the\nunderlying LLMs involved is still often reliant on its domain pre-training. Moreover,\nthe scale of the state of the art generative LLMs used today has become prohibitively\nlarge, whereas embedding focused LLMs remain much smaller, cheaper and efficient\nfor many traditional NLP tasks.\nIn this paper, we explore the efficacy of altering the pre-training of smaller, BERT-\nlike LLMs to align with the healthcare2 domain and downstream tasks of interest. The\nutility of small and resource-efficient LLMs for the healthcare domain is especially\nattractive where budget for compute may be limited and training on private data is\nrequired. Linked to this problem is the inability to utilise many cloud-based APIs with\nprivate data due to data governance laws and issues of confidentiality and security.\n1.1. Document-level label-free embeddings\nStandard language modelling objectives (e.g. autoregressive, masked language\nmodelling) work on a word or token level, with the aim of representing words based on\nthe context they appear in. For example, the LLM objective is operating on a token level\nloss objective, and embeddings are produced per token of the input sequence. Often\nthe resultant token level embeddings appear performant for the task at hand, but many\ndownstream tasks do not operate on the token level, but rather the sentence or entire\ndocument level e.g. document sentiment analysis, information retrieval, and similarity\nmatching: of which LLMs are consistently state of the art. From a BERT-like LLM, a\ndocument level representation can be achieved in a number of ways based on the token\nlevel embeddings produced, with a common method being mean pooling i.e. the average\nof all token embedding representations of the last transformer layer [1, 13, 14].\nA potential issue with this relatively crude approximation of a document level\nembedding is that the Language Model (LM) pre-training objective did not encourage\nthese representations to be useful or have a clear relationship to the loss function directly.\n2we used the term healthcare to cover the more general NHS patient safety reports dataset used in this\nwork that is not strictly an EHR\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b79d2240-b1ee-4f24-ba86-1cae3998984d": {"__data__": {"id_": "b79d2240-b1ee-4f24-ba86-1cae3998984d", "embedding": null, "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4687383f-a972-47c8-a019-4d4c807c983b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "90379fc694380adf421087316f0c17f4c9f84d5f1f87ae1e15646defa75e7ac2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A number of approaches have been explored to better train these LMs to produce\nsentence level embeddings through alignment tasks [ 14, 15, 16]. It is common to\nutilise contrastive learning loss functions during pre-training to cluster embeddings from\nsentences or documents that are of the same type or even from the same global document\ncloser together than those that are not. These approaches now incorporate a loss that\nis directly operating on the sentence or document level embedding and subsequently\nencourages the model to produce token embeddings that influence the resultant averaged\nembedding and embedding space. A potential difficulty for contrastive learning is\nderiving class labels that allow the creation of samples of positive and negative pairs\nthat most contrastive loss functions require. A common approach relies upon a dataset\u2019s\nknown labels, such as Natural Language Inference (NLI) datasets used in the training of\nsentence transformers[14].\nHowever, large labelled healthcare text datasets are rare and for the datasets explored\nin this paper we do not have extensive or completely labelled data. For this reason,\nwe focus on methods that can use readily available structured meta data or use an\nunsupervised training regime, so no extra label annotations are required.\n1.2. The Healthcare Text Domain\nThere are two related, but distinct, types of text that are often merged as a single\ncategory in the machine learning literature. These are \"clinical text\", defined as this one\nwritten by clinical professionals in Electronic Health Records (EHRs); and \"biomedical\ntext\", this one written by researchers in scientific papers or books. The clinical domain,\nas opposed to the biomedical domain, is particularly difficult for general purpose LLMs.\nThis is often thought to be due to the complexity and idiosyncrasies of the language used\nin clinical practice that inevitably reflects regional- or specialty-specific nomenclature.\nFor instance, abbreviations are over-prevalent and often non-intuitive (e.g. using\"#\" for\nbroken bone, \"bds\" for twice a day, and many others [17]), and grammar and syntax is\nused sparingly in favour of faster writing. In contrast, the biomedical research literature\ntends to enforce a more consistent and agreed upon vocabulary, text follows a formal\nstyle, and descriptions and arguments try to be complete and analytic.\nThere are several techniques to mitigate the problems that arise from merging these\ntwo domains, and these solutions typically rely on a form of transfer learning or domain\nadaptation. One typical approach is to continue training the LLM in the target specialist\ndomain using the same language modelling objective, to better prepare the model for\ndeployment in the new domain, and has delivered promising results for US based clinical\ndatasets [6, 7, 18].\nIn this paper, we sought to compare and contrast the creation of embedding models\nin different healthcare datasets and in particular, two UK NHS datasets, and compare\nwith general LLM alternatives.\n1.2.1. The UK NHS\nEven in well-constrained use cases, such as pharmaco-vigilance for medication\nadverse events, clinical language patterns, idioms and idiosyncrasies in Electronic Health\nRecord (EHR) data are notoriously difficult to work with [19]. Similarly, consultation\nwith clinical colleagues in the NHS suggests that the routine clinical language recorded\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d4e170a-3d04-4d74-a1fd-5aadb35dc4c1": {"__data__": {"id_": "1d4e170a-3d04-4d74-a1fd-5aadb35dc4c1", "embedding": null, "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0973131-09f0-4f1c-96bc-9391ea9d1a8b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6bdda40c56cf0a93145f3c794892373283048123ae764e5eb98fc8f22928422f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in EHR systems in the UK differs substantially from that used in large open-source\nmedical text datasets. For instance, \"A&E\" is sometimes used in the UK to denote\nthe emergency department, while \"ED\" is used in the US and is more common in\nopen-sourced datasets. In addition, the text of open-source medical datasets appears\nto consist of the milder examples that are closer to normal writing, and does not use\nabbreviations and grammatical transgressions as seen in the much more specialist text\nseen in actual EHRs. This can severely limit the deployment of current popular medical\nLLMs[6, 20], which are often trained on these datasets. Whilst numerous biomedical or\nclinically trained LLMs exist, no publicly available LLMs exist for UK-based \u201cNHS\nlanguage\u201d and most use data from the United States.\nIn this paper we investigate the influence that different LM pre-training objectives\nhave on the performance of embeddings in downstream tasks. To make this investigation\nsystematic, we examined three pre-training methods across three healthcare datasets,\nand inspected the results using dataset-specific sequence classification tasks, embedding\ndistance metrics and qualitative analyses. Overall, we aim to ascertain what benefits\ndifferent pre-training methods provide when re-using general-domain LLMs to the\nclinical domain, and in particular to NHS datasets, such that deployment becomes more\namenable than in prior work.\n1.3. Motivation and Related Work\nImproving the embeddings representation of LLMs in the general domain has been\nstudied extensively and influences this work directly: notably the Deep Contrastive\nLearning for Unsupervised Textual Representations (DeCLUTR) paper [15], sentence\ntransformers [14] and SimCSE [ 21] works show great promise in improving BERT-\nstyle LLMs representation of sentences and documents through contrastive methods.\nSimilar works investigating the augmentation and changing of the LLM pre-training\nobjectives with external knowledge-graphs include BioLinkBERT [22] and Dragon [23].\nSAPBert sought to improve named-entity-recognition and linkage through alignment of\nembeddings for entity synonyms [24].\nRecently, there has also been promising work showing how task specific contrastive\nloss functions combined with sentence transformers can adapt LLMs to downstream\ntasks with relatively few training samples [ 9]. Work has also sought to enhance the\nability for generative LLMs to produce both good embeddings and generation using a\ncombination of instruction tuning and embedding loss functions [5], however this still\nrelied on comparatively larger models (e.g. those with more than 7 billion parameters).\nThe key contributions of this work are the introduction of a note category pre-\ntraining objective, the development of several LLMs for different NHS datasets, and the\nexploration of resource constrained pre-training and downstream adaptation.\n2. Methods\n2.1. Datasets and downstream tasks\nAs is typical, to explore the effect of different pre-training methods on the LLMs\nwe sought to evaluate them on a set of dataset specific downstream tasks. The focus\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "852557fd-3301-4bfe-a61d-ca82c43e9c5f": {"__data__": {"id_": "852557fd-3301-4bfe-a61d-ca82c43e9c5f", "embedding": null, "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "236161d0-b7e8-49f0-bf54-b678f399062d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8a06cf0ce611492a698b7d42adc762769af83270707be6a2d6fa06bbc5fcf21f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "was primarily on the differences in document-wide embeddings dependent on the pre-\ntraining received. Thus we opted to derive document classification tasks using the\ncategorical variables available for each of the datasets, with a focus on potential clinical\nuse-cases. The specific tasks available for each dataset are outlined below, with an\noverview of the data distributions and their respective downstream task is provided in\nTable 1. For brevity, all the downstream tasks are document classification tasks whereby\na document is to be assigned a particular class label.\n2.1.1. MIMIC-III\nThe first dataset we used is the Medical Information Mart for Intensive Care III\n(MIMIC-III) [25], a medical dataset developed by the MIT Lab for Computational\nPhysiology. It is comprised of de-identified EHR records associated with 38,597\ncritical care patients and 58,976 intensive care unit (ICU) admissions at the Beth\nIsrael Deaconess Medical Center between 2001 and 2012. Data includes demographics,\nvital signs, laboratory tests, medications, caregiver notes, imaging reports, and mortality\nin and out of hospital. While the clinical tasks presented here may benefit from utilising\nthe multi-modal data available for each patient, we focus on the use of free text clinical\nnotes.\nICD-9 Triage (M-Tri)\nWe utilise the ICD-9 codes associated with discharge summaries in the MIMIC-\nIII dataset to derive a triage classification task, originally published in our previous\nwork [26]. The motivation for this task is to represent a realistic use-case within a\nhospital setting, whereby patients admitted to an ICU will be treated and then \u201cstepped\ndown\u201d (discharged) to another ward or team to continue treatment when they no longer\nrequire an ICU. The result is a mapping between particular ICD-9 diagnosis codes\nand a corresponding destination department, of which we derive the following seven\npost-ICU destination teams: Cardiology, Obstetrics, Respiratory Medicine, Neurology,\nGastroenterology, Acute or Internal Medicine, and Oncology. For further details, see\noriginal implementation details in [26].\n2.1.2. Oxford Health Foundation Trust - OHFT\nThe second dataset is from the Oxford Health NHS Foundation Trust (OHFT), a\nregional UK-based provider of specialist mental healthcare covering Oxfordshire and\nBuckinghamshire. The dataset contains full historical EHR data for approximately\n200,000 patients spanning over a decade, and within this access to around 8 million\nde-identified clinical notes.\nTriage Team Association (O-Tri)\nIn the UK, mental health services are structured into primary, secondary, and tertiary\nlevels. Most patients (96%) needing specialist care are referred to and treated by\ncommunity mental health teams (CMHTs) [27]. Referrals contain information written\nby the referring doctor or professional and are triaged by the receiving CMHT into: a)\naccept to the team for assessment, b) reject due to insufficient information, or c) route to\na sub-specialty team if warranted.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d33cb7a8-f93c-4212-8850-e68009306bfe": {"__data__": {"id_": "d33cb7a8-f93c-4212-8850-e68009306bfe", "embedding": null, "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9169ea3-c2a1-4cc7-810e-5c4a84f6730d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a29c6adf17855516b76892080f87093762c064268a7a4e603372fbca994619a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured EHR data on referral and discharge dates establishes which team accepted\nthe patient, though local administrative variations required developing supplementary\nheuristics in collaboration with OHFT clinicians. We created a classification task to\nidentify the accepting triage team for a given referral from the subset of accepted\nreferrals. Specifically, given a random clinical note, the task is to determine which\nreferral team it likely belongs to based solely on its free text contents.\n2.1.3. NHS Patient Safety Incident Reports - PSIR\nThe third dataset is a large, national collection of free-text documents relating\nto all types of patient safety incidents in the NHS, from the National Reporting and\nLearning System (NRLS). More information about the dataset and collection can be\nfound on the official website [28]. We worked with a sub-sample of approximately 2.3\nmillion de-identified reports produced in the financial year 2019/2020, with the goal of\nsupporting more efficient analysis of the dataset within NHS England, and integration\nof the learning into the newer Learn from Patient Safety Events (LFPSE) service.\nIN05 Level 1 - Incident Category (P-Cat)\nThe labels of incident category at the first level are a standard field provided along-\nside each incident report which is used to detail the type of the incident. There are 13\nclasses present in the dataset, and we have chosen not to include a second level of labels\nwhich is included in the wider NRLS dataset.\nPD09 - Incident Degree of Harm (P-Sev)\nThe degree of incident severity categories are an ordinal scale ranging from no harm\n(1) to death (5) collected for all incidents. We simplify a pseudo task related to incidence\nseverity prediction in the following way: the 1-5 incident severity labels given with the\nfree text reports are skewed, with the vast majority being attributed a 1 (or no harm). We\ncreate a much smaller and balanced binary classification dataset which bins incidence\nlabels into low (severity categories 1-3) and high (severity categories 4-5) severity.\n2.1.4. Note Category - All Datasets\nThe various origins and purposes of these clinical notes are partially captured by\ntheir note category assignment (or associated metadata). These categories can be be\nseen as a form of structured knowledge pertaining to the organisation of the part of the\nhealthcare system of focus. This is a commonly captured field in healthcare datasets\nand while the exact use and meaning of this may differ between the respective datasets,\nbroadly each dataset contains this field to record the professional role of the person who\nproduced the note or the departmental origin. We expect that note category may be a\nvaluable signal for the pre-training itself because, for example:\n\u2022 a note entered by a social worker will contain terminology and describe concepts\nrelevant to a patient\u2019s social circumstances\n\u2022 a note entered by an occupational therapist will emphasise functioning and the\npatient\u2019s capacity for everyday tasks\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c2cdfda8-7a32-4701-abb9-5380ff5418c7": {"__data__": {"id_": "c2cdfda8-7a32-4701-abb9-5380ff5418c7", "embedding": null, "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e9109f1-c1e6-4869-aa72-8a949997379a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fe08befd506b1acc4b2c0d385c4592707530b8709c745155daf4c36589becd9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 a note entered by a physician will emphasise clinical state, examinations and\ntreatment plans\n\u2022 a note entered by a care coordinator will reflect progress on executing a manage-\nment/care plan for the patient\nTherefore, embeddings arising from the contents of documents with different note\ncategories should capture the vocabulary, concepts and semantics of information rou-\ntinely recorded by different healthcare professionals and their intended use. In each\nof our dataset we identify a note category: for both MIMIC-III and OHFT, the note\ncategory reflects the clinical purpose of the document that relates to the profession\nof the individual making the note e.g. Nursing, Doctor, Social worker. Whereas for\nPSIR, the category variable (RP02 on the database) reflects the care setting in which the\npatient safety incident occurred such as: Acute / General Hospital, various Community\ngroupings, and General Practice.\n2.2. Data splits\nFor each of our datasets there are a substantial number of available clinical docu-\nments, even in the smallest there are over 2 million individual samples. To facilitate the\ndevelopment of multiple models and experiments within a resource restricted setting, we\ndeveloped and trained initial pipelines at various smaller scales. We then chose to train\nand test models on a maximum sub-sample of 250,000 documents as this would be large\nenough to show trends in the data, whilst keeping resource utilisation to a manageable\nlevel. Furthermore, where possible we utilised the unique patient identifiers to ensure\nno individuals data was both in the training and evaluation sets. Future work could\nseek to train and evaluate with a larger amount of data and focus on the magnitude of\ndifferences in results found.\nLength of documents\nAn important feature of a text dataset is the number of words, or tokens, contained\nin each individual sample (document). Transformer based language models such as\nRoBERTa[29] can only handle a maximum of 512 tokens on even the most modern\nGPUs, due to the complexity of the self-attention calculations which increases expo-\nnentially with the number of tokens [ 30]. The average document length, as well as\nthe distribution of lengths, varies considerably in our three datasets, although a large\nportion fit within the maximum sequence length for our chosen models. As is standard,\ndocuments that exceed the maximum token length are truncated. Whilst numerous\napproaches have been developed to mitigate this problem, such as introducing local\nsliding windows [30], key-value caching [31], and flash attention [32], we opt to focus\nonly on the standard transformer attention used by RoBERTa. We thus limit the context\nwindow to 512 tokens, which we see as sufficient for the investigations presented in\nthis work. For more details of the document lengths for each respective dataset, see\nAppendix A.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "973fc9e2-5db9-43a0-a1a7-ae658d8f1055": {"__data__": {"id_": "973fc9e2-5db9-43a0-a1a7-ae658d8f1055", "embedding": null, "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d90cf593-b9a6-4f03-ac91-a58dd0075c86", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ad632c393fd09fdbec2ea61073855ddd2189f139c5abd6a8b35417a571736f57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Downstream classification dataset statistics. Task names are also given short-hand codes used\nthroughout the rest of the paper to better fit tables and figures.*P-Cat is not the note category used in\npre-training for the PSIR dataset\nDataset Task (Acronym) # labels # train samples # test samples\nMimic-III Note category (M-Cat) 8 1,600 4,000\nMimic-III ICD-9 Triage (M-Tri) 7 1,400 3,150\nOHFT Note category (O-Cat) 10 10,000 2,500\nOHFT Referral team relation (O-Tri) 5 6,250 2,500\nPSIR IN05: Category (P-Cat) 13 26,000 2,600\nPSIR PD09: Severity (P-Sev) 2 14,000 14,000\n2.3. Language modelling - Preliminaries\nThe pre-training of a standard LM involves a corpus of D text documents X =\n{X1, X2, ..., XD} and two functions, fenc and fhead. Each document d consists of\na sequence of T tokens Xd = (x1, x2, x3...xT ) which is first passed through fenc to\nproduce a contextualised vector for each token, Hd = (h1, h2, h3...hT ). fhead then uses\nHd for whichever self-supervised pre-training task chosen and to perform subsequent\ndownstream tasks during fine-tuning. Because T may vary from sample d to sample d\u2032,\nto represent the whole sequence of T tokens as a fixed-length vector, as is required for\nmany tasks, we used a pooling function g to take the mean of all token level embeddings\nof the sample, Hd = (h1, h2, ..., hT ), which has proven a reasonable approach to\nrepresent a sequence of text [15, 14]. Accordingly, whole sequence embeddings ed =\ng(fenc(Hd)).\n2.3.1. Continued Masked Language Modelling\nThe first pre-training method is the standard formula for MLM, a commonly used\nobjective for pre-training language models that randomly replaces ormasks a proportion\nof tokens of the input with a special [MASK ] token. This essentially corrupts the\noriginal input and the objective of the model is to predict which tokens should appear in\nthe masked positions, a form of gap filling.\nThe standard MLM loss function is given as follows:\nLmlm(X, Y) =\u2212\nNcX\nn=1\nWn(\n|V |X\ni=1\nY n\ni ln(flm(X)n\ni )) (1)\nwhere X is the input of the model, Y denotes MLM labels which is a collection of Nc\none-hot vectors each with the size of |V | where |V | is the size of the vocabulary of the\nmodel and N is the number of input tokens3 and Wn is 1 for masked tokens and 0 for\nothers. This ensures that only masked tokens will contribute to the computation of loss.\n3Note that one-hot vectors for non-masked tokens are zero vectors.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa700a76-ff9d-4ee1-a345-aad0ce7aaff2": {"__data__": {"id_": "fa700a76-ff9d-4ee1-a345-aad0ce7aaff2", "embedding": null, "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0023bd73-95d5-4673-a31f-5f22ae536f29", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5770275cd70130128d6baca3758ecb8b17218e0bae532c6722c2aba9afadf9f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "flm represents the encoder model with a language modelling head whose output is a\nprobability distribution vector with the size of the vocabulary (|V |) for each token.\n2.3.2. Contrastive Loss Pre-training\nA common approach to improving separation of classes in an embedding space\nutilises contrastive learning, which uses a loss function that aims to encourage semanti-\ncally close or same-class members whilst pushing apart other-class members [33]. It\nassumes a set of paired examples P = {(Xi, X+\nj )}P\ni,j=1, where P are the number of\nsamples, and documents Xi and Xj are semantically related documents. The derivation\nof paired examples is the important, and arguably most difficult part when well-defined\nlabels are not present. We therefore first opt to utilise methods that are unsupervised or\nself-supervised:\nDeCLUTR\nAs we want to generate a model that can produce good document level embeddings,\nwe also explored an self-supervised cluster alignment technique used to produce the De-\nCLUTR model [15]. DeCLUTR stands for Deep Contrastive Learning for Unsupervised\nTextual Representations, and uses a contrastive loss function to encourage sentence or\ndocument level embeddings that are taken from the same document type or class to be\ncloser together in the learned embedding space. DeCLUTR utilises a self-supervised\ncontrastive loss function called InfoNCE (Noise-Contrastive Estimation) which aims to\nidentify positive pairs in a set of samples also containing numerous negatives.\nSampled anchor\nSampled positive\nsi\nsj\nd\nLLM fenc\nMinimise \ndistance\nPooling - g\nLLM fenc ei\nPooling - g\nsj\nShared weights\nFigure 1: Adapted from [15]. Overview of the DeCLUTR training process. We sample anchor spans si and\npositive spans sj from each document d in a minibatch of size N. For simplicity, we show A = P = 1,\nwhere A and P are the number of anchors and positives per document. The spans are encoded by fenc() and\npooled by g(\u00b7) to get embeddings ei = g(f(si)) and ej = g(f(sj)). The encoder and pooler are trained to\nminimize the distance between positive span pairs while maximizing the distance to negatives (omitted for\nsimplicity).\nDuring training, a batch of P anchor-positive span pairs is taken from document\nd. Each of the spans are separately encoded. The anchor and positive embeddings,\nsi and sj, are then compared for their cosine similarity by taking their dot product.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e894968-ed5d-4a49-8f6a-1feb00651611": {"__data__": {"id_": "4e894968-ed5d-4a49-8f6a-1feb00651611", "embedding": null, "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c662354f-eed1-457f-97ae-7d853352ce78", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bf50a0bcf6519768147c7d78e33e6b554f6f59646ed0d1350fbd2d5ae8d34820", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The objective of the training procedure is to maximise the cosine similarity of the P\nmatching anchor-positive pairs and minimise that of the remainingP2 \u2212P non-matching\npairs. For a given batch, the cosine similarities are then used to calculate the probability\nthat a given pair is a match, which can be defined as:\nP(si, sj; \u03c4) = exp(si \u00b7 sj/\u03c4)P\nk\u0338=i,j exp(si \u00b7 zk/\u03c4) (2)\nwhere \u03c4 is a trainable temperature parameter. This results in the InfoNCE loss which\nsymmetrically measures the success in maximising the similarity of matches and min-\nimising the similarity of non-matches, defined as:\nLInfoNCE = \u22121\n2\n\"\n1\nP\nPX\ni,j=0\nlog P(si, sj; \u03c4) + 1\nP\nPX\ni,j=0\nlog P(sj, si; \u03c4)\n#\n(3)\nDeCLUTR Data Sampling. For the DeCLUTR models, we initially only manipulated\nthe sampling strategy, through changing the maximum length of documents spans to be\nused for the creation of anchor-positive pairs, in our experiments. We kept the actual\nsampling methodology the same. As per the original paper, the minimum number of\ntokens for a document to be sampled is:2\u00d7A\u00d7Smax where A is the number of anchors\nto be sampled, and Smax is the maximum span length. For example, the minimum span\nlength for a document with 2 anchors and a maximum span length of 512 would be\n2048.\nThe span length is thus a key parameter when training using the DeCLUTR regime\nand we have not been able to exhaustively explore all possible combinations. We\ninstead opted for a span length that aligned with our average document length in each\ndataset. The sampling strategy for generating anchors and positive pairs was also fixed\nto adjacent across all datasets. We recognise that future work could seek to explore this\nspace further. Examples of the sample distributions based on the minimum document\nlength for each dataset are presented in the table in Appendix H.\nNote Category as a pre-training signal\nAs a third pre-training method, we utilised a known categorical variable which\nappears in some form across all three datasets, the \u201cnote category\u201d of the document as\ndescribed in Section 2.1.4.\nWe formulate this task as a replacement of the original next sentence prediction\ntask used in BERTs implementation [1]. Whole sequence embeddings, e, are fed to a\nclassification head fhead(\u00b7), which has the task of calculating the logits yj of each of\nthe possible c classes j \u2208 C. A softmax operation \u03c3 is applied to the logits to produce\na normalized probability score that x belongs to each of c possible classes. For one\nsample with the vector representation e, the probability of the sample belonging to class\nj is:\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7c82d17-8f70-4f1d-ab48-67c000c27d27": {"__data__": {"id_": "a7c82d17-8f70-4f1d-ab48-67c000c27d27", "embedding": null, "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbe517d5-2096-4f99-b8be-9513ee86091e", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f84f5841923c413889768526f9b51fe8e1caeed33747bdefbef2ea7c7b3d0a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "fhead(e) = [y1, y2, ..., yc] ,\nP(j) =\u03c3([y1, y2, ..., yc]) = exp(yj)Pc\nk=1 exp(yk)\nwhere fhead(\u00b7) :Rm \u2212 \u2192Rc, and yj \u2208 R for all j. The classification head can have any\nnumber of layers (depth) d \u2208 N, but here we have opted to set d = 2throughout.\nFollowing this, the loss for the note category pre-training can be defined by the\nstandard cross-entropy formula:\nLnote = \u2212\nNX\nc=1\nyc log(pc) (4)\nFigure 2: Overview of our note category pre-training approach. On the left side ( A) shows the flow of the\ninput sequence (x) through the standard MLM pipeline, and on the right side (B) shows the integration of the\nassociated note category label in parallel. The MLM and note category classification objectives are jointly\noptimised with each document.\nWith both contrastive pre-training objectives outlined, we combine them with stan-\ndard MLM to form a joint loss function. To discourage the pre-training objective\nto over-represent the note category classification task, we also applied an optional\nweighting w to the loss, as shown in Equation 5, where Lcontrastive is either LInfoNCE (for\nDeCLUTR) or Lnote (for note contrastive).\nL = LMLM + w(Lcontrastive) (5)\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ead7fc51-3cf2-48c8-b63d-d46628b0f849": {"__data__": {"id_": "ead7fc51-3cf2-48c8-b63d-d46628b0f849", "embedding": null, "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f295428-5b56-4efb-a4de-e0f23cec05e1", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "46c5204d228b6e1a028919efcf19137330e88ddd5f8ca6fd4ab98ae0e2ed01dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.4. Adapting LLMs for downstream classification Tasks\nIn order to use the various further pre-trained LLMs for downstream classification\ntasks, we used the traditional fine-tuning approach. Conventional fine-tuning can be\nachieved by adding task-specific layer(s) or an entire multi-layer perceptron (MLPs) to\nthe LLM. The exact approach to processing the LLM output is dependent on the task.\nIn our use case of sequence or document classification, the downstream task head is\nan MLP fMLP(\u00b7) made of up 2 linear layers which takes the pooled sentence embedding\noutput by the LLM, e, as input and generates an n-dimensional vector, where n is the\nnumber of classes. A softmax operation is applied to the resultant output vector in order\nto generate probabilities of each class:\nP(y | x) =\nexp ((fMLP (h(x))y)\nexp (Pn\ni=1 fMLP(h(x))i).\nSince the additional MLP block and LLMs are modular, their respective parameters are\nstored separately, and we can opt to freeze the parameters of one or the other.\n2.5. Efficiency gains from LLM freezing and few-shot training\nWe sought to explore the potential of using the embeddings produced by the different\nLLMs without any further fine-tuning in relation to a given downstream task, keeping\nthe LLM body frozen, or by freezing different numbers of layers of the LLM model.\nMoreover, we explored the performance of the LLMs on the downstream tasks by\nfine-tuning with different numbers of training samples per class in a few-shot training\nsetup, similar to previous works [18].\n2.6. Document Embeddings Analysis\nBeyond downstream classification performance, we chose a selection of different\nmetrics of the LLMs embedding space to discern any clear differences produced by the\nvaried pre-training objectives used.\n2.6.1. Uniformity and Alignment\nWe follow a similar analysis plan to the authors of the SimCSE paper [21] to probe\nthe quality of the embedding space through measures of alignment between in-class\npairs and uniformity across the entire space. Alignment calculates expected distances\nbetween the paired instances, which in our case was embeddings of documents within\nthe same class (a proxy for positive pairs).\n\u2113align \u225c E\n(x,x+)\u223cppos\n\u2225f(x) \u2212 f(x+)\u22252 (6)\nConversely, uniformity measures how well the embeddings for all documents,\nregardless of class, are uniformly distributed. Together these metrics help illuminate\nhow the embedding spaces represent within class samples, which should remain close\ntogether and random, unrelated samples should be scattered.\n\u2113uniform \u225c log E\nx,y\ni.i.d.\n\u223c pdata\ne\u22122\u2225f(x)\u2212f(y)\u22252\n(7)\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afb992d3-919b-457b-848b-40c91fdef6e6": {"__data__": {"id_": "afb992d3-919b-457b-848b-40c91fdef6e6", "embedding": null, "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78e983e3-5ca2-46d2-96c8-a2ad45268e07", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e9afff3d172dc35fee1cadeff4cb1a377e7b2f1e22a6b9aa885ce7daedbee027", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.6.2. Cosine Similarity and Clustering\nGiven we produce a set of embeddings for our texts with known labels, we can\nrun a number of analyses utilising the raw vector embeddings for each note, as well\nas dimension reduced embedding spaces. For our analysis we opted to look at simple\ncosine similarity within, and between known classes for each dataset and a simple graph\nnetwork analysis.\nGraph network analysis provides another lens for understanding the structure of\nembedding spaces. In this approach, documents are represented as nodes in a graph,\nand we define our edges connecting notes that have a cosine similarity above a defined\nthreshold. The connectivity of the graph and formation of connected subgraphs can\nreveal insights about how notes are positioned in embedding space.\n3. Implementation details\n3.1. LLM model setups\nWe performed a large and varied number of experiments across the three datasets,\nwith different training regimes and requirements. An overview of these is presented in\nTable 2, although this is a curated selection intended to provide broad coverage of the\nmodels and training objectives used.\n3.2. Training overview\nA secondary objective of this work was to investigate the efficient pre-training of\nsmall LLMs given resource constrained environments (where large suites of GPUs\nare not available nor desirable). Further, Language modelling from scratch is often\nexpensive and hardware dependent, thus we chose to continue the pre-training phase\nextending already pre-trained general domain LLM. Importantly, we conducted training,\nevaluation, and hyperparameter exploration with the use of a single local GPU, similar\nto other works attempting to complete full pre-training with a single GPU given a fixed\namount of time [34]. For more information of the hardware setups in each case, please\nsee Appendix B.\n4. Results\nAfter extending the pre-training of the different LLMs using the methods outlined\nabove (MLM, DeCLUTR, and Note Contrastive), we have 12 LLMs to compare, outlined\nin Table 2. Given these LLMs now produce different representations of text, we evaluate\nthem across each of the respective datasets and associated downstream tasks. We present\ntwo approaches to evaluating these embeddings: downstream classification performance,\nand embedding space analysis.\n4.1. Downstream Classification Performance\nWe report results for using the LLMs in different settings.First, keeping the base\nLLM frozen during fine-tuning, and only update the weights of the classification head to\nassess the utility of the pre-trained embeddings with no further updates (effectively the\nLLMs here are pure feature extractors). We compare this with freezing varying numbers\nof LLM layers, and also fully fine-tuning the entire LLM on the downstream task.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "520cab2e-7869-4b67-b8e9-cfc9ae53e0c7": {"__data__": {"id_": "520cab2e-7869-4b67-b8e9-cfc9ae53e0c7", "embedding": null, "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77984cd6-67c2-42c4-a432-5349a1a8022d", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "051b0b64a4a09f7524e02ad48b865a74442dfbca419447f28b9ddfa732e8e60f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Clinical Dataset Domain Pre-training Model size Model Name\nNone None 124.6 RoBERTa-base\nMimic-III MLM 124.6 RoBERTa-mimic\nMimic-III MLM + DeCLUTR 124.6 RoBERTa-mimic-DeCLUTR\nMimic-III MLM + Note 125.1 RoBERTa-mimic-note\nOHFT MLM 124.6 RoBERTa-OHFT\nOHFT MLM + DeCLUTR 124.6 RoBERTa-OHFT-DeCLUTR\nOHFT MLM + Note 125.1 RoBERTa-OHFT-note\nPSIR MLM 124.6 RoBERTa-PSIR\nPSIR MLM + DeCLUTR 124.6 RoBERTa-PSIR-DeCLUTR\nPSIR MLM + Note 125.1 RoBERTa-PSIR-note\nTable 2: Overview of the different datasets and LLM pre-training. Domain pre-training refers to whether this\nmodel has been explicitly pre-trained using the related clinical dataset. We will use this table to define the\nmodel names that will be used throughout the results section. Each model uses RoBERTa-base as the base\nmodel.\nPLM Domain pre-training M-Cat M-Tri O-Cat O-Tri P-Cat P-Sev\nFrozen None 0.766 0.314 0.319 0.423 0.384 0.65\nMLM 0.867 0.439 0.355 0.552 0.498 0.739\nMLM + DeCLUTR 0.859 0.711 0.411 0.601 0.555 0.748\nMLM + Note - 0.471 - 0.546 0.450 0.714\nFinetuned None 0.991 0.827 0.593 0.766 0.655 0.837\nMLM 0.991 0.846 0.629 0.779 0.660 0.842\nMLM + DeCLUTR 0.988 0.844 0.613 0.765 0.653 0.844\nMLM + Note - 0.836 - 0.757 0.663 0.847\nTable 3: F1 macro across all datasets and classification tasks based on domain pre-training received. We\nreport the maximum F1 macro achieved over 5 epochs of training per model.\n4.1.1. Evaluation - all tasks\nThe evaluation performance for each of the domain pre-training methods across\nthe different datasets and tasks is presented in Table 3 (best over five epochs), with\nDeCLUTR models performing best when the LLM remains frozen during fine-tuning.\nIn the full fine-tuned setting, MLM models generally perform marginally better. Across\nall tasks and fine-tune settings, the models with no domain pre-training achieve the\nlowest performance. For individual performance metrics for each model across each\ndataset and task, including results for alternative open-source clinical LLMs, please see\nAppendix C.\n4.1.2. Few-shot sampling\nAdditionally we focus on training with sub-samples of the training set, representing\na setting where producing annotations is often a major limiting factor (e.g. due to lack of\nexpert time or prohibitive expense) in Fig. 3. For brevity we only include this analysis\nfor the MIMIC-III ICD-9 Triage task, however similar patterns were seen across all\ndatasets and tasks.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49401694-e006-4f65-92e8-0ce28dfeb8d8": {"__data__": {"id_": "49401694-e006-4f65-92e8-0ce28dfeb8d8", "embedding": null, "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfe68aca-4b89-4975-969e-e4df13a5f0cc", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "67f7af2035fd436c8993ba1cfc3c118d1a3a7600282af6b1c98336d37a0f3be5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 32 64 128 200\nSamples per class\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7F1\nAfter 1 epoch\n16 32 64 128 200\nSamples per class\nMax over 5 epochs\nDomain pretraining\nNone MLM Note contrastive DeCLUTR\nFigure 3: F1 macro score on evaluation set for the MIMIC-III ICD-9 Triage task with frozen LLMs trained\nwith different sample sizes per class.\n4.1.3. Effect of freezing layers\nTo investigate the influence of freezing different layers of the LLM, we present\nresults based on freezing an increasing number of consecutive layers of the LLM in Fig.\n4.\n0 2 4 6 8 10 11 12\nT otal number of LLM layers frozen\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8F1\nFirst epoch\n0 2 4 6 8 10 11 12\nT otal number of LLM layers frozen\nMax over 5 epochs\nDomain pretraining\nNone MLM Note contrastive DeCLUTR\nFigure 4: F1 macro score on evaluation set for the MIMIC-III ICD-9 Triage task with varying transformer\nlayers frozen (all models utilised a 12-layer RoBERTa architecture).\n4.2. Document Embeddings Analysis\nThe main result of training a LLM is a model that has captured aspects of how human\nlanguage is organised, encapsulated by its resultant embedding space. The pre-training\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c276dad-47a1-4c9c-91af-a118fa86daaa": {"__data__": {"id_": "4c276dad-47a1-4c9c-91af-a118fa86daaa", "embedding": null, "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "299a7be1-9f06-43c1-979c-7c22ae413117", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "3a2fd49a20bb2fcfa704baba88bfcb5060a228db4f6d9f043fe8d540c874ff17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "objective has a direct impact on the embedding space, as well as the domain targeted\ndownstream tasks. For any language modelling task or text-based downstream task, the\nword or sentence derived contextualised embeddings are the numerical representation\nof that knowledge obtained during pre-training.\nWe have seen substantial differences in the usefulness of different LLMs embeddings\nfor downstream tasks, especially when no fine-tuning occurs. To attempt to understand\nthis aspect in more detail, we present a exploration of the embedding spaces of different\nLLMs for the MIMIC-III ICD-9 triage task. For more embedding analysis details, see\nE.9.\n4.2.1. Cosine Similarity\nA common approach to measuring the characteristics of an embedding space, espe-\ncially when classes are known, is to look at the distances between embeddings of each\nclass within the embedding space. As expected, the LLMs which utilised a contrastive\nloss function (RoBERTa-mimic-DeCLUTR and RoBERTa-mimic-note) exhibit a much\ngreater separation of embeddings, with a much wider range of cosine similarity values.\nHowever, the differences between and within class members shows a very similar pattern\namongst all models, see Fig. 5.\nbetween within\n0.96\n0.97\n0.98\n0.99\n1.00Cosine Similarity\nNone\nbetween within\n0.96\n0.98\n1.00\nMLM\nbetween within\n0.6\n0.7\n0.8\n0.9\n1.0\nNote contrastive\nbetween within\n0.0\n0.2\n0.4\n0.6\n0.8\nDeCLUTR\nClass membership\nFigure 5: Cosine similarity of document embeddings within and between classes for the MIMIC-III ICD-9\ntriage dataset. Note the y-axis scales are separate for each subplot, this is due to the large differences in value\nranges between models.\n4.2.2. Alignment and uniformity\nAn example of comparing uniformity versus alignment is presented in Fig. 6, which\nhighlights large differences between the LLMs dependent on their pre-training objective.\nMost notably, the DeCLUTR models appear to have produced an embedding space with\na high diversity and hence low uniformity amongst all classes, but with a high alignment\nscore, implying within class embeddings remain relatively far apart.\n4.2.3. Network analysis\nResults of a simple graph network analysis with varying cosine similarity thresholds\nis provided in Table 4.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6d200ec-9182-49a0-9837-c61edf0413de": {"__data__": {"id_": "b6d200ec-9182-49a0-9837-c61edf0413de", "embedding": null, "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "204186e0-05b9-42fe-b3f8-cd482a7eb3d8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5f7bd639f62854536f25a8a929990787a253b5b5b74807eff31b6ebe66129c3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6: Uniformity vs. alignment for the different LLM setups for the MIMIC-III ICD-9 task embeddings.\nThe colorbar represents the corresponding F1 score on the ICD-9 triage classification task using these frozen\nLLM embeddings.\nModel name Cos. threshold # Components Avg. degree\nRoBERTa-base 0.995 1 317.896\n0.996 4 54.355\n0.997 8 24.128\n0.997 42 46.628\nRoBERTa-mimic 0.994 2 160.150\n0.995 7 168.512\n0.996 16 82.579\n0.997 45 45.913\nRoBERTa-mimic-DeCLUTR 0.538 1 317.300\n0.617 3 54.242\n0.699 24 26.233\n0.742 50 44.607\nRoBERTa-mimic-note 0.959 3 115.873\n0.967 8 127.278\n0.973 23 56.543\n0.977 34 38.244\nTable 4: Quantitative analysis of graph properties for different models. Cosine similarity thresholds are\nderived from the 90th, 95th, 98th, and 99th percentiles. The number of components reflects the count of\nconnected components given the threshold chose, and the average degree of the top Nt refers to the graph\ndegree across the top Nt connected components (here Nt is chosen to be 3).\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "704c9b2d-c2c7-459b-8d3c-cf09a36b1eae": {"__data__": {"id_": "704c9b2d-c2c7-459b-8d3c-cf09a36b1eae", "embedding": null, "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1871c86-2ec3-4443-b8ff-432d7ecc887b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4420d835c82ee23c30f334357d800b9b8ab6deb1d3da6253762f262df692dbdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The network analysis highlights that whilst the cosine distances between embeddings\nfor the LLMs that used a contrastive loss during pre-training are greater, they retain a\nsimilar graph network structure to other LLMs. The number of components (sub-graphs)\ncan be considered a good indicator of the diversity of the whole graph and correlates\nstrongly with the cosine similarity threshold.\n5. Discussion\n5.1. General\nThe pre-training of LLMs is a crucial step in the production of a useful embedding\nspace for downstream tasks which rely on sentence or document level embeddings,\nsuch as sequence classification, document information retrieval, clustering and semantic\nsearch. We evaluated various approaches to pre-training LLMs on several downstream\nsequence classification tasks in frozen and full fine-tuned settings.\nWe showed that models pre-trained with contrastive loss functions tended to out-\nperform other pre-training approaches across all our domain-specific datasets, with\nfewer training samples required to obtain reasonable evaluation performance. Whilst\nthe performance in the frozen setting did not match that of the full fine-tuned setting,\nthe pre-training had a clear influence on how usable these LLMs embedding spaces are\nfor the downstream tasks where classification boundaries are important. An attempt to\nintegrate further structured metadata based on the note category information did not\nseem to provide a performance gain in the classification tasks above standard MLM\ntraining. However in the embedding space analysis, the approach yielded better clus-\ntering metrics. The metadata for each dataset differed in nature, and the usefulness of\nthis information for the use-cases presented in this work are difficult to determine and\nwhether or not they hold utility for other tasks would require further investigations.\nThe DeCLUTR based models appear to produce an embedding space quite distinct\nto the other pre-training methods, with large separation between different documents\nand classes. However, it is also clear the the cosine similarity of documents within the\nsame class appears low, potentially highlighting that DeCLUTR did not align well with\nknown classes (recall DeCLUTR is unsupervised and had no access to class labels).\nThe network analysis highlighted a surprising consistency in resultant graph spaces: the\npattern of node numbers, component sizes, and degree of major components remained\nquite stable across each LLM, regardless of the apparent differences in cosine distances.\nThe utility of the embeddings produced by LLMs as direct features for downstream\napplications is particularly sought after in resource-constrained settings, where further\nfine-tuning can be difficult. Domain adaptation of open LLMs to the clinical domain\nwithin the UK remains an important goal, with our results showing the largest per-\nformance gaps between the general LLMs and the UK NHS dataset trained LLMs\npresented.\nThe resource efficiency of domain adaptation through pre-training is not straight-\nforward. Of course, the most resource friendly approach would be to include no domain\nadaption at all, although we suggest the potential performance gain offered by all\ncontinued pre-training approaches presented here is worth the relatively low cost: all\npre-training could be completed in a matter of hours on a single GPU.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aebb4f93-89e3-456f-8ed4-9a62578d09fe": {"__data__": {"id_": "aebb4f93-89e3-456f-8ed4-9a62578d09fe", "embedding": null, "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "455a1b36-879d-4cab-859d-ba7837494abe", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6c54c67e61c33e300b45c6f601d60b25fb22ca869559d71d295e838d17bd7d11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.2. Limitations\nDeCLUTR sampling. The sampling parameters that produced the best model for each\ndataset studied were not equal, were dependent on the length of the documents, and\nwere not flexible to varying lengths of documents. Further, the sampling procedure also\neliminates documents that are not long enough to satisfy a pre-determined minimum\nlength. This in fact means the DeCLUTR models were trained on a restricted sub-sample\nwhen compared to the other pre-training methods.\nNote category pre-training loss. The number of possible contrastive loss functions\napplicable to our dataset is very large, and we were not able to explore different setups\nnor vary the hyperparameters extensively. Similarly, differential selection of samples\nto use for the pre-training objective was not explored in this work. Further work could\nalso utilise modern transformer architecture adaptations to allow larger batch sizes and\ncontext windows [5, 30, 32].\nStrict training settings. We opted to showcase different pre-training methods in a\nrestricted resource setting, with a single low-end GPU and a limit on training time in\nline with similar budget-oriented LLM training research [34]. For this reason, we have\nnot explored fully the extremes of these approaches and their application to a wider\nrange of datasets and tasks.\n5.3. Conclusion\nThis study underscores the significance of pre-training methodology in domain\naligning language models with downstream tasks reliant on document representations.\nContrastive, self-supervised objectives prove most effective across the sequence classifi-\ncation tasks, outperforming masked language modeling. While incorporating structured\nmetadata during pre-training did not further improve performance, unsupervised meth-\nods like DeCLUTR yield more distinct, clustered embedding spaces. Notably, model\nembedding graph connectivity patterns persist irrespective of pre-training differences,\nimplying consistent high-level structure.\nDomain adaptation to UK NHS data remains critical, with specialized models\nsubstantially improving over general ones. The resource efficiency of this adaptation is\nnon-trivial; no adaptation maximizes efficiency without forfeit to performance gains.\nThe low resource approaches presented still confer valuable improvements worthy of\ntheir marginal cost.\nWhile we assessed a range of pre-training schemes and NHS-adapted models on\nsubsequent classification performance, open questions persist. Future work should\nexplore modern architectures and objectives, optimized sampling for contrastive learning,\nenhanced use of metadata, and applications beyond classification like information\nretrieval. Broader hyperparameter tuning may unveil further gains. Still, this research\nvalidates the utility of pre-trained healthcare language models, provides pre-training best\npractices, and motivates specialized adaptation - advancing practice while illuminating\nareas for additional inquiry.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c03e7e5d-4b33-40ab-9633-9edab67cbb23": {"__data__": {"id_": "c03e7e5d-4b33-40ab-9633-9edab67cbb23", "embedding": null, "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "692b1c6b-80f5-4e74-8ad2-88169551f8dd", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b203ada65ffbd62f884304c5774d79b7af9fcd699921069aa88257e21b3ed470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. Acknowledgements\nThe authors would like to thank the members of the Patient Safety Team in NHS Eng-\nland who engaged with us throughout the project and shared their in-depth knowledge of\nthe area to help shape our exploration. We would also like to acknowledge the work and\nsupport of the Oxford Research Informatics Team: Tanya Smith, Research Informatics\nManager, Adam Pill, Suzanne Fisher, Research Informatics Systems Analysts and Lulu\nKane Research Informatics Administrator.\n7. Funding\nNT was supported by the EPSRC Center for Doctoral Training in Health Data\nScience (EP/S02428X/1) and completed part of this work during a PhD internship with\nNHS England in late 2022. AK, ANH, and DWJ were supported in part by the NIHR\nAI Award for Health and Social Care (AI-AW ARD02183). DWJ is part supported by an\nNIHR Infrastructure Programme (NIHR203316).\nContributions\nN.T, D.S, A.K, and A.N.J conceptualised this work. N.T and D.S curated the\ndatasets. N.T developed pre-processing, experiment running and analysis code. N.T.\nperformed experiments across the three datasets. D.S performed extension experiments\non PSIR data. N.T and D.S explored and evaluated experimental results. N.T drafted\nthe manuscript. D.S, D.W.J, A.K, and A.N.H revised and edited the manuscript. All\nauthors read and approved the final version of the manuscript.\nReferences\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\n[2] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 38\u201345, Online, October 2020. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttps://aclanthology.org/2020.emnlp-demos.6.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9993feb-f461-448f-b430-db581f5b543d": {"__data__": {"id_": "f9993feb-f461-448f-b430-db581f5b543d", "embedding": null, "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5faeaca7-9d4f-45f3-915c-6319dfcd58e6", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1ae40ae32cf25a924e6c5e5cfa5cfe789a63ebf361e948ca9a23fb7d8211e572", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Atten-\ntion is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , volume 30. Curran Associates,\nInc., 2017. URL https://proceedings.neurips.cc/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[4] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for\nparameter-efficient prompt tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing , pages 3045\u20133059, On-\nline and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL\nhttps://aclanthology.org/2021.emnlp-main.243.\n[5] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\nAmanpreet Singh, and Douwe Kiela. Generative Representational Instruction\nTuning, February 2024. URL http://arxiv.org/abs/2402.09906.\narXiv:2402.09906 [cs].\n[6] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling\nclinical notes and predicting hospital readmission, 2019. URLhttps://arxiv.\norg/abs/1904.05342.\n[7] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan\nNaumann, and Matthew McDermott. Publicly available clinical BERT embeddings.\nIn Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages\n72\u201378, Minneapolis, Minnesota, USA, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/W19-1909. URL https://aclanthology.\norg/W19-1909.\n[8] Milad Moradi, Kathrin Blagec, Florian Haberl, and Matthias Samwald. Gpt-\n3 models are poor few-shot learners in the biomedical domain, 2021. URL\nhttps://arxiv.org/abs/2109.02555.\n[9] Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat,\nMoshe Wasserblat, and Oren Pereg. Efficient Few-Shot Learning Without\nPrompts, September 2022. URL http://arxiv.org/abs/2209.11055.\narXiv:2209.11055 [cs].\n[10] Bernal Jim\u00e9nez Guti\u00e9rrez, Nikolas McNeal, Clay Washington, You Chen, Lang Li,\nHuan Sun, and Yu Su. Thinking about GPT-3 In-Context Learning for Biomedical\nIE? Think Again. 2022. doi: 10.48550/ARXIV .2203.08410. URL https:\n//arxiv.org/abs/2203.08410. Publisher: arXiv Version Number: 3.\n[11] Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and\nGuoyin Wang. Text Classification via Large Language Models, May 2023. URL\nhttp://arxiv.org/abs/2305.08377. arXiv:2305.08377 [cs].\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71428714-d8f6-46fd-a802-8cdd7fb84c82": {"__data__": {"id_": "71428714-d8f6-46fd-a802-8cdd7fb84c82", "embedding": null, "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f37bc684-70d9-48ca-9a39-e95b925fdb24", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a78839e4ec127409fdf5a1eb7095026cb5f2bdce5fd35a4d84860111175afe72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does Synthetic\nData Generation of LLMs Help Clinical Text Mining?, April 2023. URL http:\n//arxiv.org/abs/2303.04360. arXiv:2303.04360 [cs].\n[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. 2019. doi: 10.48550/ARXIV .1907.11692.\nURL https://arxiv.org/abs/1907.11692.\n[14] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using\nSiamese BERT-Networks, August 2019. URL http://arxiv.org/abs/\n1908.10084. arXiv:1908.10084 [cs].\n[15] John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. DeCLUTR: Deep\nContrastive Learning for Unsupervised Textual Representations. In Proceed-\nings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 879\u2013895, Online, 2021. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.72. URL\nhttps://aclanthology.org/2021.acl-long.72.\n[16] Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylip\u00e4\u00e4\nHellqvist, and Magnus Sahlgren. Semantic re-tuning with contrastive tension.\nIn International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=Ov_sMNau-PF.\n[17] UK NHS. Abbreviations you may find in your health records\n- NHS App help and support, June 2021. URL https:\n//www.nhs.uk/nhs-app/nhs-app-help-and-support/\nhealth-records-in-the-nhs-app/abbreviations-commonly-found-in-medical-records/ .\n[18] Niall Taylor, Yi Zhang, Dan W. Joyce, Ziming Gao, Andrey Kormilitzin, and\nAlejo Nevado-Holgado. Clinical Prompt Learning With Frozen Language Mod-\nels. IEEE Transactions on Neural Networks and Learning Systems , pages\n1\u201311, 2023. ISSN 2162-2388. doi: 10.1109/TNNLS.2023.3294633. URL\nhttps://ieeexplore.ieee.org/document/10215061.\n[19] Yan Luo, Yuki Kataoka, Edoardo G. Ostinelli, Andrea Cipriani, and Toshi A.\nFurukawa. National Prescription Patterns of Antidepressants in the Treatment of\nAdults With Major Depression in the US Between 1996 and 2015: A Population\nRepresentative Survey Based Analysis. Frontiers in Psychiatry, 11, 2020. ISSN\n1664-0640.\n[20] Zeming Chen, Alejandro Hern\u00e1ndez Cano, Angelika Romanou, Antoine Bonnet,\nKyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K\u00f6pf,\nAmirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy,\nIgor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne\nHartley, Martin Jaggi, and Antoine Bosselut. MEDITRON-70B: Scaling Medical\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c9156b8-8038-4fd7-a7dd-7b220b5e7c2a": {"__data__": {"id_": "0c9156b8-8038-4fd7-a7dd-7b220b5e7c2a", "embedding": null, "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfa34520-04f4-4089-bb66-4d3abdf2a2cb", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f2733a6b7f4fad8cb728eb21e57ff2848d820e83a33491e6dd1d40892463e300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pretraining for Large Language Models, November 2023. URLhttp://arxiv.\norg/abs/2311.16079. arXiv:2311.16079 [cs].\n[21] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple Contrastive\nLearning of Sentence Embeddings, May 2022. URL http://arxiv.org/\nabs/2104.08821. arXiv:2104.08821 [cs].\n[22] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. Linkbert: Pretraining lan-\nguage models with document links. In Association for Computational Linguistics\n(ACL), 2022.\n[23] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christo-\npher D. Manning, Percy Liang, and Jure Leskovec. Deep Bidirectional Language-\nKnowledge Graph Pretraining, October 2022. URL http://arxiv.org/\nabs/2210.09338. arXiv:2210.09338 [cs].\n[24] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier.\nSelf-alignment pretraining for biomedical entity representations. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 4228\u20134238,\nJune 2021.\n[25] Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li Wei H. Lehman, Mengling\nFeng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\nCeli, and Roger G. Mark. Mimic-iii, a freely accessible critical care database.\nScientific Data, 3, 5 2016. ISSN 20524463. doi: 10.1038/sdata.2016.35.\n[26] Niall Taylor, Yi Zhang, Dan W. Joyce, Ziming Gao, Andrey Kormilitzin, and Alejo\nNevado-Holgado. Clinical prompt learning with frozen language models. IEEE\nTransactions on Neural Networks and Learning Systems, pages 1\u201311, 2023. doi:\n10.1109/TNNLS.2023.3294633.\n[27] NHS Digital. Mental health bulletin: 2019-20 annual report. Technical report,\n2020. URL https://digital.nhs.uk/data-and-information/\npublications/statistical/mental-health-bulletin/\n2019-20-annual-report .\n[28] NHS England \u00bb National patient safety incident reports.\nURL https://www.england.nhs.uk/patient-safety/\nnational-patient-safety-incident-reports/ .\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach, July 2019. URL http:\n//arxiv.org/abs/1907.11692. arXiv:1907.11692 [cs].\n[30] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-\ndocument transformer. 2020. URL https://arxiv.org/abs/2004.\n05150.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebc1ee53-2e8c-423b-9413-5f8e385573b4": {"__data__": {"id_": "ebc1ee53-2e8c-423b-9413-5f8e385573b4", "embedding": null, "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21debbee-1d51-46b0-a172-697bf6d5d2e5", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "3205ae2da3428904a14af011b274f047871044b81551737e60d07f870281679e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[31] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-\nbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. Efficiently Scaling Transformer Inference, November 2022. URL\nhttp://arxiv.org/abs/2211.05102. arXiv:2211.05102 [cs].\n[32] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAtten-\ntion: Fast and Memory-Efficient Exact Attention with IO-Awareness, June 2022.\nURL http://arxiv.org/abs/2205.14135. arXiv:2205.14135 [cs].\n[33] Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised Con-\ntrastive Learning for Pre-trained Language Model Fine-tuning, April 2021. URL\nhttp://arxiv.org/abs/2011.01403. arXiv:2011.01403 [cs].\n[34] Jonas Geiping and Tom Goldstein. Cramming: Training a Language Model on a\nSingle GPU in One Day, December 2022. URL http://arxiv.org/abs/\n2212.14034. arXiv:2212.14034 [cs].\n[35] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. CoRR, abs/1901.08746, 2019.\nURL http://arxiv.org/abs/1901.08746.\n[36] Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan\nNaumann, and Matthew B. A. McDermott. Publicly Available Clinical BERT\nEmbeddings. April 2019. URL http://arxiv.org/abs/1904.03323.\n[37] David L. Davies and Donald W. Bouldin. A Cluster Separation Measure. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, PAMI-1(2):224\u2013227,\nApril 1979. ISSN 1939-3539. doi: 10.1109/TPAMI.1979.4766909. URL\nhttps://ieeexplore.ieee.org/document/4766909. Conference\nName: IEEE Transactions on Pattern Analysis and Machine Intelligence.\n[38] \u00d6zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2010 i2b2/V A\nchallenge on concepts, assertions, and relations in clinical text. Journal of the\nAmerican Medical Informatics Association : JAMIA, 18(5):552\u2013556, 2011. ISSN\n1067-5027. doi: 10.1136/amiajnl-2011-000203. URL https://www.ncbi.\nnlm.nih.gov/pmc/articles/PMC3168320/.\n[39] Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. Evaluating temporal relations in\nclinical text: 2012 i2b2 Challenge. Journal of the American Medical Informatics\nAssociation : JAMIA , 20(5):806\u2013813, September 2013. ISSN 1067-5027. doi:\n10.1136/amiajnl-2013-001628. URL https://www.ncbi.nlm.nih.gov/\npmc/articles/PMC3756273/.\n[40] Amber Stubbs, Christopher Kotfila, and \u00d6zlem Uzuner. Automated systems\nfor the de-identification of longitudinal clinical narratives: Overview of 2014\ni2b2/UTHealth shared task Track 1. Journal of Biomedical Informatics , 58:\nS11\u2013S19, December 2015. ISSN 1532-0464. doi: 10.1016/j.jbi.2015.06.007.\nURL https://www.sciencedirect.com/science/article/pii/\nS1532046415001173.\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eff31749-96ea-4fb3-9fbb-23af8f244693": {"__data__": {"id_": "eff31749-96ea-4fb3-9fbb-23af8f244693", "embedding": null, "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fe9dfff-bf79-4b07-afc0-3ec00caf91c4", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7791361e6adffaa719cebafa0c3beeeb4d9b6ee47863d6f1c58aa8a01f7b28ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] Omid Rohanian, Mohammadmahdi Nouriborji, Hannah Jauncey, Samaneh\nKouchaki, ISARIC Clinical Characterisation Group, Lei Clifton, Laura Merson,\nand David A. Clifton. Lightweight Transformers for Clinical Natural Language\nProcessing, February 2023. URL http://arxiv.org/abs/2302.04725.\narXiv:2302.04725 [cs].\nAppendix A. Dataset details\nAppendix A.1. Document lengths\nDistributions and median document lengths for each dataset are provided below\nin Fig.A.7. The MIMIC-III dataset has consistently longer documents, with the PSIR\ndataset having relatively short documents.\nFigure A.7: Distribution of the number of tokens in documents for each respective dataset\nAppendix B. Hardware details\nDue to the varying secure locations and computational infrastructure of the hosts\nof the datasets, it was impossible to match the hardware used. However, all training\nwas carried out on one GPU only. The only difference between the setups was the exact\nmodel of GPU. For the PSIR dataset both training and inference were performed on a\nprivate single machine hosted by Microsoft Azure with the following main specifica-\ntions: 1 x NVIDIA Tesla T4 GPU. The OHFT dataset utilised the same Tesla T4 GPU\narchitecture but accessed via a private Amazon Web Services (AWS) instance. The\nMIMIC-III dataset was housed locally, and we utilised single NVIDIA RTX 2080.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5e4aa46c-c968-4769-9eb8-2c87675c49bb": {"__data__": {"id_": "5e4aa46c-c968-4769-9eb8-2c87675c49bb", "embedding": null, "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbda075b-6840-4c04-b4de-d56fd8eaa43c", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95752f6c5bf4a98dcd9a9451cee046368dab718e8115bfd3c6ea074e871c9708", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix C. Downstream classification performance\nAppendix C.1. Patient Safety Incident Reports\nThe evaluation results for the severity classification task (P-Sev) and incident type\n(P-Cat) task are presented in Table C.5.\nFor both tasks we present results in both the frozen LLM and fine-tuned settings.\nThe frozen LLM setting means the parameters of the LLM are set tonot require gradient,\nwhich means gradients will not be computed during the backward pass and keeping\nthese weights fixed and only the newly introduced parameters of the classification head\nare updated during training. Fine-tuning on the other hand will update all parameters,\nincluding the LLM.\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.644 0.643 0.718 0.643 0.644\nRoBERTa-PSIR Frozen 0.704 0.687 0.787 0.679 0.704\nRoBERTa-PSIR-DeCLUTR Frozen 0.786 0.765 0.869 0.752 0.786\nRoBERTa-PSIR-note Frozen 0.699 0.678 0.773 0.669 0.699\nRoBERTa-base Finetuned 0.847 0.636 0.926 0.640 0.646\nRoBERTa-PSIR Finetuned 0.866 0.835 0.942 0.816 0.866\nRoBERTa-PSIR-DeCLUTR Finetuned 0.870 0.850 0.944 0.836 0.870\nRoBERTa-PSIR-note Finetuned 0.863 0.833 0.936 0.814 0.863\n(a) Severity classification task (P-Sev)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.197 0.163 0.794 0.314 0.197\nRoBERTa-PSIR Frozen 0.498 0.474 0.851 0.494 0.499\nRoBERTa-PSIR-DeCLUTR Frozen 0.580 0.549 0.900 0.559 0.580\nRoBERTa-PSIR-note Frozen 0.421 0.354 0.847 0.415 0.421\nRoBERTa-base Finetuned 0.646 0.636 0.926 0.640 0.646\nRoBERTa-PSIR Finetuned 0.665 0.652 0.935 0.655 0.665\nRoBERTa-PSIR-DeCLUTR Finetuned 0.670 0.659 0.935 0.667 0.670\nRoBERTa-PSIR-note Finetuned 0.660 0.647 0.933 0.652 0.660\n(b) Incident category classification task (P-Cat)\nTable C.5: Evaluation metrics for the incident category classification task (P-Cat) after one epoch for various\nmodels in both frozen and full fine-tuned settings\nAppendix C.2. MIMIC-III\nEvaluation of the different LLMs for the MIMIC-III note category task (M-Cat) and\nfor the MIMIC-III ICD-9 triage task (M-Tri) are presented Table C.6.\nAppendix C.3. OHFT\nEvaluation results for the OHFT note category task (O-Cat) and OHFT Accepted\nTriage Team task (O-Tri) are presented in Table C.7.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e566adc4-edaf-4f8b-964f-eb6be0252d2c": {"__data__": {"id_": "e566adc4-edaf-4f8b-964f-eb6be0252d2c", "embedding": null, "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a437b24d-5323-4c43-a1dd-0e9eff58e51b", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7e58657d9d72111c289ef8b6e8b8ccceca9e3a90905af3ba06882769b6e4afd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.433 0.347 0.949 0.406 0.433\nRoBERTa-mimic Frozen 0.448 0.353 0.979 0.434 0.448\nRoBERTa-mimic-DeCLUTR Frozen 0.778 0.770 0.962 0.791 0.778\nRoBERTa-base Finetuned 0.979 0.979 0.999 0.980 0.979\nRoBERTa-mimic Finetuned 0.978 0.978 0.999 0.979 0.978\nRoBERTa-mimic-DeCLUTR Finetuned 0.985 0.985 0.999 0.986 0.985\n(a) MIMIC-III note category task (M-Cat)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.702 0.264 0.778 0.381 0.293\nRoBERTa-mimic Frozen 0.315 0.188 0.871 0.435 0.303\nRoBERTa-mimic-DeCLUTR Frozen 0.776 0.545 0.893 0.561 0.542\nRoBERTa-mimic-note Frozen 0.591 0.409 0.834 0.439 0.473\nRoBERTa-base Finetuned 0.909 0.827 0.984 0.808 0.855\nRoBERTa-mimic Finetuned 0.912 0.824 0.990 0.797 0.884\nRoBERTa-mimic-DeCLUTR Finetuned 0.917 0.831 0.990 0.794 0.890\nRoBERTa-mimic-note Finetuned 0.906 0.819 0.987 0.788 0.867\n(b) MIMIC-III ICD-9 triage task (M-Tri)\nTable C.6: Evaluation metrics for text classification tasks after one epoch\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.212 0.171 0.624 0.156 0.212\nRoBERTa-OHFT Frozen 0.224 0.163 0.697 0.162 0.224\nRoBERTa-OHFT-DeCLUTR Frozen 0.292 0.255 0.709 0.272 0.292\nRoBERTa-base Finetuned 0.380 0.348 0.800 0.453 0.380\nRoBERTa-OHFT Finetuned 0.455 0.431 0.852 0.463 0.455\nRoBERTa-OHFT-DeCLUTR Finetuned 0.404 0.390 0.821 0.434 0.404\n(a) OHFT note category task (O-Cat)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.424 0.390 0.752 0.415 0.424\nRoBERTa-OHFT Frozen 0.495 0.469 0.830 0.572 0.495\nRoBERTa-OHFT-DeCLUTR Frozen 0.557 0.539 0.831 0.565 0.557\nRoBERTa-OHFT-note Frozen 0.414 0.391 0.767 0.466 0.414\nRoBERTa-base Finetuned 0.677 0.665 0.918 0.692 0.677\nRoBERTa-OHFT Finetuned 0.752 0.753 0.935 0.769 0.752\nRoBERTa-OHFT-DeCLUTR Finetuned 0.738 0.738 0.934 0.750 0.738\nRoBERTa-OHFT-note Finetuned 0.744 0.743 0.933 0.745 0.744\n(b) OHFT Accepted triage task (O-Tri)\nTable C.7: Evaluation metrics for text classification tasks after one epoch\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b7976034-b756-4fa5-b20c-47413e9e492c": {"__data__": {"id_": "b7976034-b756-4fa5-b20c-47413e9e492c", "embedding": null, "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff3ad51b-9b19-4f0d-ac01-dfd784629f2f", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "82b7197a4395c28774faf26fdb1085a001a3211f43d691f2cb8cef3c609f4fe5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix D. Performance of open clinical models\nIn Table. D.8 we provide some further evaluation results of similarly sized open\nLLMs pre-trained on biomedical and clinical text, including models trained on all\nMIMIC-III notes. BioLinkBERT and Bio-ClinicalBERT have previously achieved near\nstate of the art when applied to open medical NLP datasets [6, 35, 36]. We have results\nfor only the MIMIC-III triage task (M-Tri) and the OHFT accepted triage task (O-Tri).\nWe can see a slight performance drop with the open LLMs when compared with our own\non the M-Tri task, and a larger performance drop for the OHFT dataset. This highlights\nthe relative importance of domain pre-training for the UK based dataset.\nModel LLM Accuracy F1 AUC Precision Recall\nBioLinkBERT-base Frozen 0.492 0.298 0.877 0.386 0.413\nBio-ClinicalBERT Frozen 0.615 0.405 0.820 0.435 0.440\nBioLinkBERT-base Finetuned 0.942 0.870 0.988 0.846 0.904\nBio-ClinicalBERT Finetuned 0.919 0.827 0.988 0.812 0.855\n(a) MIMIC-III ICD-9-triage task (M-Tri)\nModel PLM Accuracy F1 AUC Precision Recall\nBioLinkBERT-base Frozen 0.376 0.356 0.706 0.410 0.376\nBio-ClinicalBERT Frozen 0.395 0.382 0.713 0.435 0.395\nBioLinkBERT-base Finetuned 0.703 0.700 0.915 0.714 0.703\nBio-ClinicalBERT Finetuned 0.715 0.714 0.922 0.718 0.715\n(b) OHFT accepted triage team task (O-Tri)\nTable D.8: Evaluation metrics for text classification tasks after one epoch\nAppendix E. Cluster analysis\nA common approach to exploring a dataset through an LLMs embedding space is\nto perform a form of unsupervised clustering analysis. A simple analysis using the\nK-Means clustering technique is provided below. The dataset used here is the MIMIC-III\nICD-9 triage task (M-Tri). The following clustering metrics are reported: David Bouldin\nindex (DBi) [37], Calinski Harabaz Index (CHi), and the silhouette score (SS).\nThe DBi determines the average similarity measure of each derived cluster with its\nmost similar cluster, with similarity defined as the ratio of within-cluster distances to\nbetween-cluster distance (far apart clusters with little dispersion resulting in a better,\nlower score). The CHi is the ratio of the sum of between-cluster and within-cluster\ndispersion, with higher scores indicating more separable clusters. The SS assesses the\noverlap of clusters and ranges from -1 to 1 with 1 being optimal.\nWe find that the model trained on MIMIC-III using MLM only (RoBERTa-mimic)\nappears to perform best, with DeCLUTR models actually fairing worse. This is a little\nsurprising considering the DeCLUTR models generally were optimal for downstream\nclassification adaptation.\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a5afaab4-c60b-4b85-9368-5531f9005c1c": {"__data__": {"id_": "a5afaab4-c60b-4b85-9368-5531f9005c1c", "embedding": null, "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9cda855-5d81-482b-a385-bb9c488d130a", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "09115b6aa4cb15fb629915fd79f689fb18c9491979a5b831b175c0d1b4d71188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model DBi score (<) CH score (>) Silhouette score (>) Optimal K\nRoBERTa-base 2.103 478.670 0.147 5\nDeCLUTR-base 3.031 284.237 0.096 4\nRoBERTa-base-DeCLUTR 2.848 300.310 0.114 4\nRoBERTa-mimic 1.663 911.911 0.235 4\nRoBERTa-mimic-note 1.582 755.344 0.230 6\nTable E.9: K-means cluster analysis results for each pre-trained LLM related to the Mimic-III ICD-9-Triage\ndataset.\nAppendix F. Pre-training effects on token classification\nOne aspect of this work was to determine how the pre-training effected the docu-\nment level embeddings, however it is also interesting to consider how the word level\nembeddings have changed: in particular we may expect the contrastive loss functions to\nhave moved the LLMs objective function away from the original MLM objective which\nshould impact token level tasks.\nWe investigated the performance of each pre-trained model from the Mimic-III\ndataset on three token classification tasks (these were readily available with gold standard\nground truths). The tasks were Named Entity Recognition (NER) tasks formed during\nvarious I2B2 challenges [38, 39, 40].\nMicro averaged F1 scores for each task are provided in Table F.10, and to determine\nthe utility of the models embeddings as features we explore downstream fine-tuning\nwith the LLM frozen and compare with full fine-tuning. Results do not show any major\ndifferences between the models, and generally keeping the LLM frozen did not allow\nany learning of the task in two of the three tasks. In the fine-tuned setting, all models\nconverge on similar performance in line with other studies utilising the same tasks [41].\nModel PLM i2b2 2010 i2b2 2012 i2b2 2014\nRoBERTa-base Frozen 0.052 0.042 0.639\nDeCLUTR-base 0.043 0.065 0.651\nRoBERTa-base-DeCLUTR 0.083 0.092 0.551\nRoBERTa-mimic 0.041 0.08 0.636\nRoBERTa-mimic-note 0.006 0.067 0.616\nRoBERTa-base Fine-tuned 0.847 0.83 0.975\nDeCLUTR-base 0.844 0.833 0.976\nRoBERTa-base-DeCLUTR 0.854 0.837 0.977\nRoBERTa-mimic 0.854 0.847 0.976\nRoBERTa-mimic-note 0.855 0.841 0.979\nTable F.10: Token classification results. For brevity we report F1 micro for each of the models for each dataset\nAppendix G. Training details\nData splits. In order to avoid direct data leakage from the continued language model pre-\ntraining steps and the subsequent downstream classification tasks, we created entirely\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b16887e-d4d6-4077-8acc-67d7e89a0ac4": {"__data__": {"id_": "8b16887e-d4d6-4077-8acc-67d7e89a0ac4", "embedding": null, "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "589c5b12-4927-4cb1-b8f5-a5d021f7fe2e", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f41f0fbe4e38bf4e73abf89942ae4658f2d87708c232969584692ee0cb533360", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "separate subsets of data: one partition formed the full training and validation sets for\nthe pre-training and the other formed the entire training and validation sets for the\ndownstream tasks. For the different language model pre-training approaches, we subset\napproximately 250,000 notes for each our datasets.\nPre-processing. For language modelling with transformer based models minimal data\ncleaning is required, as the tokenization of inputs paired with the contextualised repre-\nsentations of words means we want to keep as much of the original input as possible.\nTypical processing steps would include removal of carriage returns, tabs, and white\nspace and any poorly encoded characters.\nHyperparameter choices. Table G.11 reports the main hyperparameter choices for our\nexperiments. Notably, the batch size has direct bearing on the difficulty of the contrastive\nobjective the DeCLUTR and note contrastive models were trained with. In contrastive\nloss paradigms, for any given anchor, the model needs to make the correct match out of\nthe Nb reports in the batch. The probability of successfully finding the correct match by\nchance decreases with the batch size. We set the batch size to 32 on the single-GPU\nmachine as this was the maximum possible across all span length options we tried.\nThe original authors of DeCLUTR found 2 anchors was optimal for training, whereas\nthe number of positives had little effect. The optimal span length differed between each\nof our datasets: 1024 for MIMIC-III, 64 for OHFT and Patient Safety, which appears in\npart related to the average length of document per dataset.\nFor the note contrastive models, we found combining MLM and the note category\nlosses to be optimal, although when given equal weighting the note category loss often\ndominated due to it being new to the already MLM pre-trained model. Thus, we found\nlowering the weighting of that loss function dramatically improved the subsequent\ndownstream performance. Whilst we did not explore this thoroughly, we found a\nweighting of 0.1 for the note category loss performed reasonably. Each of the different\nexperiments and objectives can require different hyperparameters and we opted to follow\nthose used by original implementations where possible.\nParameter MLM DeCLUTR Note Contrastive Downstream tasks\nBatch size 16 32 16 16\nGradient accumulation steps 4 1 1 1\nEmbedding dimension 768 768 768 768\nLearning rate 1 \u00d7 10\u22125 1 \u00d7 10\u22125 1 \u00d7 10\u22125 1 \u00d7 10\u22124\nOptimiser Adam W Adam W Adam W Adam W\nSpan length - [16, 64, 512, 1024] - -\nContrastive loss weight - - [0.1,0.3, 1.0] -\nEpochs 3 3 3 5\nTable G.11: Overview of hyperparameters used in our experiments. All training regimes utilised a linear\nscheduler with warm-up.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fcad995-6fc0-421b-86ed-fb3538ead9f0": {"__data__": {"id_": "9fcad995-6fc0-421b-86ed-fb3538ead9f0", "embedding": null, "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eca59d18-33f3-4a2f-b059-7b89c3f325a6", "node_type": "4", "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8c63570e1af5e658fff674fe5a4dc07a3b1277f8241f88d457c157eaeb4c96f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix H. DeCLUTR Sampling\nThe DeCLUTR sampling algorithm used enforces a minimum length of documents\ndependent on the span length, and the number of anchor spans to derive. The effect of\nthe minimum length on number of suitable samples for each dataset is provided in Table\nH.12.\nProportion of 250k LM training dataset\nMin. document length OHFT PSIR MIMIC-III\n16 0.97 0.88 0.99\n32 0.89 0.69 0.95\n64 0.72 0.41 0.97\n128 0.46 0.15 0.92\n256 0.21 0.04 0.81\n512 0.07 0.02 0.57\n1024 0.02 0.0 0.35\n2048 0.002 0.0 0.11\nTable H.12: Sample distributions for different DeCLUTR sampling minimum document lengths for each of\nthe datasets: OHFT, PSIR, and MIMIC-III. The proportion refers to the amount of samples that meet each of\nthe minimum document length thresholds.\nAppendix I. DeCLUTR extended results\nMore granular evaluation results for the incident report severity (P-Sev) and incident\ncategory (P-Cat) tasks, varying numbers of epochs of pre-training with the DeCLUTR\napproach (i.e. both the MLM and contrastive loss objective). The models were trained\nusing documents with a minimum length of 64 tokens, see Table H.12 for more details.\nAppendix J. Ablation results\nIn order to determine the effect of the contrastive loss components of the note\ncategory pre-training, we investigated isolating each of the MLM and note category\nlosses to create two separate models. Due to data access constraints, we perform this\nanalysis only on the MIMIC-III and OHFT datasets.\nFor the MIMIC-III ICD-9 triage task (M-Tri) we found that the MLM only models\ngenerally perform similarly to the combined loss model, with only a 0.05 drop in F1\nmacro in the frozen setting and a 0.1 increase in the full fine-tuned setting. However,\nthe note category loss only model affects downstream task performance dramatically,\nwith a 0.2 and 0.1 drop in F1 macro in the frozen and fine-tuned settings respectively.\nWith the OHFT Accepted triage team task (O-Tri), there was very little difference:\nnote only loss lead to a drop of 0.05 and 0.01 in frozen and fine-tuned settings respec-\ntively. The MLM only loss model had a subtle increase of 0.02 and 0.01 in frozen and\nfine-tuned settings, but this is negligible.\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "895f306f-0c03-4f11-9418-34792a0c7a35": {"__data__": {"id_": "895f306f-0c03-4f11-9418-34792a0c7a35", "embedding": null, "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52eb3eae-9d0d-4712-9197-64d8281f7aed", "node_type": "4", "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e41e6f3a724ec6ecfff8ef93a6ac7b1bb8262a194d619ee343303dcfa1d2d5b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Performance after 1 epoch\nName PLM Pre. Epochs Trainable params.(m) Accuracy ROC AUC F1 Prec. Recall\nDeCLUTR-base-incident Frozen 2 0.5 0.758 0.842 0.725 0.713 0.758\n3 0.5 0.763 0.847 0.719 0.709 0.763\n5 0.5 0.763 0.848 0.75 0.741 0.763\n10 0.5 0.771 0.855 0.752 0.74 0.771\n25 0.5 0.769 0.859 0.756 0.746 0.769\n50 0.5 0.755 0.840 0.717 0.705 0.755\nRoBERTa-incident-DeCLUTR Frozen 2 0.5 0.775 0.858 0.750 0.736 0.775\n3 0.5 0.786 0.869 0.765 0.752 0.786\n5 0.5 0.780 0.871 0.769 0.760 0.780\n10 0.5 0.782 0.865 0.751 0.731 0.762\n25 0.5 0.776 0.866 0.752 0.740 0.776\n50 0.5 0.765 0.851 0.734 0.720 0.765\nDeCLUTR-base-incident Fine-tuned 2 125 0.859 0.936 0.833 0.816 0.859\n3 125 0.861 0.937 0.823 0.803 0.861\n5 125 0.858 0.936 0.841 0.828 0.858\n10 125 0.859 0.934 0.827 0.808 0.859\n25 125 0.854 0.934 0.846 0.835 0.854\n50 125 0.851 0.931 0.792 0.767 0.851\nRoBERTa-incident-DeCLUTR Fine-tuned 2 125 0.870 0.944 0.850 0.836 0.870\n3 125 0.868 0.942 0.838 0.819 0.868\n5 125 0.867 0.941 0.831 0.811 0.867\n10 125 0.858 0.938 0.825 0.807 0.858\n25 125 0.863 0.939 0.833 0.815 0.863\n50 125 0.857 0.934 0.831 0.815 0.857\nTable I.13: Evaluation metrics for the severity classification task (P-Sev) for various models in both frozen\nand full fine-tuned settings of DeCLUTR models at various different total epochs of further pre-training for\nperformance after one epoch of training on the task.\nThe drop in performance for the MIMIC-III task may be related to the relationship\nbetween the note category meta data and the subsequent downstream tasks: the ICD-9\ntriage task (M-Tri), as it actually utilises only one type of clinical note, discharge\nsummaries and thus the influence of other note types is not examined.\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d895f2fb-2d30-4093-87ff-4f9a477e5346": {"__data__": {"id_": "d895f2fb-2d30-4093-87ff-4f9a477e5346", "embedding": null, "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "233b941d-4b35-44a9-890f-7e9135c16b4d", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "63336ef3720a36a839ca064d32e5366430897631bfee78a82b0d8a313b114595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19294647-156a-470a-965e-f7f63c9117c8", "node_type": "1", "metadata": {}, "hash": "2b759f58eda3587eaa2bc6b219a47a9e886935b69f9bcaace9976a852b8bcd2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Performance after 1 epoch\nName PLM Pre. Epochs Trainable params.(m) Accuracy ROC AUC F1 Prec. Recall\nDeCLUTR-base-incident Frozen 2 0.5 0.548 0.878 0.511 0.525 0.548\n3 0.5 0.542 0.883 0.503 0.509 0.542\n5 0.5 0.559 0.886 0.527 0.531 0.559\n10 0.5 0.562 0.888 0.532 0.534 0.562\n25 0.5 0.577 0.893 0.544 0.557 0.577\n50 0.5 0.550 0.877 0.518 0.538 0.550\nRoBERTa-incident-DeCLUTR Frozen 2 0.5 0.555 0.883 0.521 0.527 0.555\n3 0.5 0.568 0.892 0.531 0.541 0.568\n5 0.5 0.580 0.900 0.549 0.559 0.580\n10 0.5 0.578 0.894 0.544 0.551 0.578\n25 0.5 0.574 0.892 0.545 0.545 0.574\n50 0.5 0.572 0.883 0.548 0.552 0.572\nDeCLUTR-base-incident Fine-tuned 2 125 0.648 0.930 0.641 0.651 0.648\n3 125 0.661 0.930 0.651 0.657 0.661\n5 125 0.649 0.931 0.632 0.641 0.649\n10 125 0.658 0.931 0.647 0.647 0.658\n25 125 0.651 0.931 0.640 0.643 0.651\n50 125 0.645 0.929 0.631 0.635 0.645\nRoBERTa-incident-DeCLUTR Fine-tuned 2 125 0.670 0.935 0.659 0.667 0.670\n3 125 0.668 0.933 0.660 0.672 0.668\n5 125 0.660 0.934 0.646 0.660 0.660\n10 125 0.658 0.932 0.644 0.653 0.658\n25 125 0.659 0.935 0.649 0.654 0.659\n50 125 0.653 0.934 0.644 0.654 0.653\nTable I.14: Evaluation metrics for the incident category classification task (P-Cat) for various models in both\nfrozen and full fine-tuned settings of DeCLUTR models at various different total epochs of further pre-training\nfor performance after one epoch of training on the task.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "19294647-156a-470a-965e-f7f63c9117c8": {"__data__": {"id_": "19294647-156a-470a-965e-f7f63c9117c8", "embedding": null, "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "233b941d-4b35-44a9-890f-7e9135c16b4d", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "63336ef3720a36a839ca064d32e5366430897631bfee78a82b0d8a313b114595", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d895f2fb-2d30-4093-87ff-4f9a477e5346", "node_type": "1", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a660afb9978f0b03775344f858cc95dd11ba20c04ff76e6ded0ab9a47fc9a2b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample size 16 32 64 128 200\nDomain pre-training Task\nNone M-Cat 0.028 0.098 0.082 0.113 0.347\nM-Tri 0.028 0.130 0.207 0.013 0.492\nO-Cat NaN 0.028 0.015 0.042 0.171\nO-Tri 0.071 0.109 0.095 0.186 0.079\nP-Cat 0.022 0.011 0.011 0.011 0.011\nP-Sev 0.466 0.430 0.196 0.430 0.196\nMLM M-Cat 0.075 0.184 0.029 0.167 0.353\nM-Tri 0.098 0.212 0.349 0.196 0.510\nO-Cat NaN 0.021 0.081 0.108 0.163\nO-Tri 0.069 0.134 0.086 0.168 0.213\nP-Cat 0.015 0.011 0.031 0.013 0.038\nP-Sev 0.430 0.271 0.440 0.196 0.431\nDeCLUTR M-Cat 0.269 0.287 0.495 0.618 0.770\nM-Tri 0.146 0.095 0.315 0.431 0.683\nO-Cat 0.120 0.097 0.194 0.248 0.255\nO-Tri 0.190 0.238 0.265 0.356 0.453\nP-Cat 0.024 0.031 0.020 0.065 0.119\nP-Sev 0.435 0.519 0.361 0.506 0.494\nNote contrastive M-Tri 0.025 0.038 0.091 0.190 0.426\nO-Tri 0.106 0.212 0.173 0.349 0.413\nP-Cat 0.027 0.011 0.027 0.015 0.015\nP-Sev 0.237 0.467 0.437 0.454 0.196\nTable I.15: F1 macro score on all tasks after one epoch of training with different number of samples per class.\nBase LLMs were frozen and only the classification head received updates.\n33", "mimetype": "text/plain", "start_char_idx": 1394, "end_char_idx": 2457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "80db4b67-ce50-427b-8322-c2144e823ca3": {"__data__": {"id_": "80db4b67-ce50-427b-8322-c2144e823ca3", "embedding": null, "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e35c7bd-27ac-46ca-b1bb-cce296931b82", "node_type": "4", "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "623f55bd7fc64dae8ec833db4aa3c72b791f26c056bce730491bab6771bc828b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample size 16 32 64 128 200\nDomain pre-training Task\nNone M-Cat 0.296 0.831 0.878 0.921 0.979\nM-Tri 0.050 0.023 0.748 0.732 0.827\nO-Cat 0.121 0.165 0.238 0.316 0.348\nO-Tri 0.069 0.115 0.331 0.466 0.490\nP-Cat 0.011 0.012 0.011 0.057 0.397\nP-Sev 0.196 0.223 0.196 0.196 0.412\nMLM M-Cat 0.531 0.892 0.968 0.981 0.978\nM-Tri 0.282 0.367 0.596 0.802 0.824\nO-Cat 0.223 0.245 0.367 0.388 0.431\nO-Tri 0.221 0.236 0.494 0.669 0.706\nP-Cat 0.022 0.012 0.017 0.443 0.557\nP-Sev 0.451 0.435 0.404 0.442 0.197\nDeCLUTR M-Cat 0.734 0.921 0.967 0.981 0.985\nM-Tri 0.294 0.493 0.727 NaN 0.831\nO-Cat 0.237 0.247 0.307 0.328 0.390\nO-Tri 0.218 0.380 0.493 0.504 0.605\nP-Cat 0.051 0.037 0.099 0.399 0.452\nP-Sev 0.204 0.450 0.464 0.391 0.401\nNote contrastive M-Tri 0.134 0.258 0.537 0.759 0.827\nO-Tri 0.232 0.210 0.409 0.539 0.618\nP-Cat 0.013 0.012 0.019 0.129 0.519\nP-Sev 0.430 0.196 0.430 0.340 0.196\nTable I.16: F1 macro score on all tasks after one epoch of training with different number of samples per class.\nAll models were fully fine-tuned.\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6779e741-70c4-4c3e-bbab-08bc83490797": {"__data__": {"id_": "6779e741-70c4-4c3e-bbab-08bc83490797", "embedding": null, "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80076866-e602-4250-818b-46ec6facb36f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "89e1845be1e8ed25816d50adf75b7e892db619885bd21a1a058ec0845cb689e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2409.16860v1  [cs.CV]  25 Sep 2024\nTH E RO L E O F LA N G UAG E MO D E L S I N MO D E R N HE A LT H CA R E :\nA C O M P R E H E N S IV E RE V I E W\nAmna Khalid\nCHRISTUS Santa Rosa Hospital\nNew Braunfels, TX\nA yma Khalid\nRiphah International University\nLahore, Pakistan\nUmar Khalid\nPalo Alto, CA\nABSTRACT\nThe application of large language models (LLMs) in healthca re has gained signi\ufb01cant attention due\nto their ability to process complex medical data and provide insights for clinical decision-making.\nThese models have demonstrated substantial capabilities i n understanding and generating natural\nlanguage, which is crucial for medical documentation, diag nostics, and patient interaction. This\nreview examines the trajectory of language models from thei r early stages to the current state-of-\nthe-art LLMs, highlighting their strengths in healthcare a pplications and discussing challenges such\nas data privacy, bias, and ethical considerations. The pote ntial of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensu re their ethical and effective integration\ninto medical practice.\nKeywords Large Language Models, Healthcare, Machine Learning, Natu ral Language Processing, Medical AI,\nEthics in AI, Clinical Decision Support\n1 Introduction\nDeep learning has revolutionized the way we understand human behavior, emotions, and healthcare-related chal-\nlenges [1, 2, 3, 4]. In recent years, breakthroughs in clinic al language processing have paved the way for transfor-\nmative changes in the healthcare industry. These advanceme nts hold great promise for the deployment of intelligent\nsystems that can support decision-making, accelerate diag nostic work\ufb02ows, and enhance the quality of patient care.\nSuch systems have the potential to assist healthcare profes sionals as they navigate the growing body of medical knowl-\nedge, interpret complex patient records, and craft individ ualized treatment plans. The promise of these systems has\ngenerated signi\ufb01cant excitement within the healthcare com munity [5, 6, 7].\nThe power of large language models (LLMs) lies in their abili ty to analyze vast amounts of medical literature, patient\ndata, and the rapidly growing body of clinical research. Hea lthcare data [8, 9] is inherently intricate, heterogeneous ,\nand extensive. LLMs function as critical tools that help all eviate information overload for healthcare professionals .\nBy automating the processing of medical texts, extracting k ey insights, and applying the knowledge, LLMs have\nthe potential to drive signi\ufb01cant research breakthroughs a nd improve patient care, contributing meaningfully to the\nevolution of the medical \ufb01eld.\nThe excitement surrounding LLMs is largely driven by the imp ressive capabilities of advanced models like OpenAI\u2019s\nGPT -3.5, GPT -4 [10, 11], and Google\u2019s Bard. These models hav e shown remarkable pro\ufb01ciency across a broad range\nof natural language understanding tasks, underscoring the ir pivotal role in healthcare applications. With their abil ity to\ncomprehend and generate human-like text, these models are s et to have a transformative impact on healthcare, where\naccurate communication and information management are par amount [12].\nNatural language processing (NLP) has undergone signi\ufb01can t advancements, with each milestone building on the\nstrengths and limitations of previous approaches. Early de velopments, such as recurrent neural networks (RNNs),\nlaid the groundwork for contextual understanding in NLP tas ks. However, their limitations in handling long-range\ndependencies became clear, necessitating new approaches i n the \ufb01eld.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a25db37d-f01e-4170-a8e6-3bffd64191f3": {"__data__": {"id_": "a25db37d-f01e-4170-a8e6-3bffd64191f3", "embedding": null, "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b85b298-fd49-4ba8-9383-736c92d57466", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7cf3a297c91158e3e218340590b9983e3e9fc1c030b3d9964ccfaecbc4a5a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52841cf5-5677-4734-b950-c4c2d22a498a", "node_type": "1", "metadata": {}, "hash": "16f822e3a76357f724db4404dbef4751c9ec26cf45bfde6b486f0ef01f7bbb0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nThe turning point came with the introduction of the Transfor mer architecture, which effectively addressed the challen ge\nof capturing distant relationships between words. This inn ovation was crucial for the development of more advanced\nNLP models. The advent of sophisticated language models suc h as Llama 2 [13] and GPT -4, both of which bene\ufb01t\nfrom extensive training datasets, has propelled NLP to new h eights, allowing for deeper understanding and near-\nhuman-level text generation.\nWithin healthcare, specialized versions of models like BER T , including BioBER T and ClinicalBER T [14, 15], were de-\nveloped to address the unique challenges of clinical langua ge, such as medical terminology, ambiguity, and variabilit y\nin usage. However, the use of LLMs in the highly sensitive hea lthcare sector requires careful consideration of privacy,\nsecurity, and ethics. Patient data must be rigorously prote cted, and models must be designed to avoid perpetuating\nbiases or causing harm. Despite these challenges, the poten tial for LLMs to improve healthcare outcomes and drive\ninnovation remains a key focus of ongoing research and devel opment.\nThis review serves as a comprehensive guide for medical rese archers and healthcare professionals aiming to optimize\nthe use of LLMs in their practices. It provides a detailed exp loration of LLM technologies, their applications in\nhealthcare, and critical discussions on fairness, bias, pr ivacy, transparency, and ethical considerations. By addre ssing\nthese aspects, this review highlights the importance of int egrating LLMs into healthcare in a responsible, equitable,\nand effective manner to maximize bene\ufb01ts for both patients a nd providers.\nThe paper is organized into the following sections:\n\u2022 Section 2 introduces the fundamental architecture of LLMs, includin g key components such as Transformers,\nfoundational models, and their multi-modal capabilities.\n\u2022 Section 3 explores the application of LLMs in healthcare, detailing t heir various use cases and the perfor-\nmance metrics used to evaluate them in clinical environment s.\n\u2022 Section 4 delves into the challenges that LLMs face in healthcare, foc using on issues such as explainability,\nsecurity, bias, and ethical concerns.\n\u2022 Finally, the paper concludes with a summary of the \ufb01ndings, discussing the transformative potential of LLMs\nwhile addressing the need for careful implementation to mit igate limitations and ethical challenges.\n2 Overview of Large Language Models\nLarge language models (LLMs) have rapidly advanced due to their ability to understand and generate human-like\ntext across a variety of natural language processing (NLP) t asks [16, 10]. These models are distinguished by their\nextensive number of parameters, pre-training on vast text d atasets, and subsequent \ufb01ne-tuning for speci\ufb01c tasks [17,\n18, 13]. In this section, we examine the core architecture of LLMs, highlight key examples, and explore pre-training\nmethodologies as well as the role of transfer learning [19].\nLLMs leverage the Transformer architecture, which excels in capturing long-range dependencies within text [20]. The\nself-attention mechanism inherent to this architecture en ables models to focus on different parts of the input text bas ed\non their relevance, improving the handling of complex lingu istic relationships.\n2.1 T ransformers and Their Role in Language Models\nA hallmark of LLMs is their scale [21, 22], pre-training on im mense text corpora [23, 13], and the \ufb01ne-tuning process\ntailored to particular tasks [24]. These models, composed o f billions of parameters, are designed to recognize intrica te\npatterns in language data. After undergoing broad pre-trai ning, they are re\ufb01ned using smaller, task-speci\ufb01c datasets ,\nresulting in enhanced performance across a variety of NLP ap plications.\nThe introduction of the Transformer framework revolutioni zed the \ufb01eld by addressing the limitations of earlier archi-\ntectures like recurrent neural networks (RNNs) [20]. This e volution led to the development of powerful models like\nGPT -4 [11] and Llama 2 [13], signi\ufb01cantly improving natural language understanding and generation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52841cf5-5677-4734-b950-c4c2d22a498a": {"__data__": {"id_": "52841cf5-5677-4734-b950-c4c2d22a498a", "embedding": null, "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b85b298-fd49-4ba8-9383-736c92d57466", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7cf3a297c91158e3e218340590b9983e3e9fc1c030b3d9964ccfaecbc4a5a5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a25db37d-f01e-4170-a8e6-3bffd64191f3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7089467e7e79e4c8f116b55284c233f679f8c98b86c20c9eb70e5b769c4bc61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After undergoing broad pre-trai ning, they are re\ufb01ned using smaller, task-speci\ufb01c datasets ,\nresulting in enhanced performance across a variety of NLP ap plications.\nThe introduction of the Transformer framework revolutioni zed the \ufb01eld by addressing the limitations of earlier archi-\ntectures like recurrent neural networks (RNNs) [20]. This e volution led to the development of powerful models like\nGPT -4 [11] and Llama 2 [13], signi\ufb01cantly improving natural language understanding and generation.\n2.2 Multi-Modal Language Models: Expanding Capabilities\nA signi\ufb01cant progression in AI is the rise of multi-modal lan guage models (MLLMs), which integrate data from mul-\ntiple sources, such as text, images, and audio. These models , such as BLIP-2 [25], extend the traditional capabilities\nof LLMs by incorporating multiple modalities, allowing for more versatile and robust outputs [26]. MLLMs enable\ntasks such as visual question answering (VQA) and cross-mod al content generation, opening up new possibilities for\nreal-world applications.\n2", "mimetype": "text/plain", "start_char_idx": 3730, "end_char_idx": 4776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d51845d3-4b44-4344-9d00-dc96c6f86884": {"__data__": {"id_": "d51845d3-4b44-4344-9d00-dc96c6f86884", "embedding": null, "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "279328f0-56f2-427d-ae9e-01db9f67472c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d097a1fb7909df82c71f98d6909205036412b9179f3fb11763914caf2e9be7c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 1: Summary of Multi-Modal Language Models\nModel Y ear Capabilities Applications\nBLIP-2 [25] 2023 Image-text integration using\nQformer\nV isual question answering, image-\ntext retrieval\nV isual ChatGPT [26] 2023 T ext and image interaction via GPT Complex queries requiring visual\ninputs\nMoV A [27] 2024 Mixture of experts for image and\ntext\nMulti-modal content generation\nand analysis\nT able 2: Overview of Large Language Models in Healthcare\nModel Y ear Use Case Institution Source\nCode\nBioMistral [30] 2024 Medical Question Answer-\ning\nA vignon Universit\u00e9,\nNantes Universit\u00e9\nmodel\nMed-PaLM 2\n[31]\n2023 Medical Question Answer-\ning\nGoogle Research, Deep-\nMind\nRadiology-\nLlama2 [32]\n2023 Radiology Imaging Analy-\nsis\nUniversity of Georgia\nDeID-GPT\n[33]\n2023 Data De-identi\ufb01cation University of Georgia code\nMed-HAL T\n[34]\n2023 Hallucination Detection Saama AI Research code\nChatCAD [35] 2023 Computer-Aided Diagno-\nsis\nShanghaiT ech Univer-\nsity\ncode\nBioGPT [36] 2023 Classi\ufb01cation, Relation Ex-\ntraction, Question Answer-\ning\nMicrosoft Research code\nGatorTron [37] 2022 Medical T extual Similarity,\nInference, Question An-\nswering\nUniversity of Florida code\n2.3 Applications of Large Language Models in Healthcare\nLLMs have also become prominent in healthcare, where they support tasks such as medical diagnostics, patient care,\nand drug discovery [28, 29]. T ailored models like BioBER T [1 4] and ClinicalBER T [15] are designed to handle the\nspecialized language found in medical records and research . Newer models, including GPT -4 and Google\u2019s Bard, are\nsetting new benchmarks in medical question answering and re lated healthcare applications [6].\n2.4 Real-W orld Healthcare Applications of Large Language M odels\nLLMs have been widely adopted across various healthcare fun ctions, with applications continuing to expand rapidly.\nThese models assist in clinical decision-making, analysis of medical records, and improving patient interactions [38 ].\nThe vast capability of LLMs to process medical data offers be ne\ufb01ts in areas such as diagnostics, administrative ef\ufb01-\nciency, and overall healthcare delivery [39, 40].\n\u2022 Medical Diagnostics: LLMs can help physicians diagnose illnesses by analyzing pa tient data, including\nsymptoms and medical histories, to identify potential heal th conditions [41].\n\u2022 Patient Care: Through personalized recommendations and ongoing patient monitoring, LLMs improve the\nquality of patient care by providing real-time insights [42 ].\n\u2022 Clinical Decision Support: LLMs offer healthcare professionals evidence-based recom mendations, enhanc-\ning clinical decision-making and treatment strategies [43 ].\n\u2022 Medical Literature Review: By summarizing large volumes of medical literature, LLMs he lp healthcare\nprofessionals stay current with new developments and best p ractices [44].\n\u2022 Drug Discovery: LLMs facilitate drug discovery by analyzing molecular data to identify potential com-\npounds for new drugs [28, 45].\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "704d03f9-b78f-479f-b0c0-7b0c1576ae53": {"__data__": {"id_": "704d03f9-b78f-479f-b0c0-7b0c1576ae53", "embedding": null, "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "350146c2-9a3c-400f-873f-92b3b564abe3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "65c7e94fa2a0ba9b58f1482ba11cb2aefa81b91919d7bd49785ead0c53713e97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 3: Evaluation Metrics for LLMs in Healthcare Applicat ions\nMetric T ask Description Key Results\nPerplexity Language Generation Measures model uncertainty Lower perplexity indicates better\nlanguage generation performance\nBLEU Translation Evaluates overlap between\ngenerated and reference text\nClinicalGPT achieved a BLEU\nscore of 13.9 [48]\nROUGE Summarization Assesses recall of generated\nsummaries\nBioMedLM attained a ROUGE-L\nscore of 24.85 [49]\nF1 Score Classi\ufb01cation Combines precision and re-\ncall for a balanced metric\nGatorTron obtained an F1 score of\n0.9627 for medical relation extrac-\ntion [37]\nT able 4: Benchmark Comparison of Large Language Models\nModel MMLU Score HumanEval (Coding) Release Date\nGPT -4 Turbo 86.4 85.4 April 2024\nClaude 3.5 88.7 92.0 June 2024\nLlama 3 86.1 81.7 March 2024\nGemini Ultra 83.7 74.3 December 2023\n\u2022 Virtual Health Assistants: LLMs serve as the backbone for healthcare chatbots that prov ide continuous\nhealth monitoring and medical advice [46].\n\u2022 Radiology and Imaging: Multi-modal LLMs assist radiologists by analyzing imaging data and improving\ndiagnostic precision [47].\n\u2022 Automated Report Generation: LLMs automate the generation of medical reports from diagno stic images,\nspeeding up work\ufb02ows in radiology and pathology [35].\n2.5 Performance Metrics and Model Comparisons\nBenchmarking LLM performance is crucial for assessing their effectiveness across different healthcare tasks. Com-\nmonly used benchmarks, such as MMLU (Massive Multitask Lang uage Understanding) and HumanEval, evaluate\nLLMs on various tasks, including problem-solving and code g eneration [50, 51]. T able 4 presents a comparison of\nseveral state-of-the-art models based on these benchmarks .\n3 Challenges and Future Directions\nThe incorporation of large language models (LLMs) in health care is not without obstacles. These hurdles include the\nneed for greater transparency in model decisions, ensuring data privacy and security for sensitive patient informatio n,\naddressing biases to guarantee fairness, preventing the ge neration of false or misleading outputs, and establishing\nregulatory frameworks for ethical AI use in medical context s. Overcoming these challenges is vital for fully harnessin g\nLLMs\u2019 potential to improve healthcare while maintaining et hical and legal standards.\n3.1 Improving Model T ransparency and Interpretability\nOne signi\ufb01cant challenge when applying LLMs in healthcare is their lack of interpretability. These models often\nfunction as \"black boxes,\" making it dif\ufb01cult for healthcar e providers to understand how speci\ufb01c recommendations\nor predictions are generated. This lack of clarity can hinde r adoption, as medical professionals require transparent\ndecision-making processes to ensure accuracy and trust. In healthcare, where every decision must be well-founded, the\nopaque nature of LLMs is particularly problematic. T o addre ss this, efforts are underway to develop more interpretable\nmodels that offer insight into their decision-making proce sses, fostering trust in AI-generated recommendations [52 ,\n53]. Enhancing transparency and interpretability remains a key research focus in healthcare AI [54, 55, 56].\n3.2 Data Privacy and Security Risks\nWhen applied in healthcare settings, LLMs handle vast amoun ts of sensitive information, including personally identi-\n\ufb01able data. Ensuring this data is processed and stored secur ely, in compliance with privacy regulations, is a signi\ufb01can t\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de8bd408-a7ff-46b0-8615-27b3262fc1bc": {"__data__": {"id_": "de8bd408-a7ff-46b0-8615-27b3262fc1bc", "embedding": null, "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63146ebb-319f-461f-8f5a-86cc7adc1498", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fa489a893cc9bcf57eb002d0c799b0263eb80b919810a5f6fc5cbe33692a7464", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nchallenge. One concern is the unintentional exposure of per sonal health information (PHI) during the training pro-\ncess, which could lead to privacy violations. Furthermore, the ability of LLMs to infer sensitive information from\nanonymized data presents additional privacy risks [57]. T o mitigate these threats, it is essential to implement robust\nanonymization techniques, secure data storage, and compli ance with ethical guidelines, ensuring that patient data\nremains protected throughout the use of LLMs in healthcare [ 58].\n3.3 Ensuring Fairness and Reducing Bias\nLLMs can inherit biases from the data they are trained on, particularly if the datasets include unequal representations\nof demographic groups or healthcare outcomes. These biases can lead to disparities in medical recommendations\nand outcomes, which can be harmful in clinical settings. Res earchers must develop strategies to identify, reduce,\nand prevent biases within these models, ensuring that LLMs c ontribute to equitable healthcare solutions. Ongoing\naudits and evaluations are critical for identifying and mit igating biases in both training data and model outputs [58].\nCollaboration between domain experts, data scientists, an d ethicists can foster the development of fair and unbiased\nAI in healthcare.\n3.4 Preventing Hallucinations in Medical AI\nLLMs sometimes generate false or misleading information\u2014commonly referred to as hallucinations\u2014which can be\nparticularly dangerous in healthcare applications where a ccuracy is critical. These models may produce plausible-\nsounding, but factually incorrect, content without provid ing traceable sources [59]. Healthcare professionals must\nbe cautious when using LLMs, validating AI-generated conte nt to avoid the risks associated with incorrect medical\nguidance. Current research is focused on addressing these h allucination challenges, with benchmarks like Med-HAL T\nbeing developed to evaluate how well models perform in medic al reasoning and information retrieval [34].\n3.5 Legal, Ethical, and Regulatory Frameworks\nThe use of LLMs in healthcare also raises signi\ufb01cant legal an d ethical questions. Issues such as the generation of sensi-\ntive or distressing medical content, or the potential for sp reading misinformation, necessitate strict regulatory ov ersight.\nFurthermore, there are concerns about plagiarism, imperso nation, and the overall integrity of LLM-generated content .\nRegulatory frameworks, such as the EU\u2019s AI Act and the U.S. HI P AA, provide essential guidelines for the safe and re-\nsponsible deployment of AI in healthcare [57, 60]. These law s ensure patient data protection and set ethical standards\nfor the use of AI technologies in sensitive environments, fo stering trust and accountability in AI-powered healthcare .\n4 Closing Remarks\nThe adoption of large language models in healthcare present s substantial opportunities for enhancing medical decisio n-\nmaking and information retrieval. These models, equipped w ith advanced capabilities, have the potential to improve\nwork\ufb02ows and patient outcomes across various healthcare ap plications. However, realizing their full potential re-\nquires overcoming key challenges such as ensuring model tra nsparency, protecting sensitive data, reducing biases, an d\npreventing erroneous outputs. As researchers and practiti oners continue to collaborate, the focus must remain on deve l-\noping ethical, trustworthy, and fair AI systems that meet th e rigorous standards of healthcare. Continued innovation,\ncombined with careful consideration of ethical and regulat ory concerns, will shape the future of LLMs in medical\npractice.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f02b3c1a-6580-4633-b9b9-c28dfa27a808": {"__data__": {"id_": "f02b3c1a-6580-4633-b9b9-c28dfa27a808", "embedding": null, "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2cf2e60-32c6-498b-bdc6-785575e4f765", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d01cd98e32a11bb6e6a3fc685fbb5df0d104b8f0baa5bcb6428a50631d14af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ae011c2-b13b-4c81-9f02-d01836c5b13d", "node_type": "1", "metadata": {}, "hash": "6c6a3d2da056e9e34e71748d6e6f25f827cac04d6b570ac169169ad1f972852c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 5: Overview of Challenges and Mitigation Strategies f or LLMs in Healthcare\nChallenge Impact Proposed Solution\nTransparency Lack of understanding in AI-\ngenerated decisions\nDevelop interpretable models and provide\ndecision explanations\nData Security Risk of exposing sensitive\npatient information\nUse advanced anonymization and secure\ndata storage protocols\nBias Perpetuation of unfair treat-\nment outcomes\nConduct regular bias audits and collaborate\nwith domain experts\nHallucinations Creation of inaccurate or\nmisleading content\nImplement rigorous validation and special-\nized benchmarks like Med-HAL T\nEthical and Legal\nConcerns\nRisk of misuse and data\nbreaches\nComply with regulations such as HIP AA\nand the AI Act, and ensure ethical use of\nAI\nReferences\n[1] Henglin Shi, W ei Peng, Haoyu Chen, Xin Liu, and Guoying Zh ao. Multiscale 3d-shift graph convolution network\nfor emotion recognition from human actions. IEEE Intelligent Systems , 37(4):103\u2013110, 2022.\n[2] Hao Y u, Xu Cheng, W ei Peng, W eihao Liu, and Guoying Zhao. M odality unifying network for visible-infrared\nperson re-identi\ufb01cation. In Proceedings of the IEEE/CVF International Conference on Co mputer V ision , pages\n11185\u201311195, 2023.\n[3] Y ante Li, W ei Peng, and Guoying Zhao. Micro-expression a ction unit detection with dual-view attentive\nsimilarity-preserving knowledge distillation. In 2021 16th IEEE International Conference on Automatic F ace\nand Gesture Recognition (FG 2021) , pages 01\u201308. IEEE, 2021.\n[4] Xiaopeng Hong, W ei Peng, Mehrtash Harandi, Ziheng Zhou, Matti Pietik\u00e4inen, and Guoying Zhao. Characteriz-\ning subtle facial movements via riemannian manifold. ACM T ransactions on Multimedia Computing, Communi-\ncations, and Applications (TOMM) , 15(3s):1\u201324, 2019.\n[5] Kai He, Rui Mao, Qika Lin, Y ucheng Ruan, Xiang Lan, Mengli ng Feng, and Erik Cambria. A survey of large\nlanguage models for healthcare: from data, technology, and applications to accountability and ethics. arXiv\npreprint arXiv:2310.05694 , 2023.\n[6] Y uqing W ang, Y un Zhao, and Linda Petzold. Are large langu age models ready for healthcare? a comparative\nstudy on clinical language understanding. arXiv preprint arXiv:2304.05368 , 2023.\n[7] Ping Y u, Hua Xu, Xia Hu, and Chao Deng. Leveraging generat ive ai and large language models: a comprehensive\nroadmap for healthcare integration. In Healthcare, volume 11, page 2776. MDPI, 2023.\n[8] W ei Peng, Li Feng, Guoying Zhao, and Fang Liu. Learning op timal k-space acquisition and reconstruction using\nphysics-informed neural networks. In Proceedings of the IEEE/CVF Conference on Computer V ision a nd P attern\nRecognition, pages 20794\u201320803, 2022.\n[9] W ei Peng, Ehsan Adeli, T omas Bosschieter, Sang Hyun Park , Qingyu Zhao, and Kilian M Pohl. Generating\nrealistic brain mris via a conditional diffusion probabili stic model. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention , pages 14\u201324. Springer, 2023.\n[10] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4ae011c2-b13b-4c81-9f02-d01836c5b13d": {"__data__": {"id_": "4ae011c2-b13b-4c81-9f02-d01836c5b13d", "embedding": null, "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2cf2e60-32c6-498b-bdc6-785575e4f765", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d01cd98e32a11bb6e6a3fc685fbb5df0d104b8f0baa5bcb6428a50631d14af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f02b3c1a-6580-4633-b9b9-c28dfa27a808", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e6f2ebf98bd8f55aaab785e3d1f950f72937f096f0adbad847170ce42096a15a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, 2023.\n[10] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Y u Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong T ae Kim, Jinwoo Choi, et al. One smal l step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488 , 2023.\n[13] Hugo T ouvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al . Llama: Open and ef\ufb01cient foundation language\nmodels. arXiv preprint arXiv:2302.13971 , 2023.\n[14] Jinhyuk Lee, W onjin Y oon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representatio n model for biomedical text mining. Bioinformatics,\n36(4):1234\u20131240, 2020.\n6", "mimetype": "text/plain", "start_char_idx": 3028, "end_char_idx": 4171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9b03e82-7837-484a-adcd-3c8d49d330aa": {"__data__": {"id_": "b9b03e82-7837-484a-adcd-3c8d49d330aa", "embedding": null, "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8db6264f-0b28-4326-88db-6038bc565edc", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2d92819a183ee8d538db7bb39b4af15e365157c8c662a35200df793549fff6ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9d3b38d-2100-4cca-bac8-dd421ca3556a", "node_type": "1", "metadata": {}, "hash": "b01f1334c35940b22077c78eaee0560b554d07376fedf638635407170207deb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[15] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clin icalbert: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342 , 2019.\n[16] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton B akhtin, Y uxiang Wu, Alexander H Miller, and Sebastian\nRiedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 , 2019.\n[17] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya S utskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maa rten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung W on Chung, Charles Sutton, Sebastian Gehrman n, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311 , 2022.\n[19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dari o Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[20] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\n[21] William Fedus, Barret Zoph, and Noam Shazeer. Switch tr ansformers: Scaling to trillion parameter models with\nsimple and ef\ufb01cient sparsity. The Journal of Machine Learning Research , 23(1):5232\u20135270, 2022.\n[22] Nan Du, Y anping Huang, Andrew M Dai, Simon T ong, Dmitry L epikhin, Y uanzhong Xu, Maxim Krikun, Y anqi\nZhou, Adams W ei Y u, Orhan Firat, et al. Glam: Ef\ufb01cient scalin g of language models with mixture-of-experts.\nIn International Conference on Machine Learning , pages 5547\u20135569. PMLR, 2022.\n[23] Haifeng W ang, Jiwei Li, Hua Wu, Eduard Hovy, and Y u Sun. P re-trained language models and their applications.\nEngineering, 2022.\n[24] Jason W ei, Maarten Bosma, V incent Y Zhao, Kelvin Guu, Ad ams W ei Y u, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learn ers. arXiv preprint arXiv:2109.01652 , 2021.\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. B lip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning , pages\n19730\u201319742. PMLR, 2023.\n[26] Chenfei Wu, Shengming Y in, W eizhen Qi, Xiaodong W ang, Z echeng T ang, and Nan Duan. V isual chatgpt:\nT alking, drawing and editing with visual foundation models . arXiv preprint arXiv:2303.04671 , 2023.\n[27] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Ha o Shao, Dongzhi Jiang, Hongsheng Li, and Y u Liu.\nMova: Adapting mixture of vision experts to multimodal cont ext. arXiv preprint arXiv:2404.13046 , 2024.\n[28] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Rui li Huang, and W eida T ong. Ai-based language\nmodels powering drug discovery and development.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9d3b38d-2100-4cca-bac8-dd421ca3556a": {"__data__": {"id_": "f9d3b38d-2100-4cca-bac8-dd421ca3556a", "embedding": null, "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8db6264f-0b28-4326-88db-6038bc565edc", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2d92819a183ee8d538db7bb39b4af15e365157c8c662a35200df793549fff6ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9b03e82-7837-484a-adcd-3c8d49d330aa", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d229650a63533ea189dbaceeb92b874beec828abea520ba6d230ab308f5c65f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[27] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Ha o Shao, Dongzhi Jiang, Hongsheng Li, and Y u Liu.\nMova: Adapting mixture of vision experts to multimodal cont ext. arXiv preprint arXiv:2404.13046 , 2024.\n[28] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Rui li Huang, and W eida T ong. Ai-based language\nmodels powering drug discovery and development. Drug Discovery T oday , 26(11):2593\u20132607, 2021.\n[29] T anmoy T apos Datta, Pintu Chandra Shill, and Zabir Al Na zi. Bert-d2: Drug-drug interaction extraction using\nbert. In 2022 International Conference for Advancement in T echnolo gy (ICONAT) , pages 1\u20136. IEEE, 2022.\n[30] Y anis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-A ntoine Gourraud, Mickael Rouvier, and Richard Du-\nfour. Biomistral: A collection of open-source pretrained l arge language models for medical domains. arXiv\npreprint arXiv:2402.10373 , 2024.\n[31] Karan Singhal, T ao Tu, Juraj Gottweis, Rory Sayres, Ell ery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, et al. T owards expert-le vel medical question answering with large language\nmodels. arXiv preprint arXiv:2305.09617 , 2023.\n[32] Zhengliang Liu, Y iwei Li, Peng Shu, Aoxiao Zhong, Longt ao Y ang, Chao Ju, Zihao Wu, Chong Ma, Jie\nLuo, Cheng Chen, et al. Radiology-llama2: Best-in-class la rge language model for radiology. arXiv preprint\narXiv:2309.06419 , 2023.\n[33] Zhengliang Liu, Xiaowei Y u, Lu Zhang, Zihao Wu, Chao Cao , Haixing Dai, Lin Zhao, W ei Liu, Dinggang\nShen, Quanzheng Li, et al. Deid-gpt: Zero-shot medical text de-identi\ufb01cation by gpt-4. arXiv preprint\narXiv:2303.11032 , 2023.\n[34] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sank arasubbu. Med-halt: Medical domain hallucination\ntest for large language models. arXiv preprint arXiv:2307.15343 , 2023.\n[35] Zihao Zhao, Sheng W ang, Jinchen Gu, Y itao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian W ang,\nand Dinggang Shen. Chatcad+: T owards a universal and reliab le interactive cad using llms. arXiv preprint\narXiv:2305.15964 , 2023.\n7", "mimetype": "text/plain", "start_char_idx": 2569, "end_char_idx": 4623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ce040ea-1e5e-4b93-8546-b1cfc86ced1e": {"__data__": {"id_": "8ce040ea-1e5e-4b93-8546-b1cfc86ced1e", "embedding": null, "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b74ff41-9bfd-488f-9319-a5f56fe276b0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "597864202a41ebf96c58358911291ab4d501dd7bf1c97582cd068fa3d0e1965e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9337de9-5869-4490-9591-5d05bfb30f5a", "node_type": "1", "metadata": {}, "hash": "a8755e306e0abfafc9f5a2e061c6913448ca4ffa4f09858fd81c52a493541d33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[36] Renqian Luo, Liai Sun, Y ingce Xia, T ao Qin, Sheng Zhang, Hoifung Poon, and Tie-Y an Liu. Biogpt: generative\npre-trained transformer for biomedical text generation an d mining. Brie\ufb01ngs in Bioinformatics , 23(6):bbac409,\n2022.\n[37] Xi Y ang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Com-\npas, Cheryl Martin, Mona G Flores, Y ing Zhang, et al. Gatortr on: A large clinical language model to unlock\npatient information from unstructured electronic health r ecords. arXiv preprint arXiv:2203.03540 , 2022.\n[38] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L\nMcClelland, and Felix Hill. Language models show human-lik e content effects on reasoning. arXiv preprint\narXiv:2207.07051 , 2022.\n[39] Hongyang Li, Richard C Gerkin, Alyssa Bakke, Raquel Nor el, Guillermo Cecchi, Christophe Laudamiel,\nMasha Y Niv, Kathrin Ohla, John E Hayes, V alentina Parma, et a l. T ext-based predictions of covid-19 diag-\nnosis from self-reported chemosensory descriptions. Communications Medicine , 3(1):104, 2023.\n[40] Felix Agbavor and Hualou Liang. Predicting dementia fr om spontaneous speech using large language models.\nPLOS Digital Health , 1(12):e0000168, 2022.\n[41] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Bo osting transformers and language models for clinical\nprediction in immunotherapy. arXiv preprint arXiv:2302.12692 , 2023.\n[42] Stephen R Ali, Thomas D Dobbs, Hayley A Hutchings, and Ia in S Whitaker. Using chatgpt to write patient clinic\nletters. The Lancet Digital Health , 5(4):e179\u2013e181, 2023.\n[43] Rubeta N Matin, Eleni Linos, and Neil Rajan. Leveraging large language models in dermatology, 2023.\n[44] Malik Sallam. The utility of chatgpt as an example of lar ge language models in healthcare education, research\nand practice: Systematic review on the future perspectives and potential limitations. medRxiv, pages 2023\u201302,\n2023.\n[45] G\u00f6k\u00e7e Uludo \u02d8gan, Elif Ozkirimli, Kutlu O Ulgen, Nilg\u00fcn Karal\u0131, and Arzuc an \u00d6zg\u00fcr. Exploiting pretrained\nbiochemical language models for targeted drug design. Bioinformatics, 38(Supplement_2):ii155\u2013ii161, 2022.\n[46] Desir\u00e9e Bill and Theodor Eriksson. Fine-tuning a llm us ing reinforcement learning from human feedback for a\ntherapy chatbot application, 2023.\n[47] Lei Ma, Jincong Han, Zhaoxin W ang, and Dian Zhang. Cephg pt-4: An interactive multimodal cephalometric\nmeasurement and diagnostic system with visual large langua ge model. arXiv preprint arXiv:2307.07518 , 2023.\n[48] Guangyu W ang, Guoxing Y ang, Zongxin Du, Longjun Fan, an d Xiaohu Li. Clinicalgpt: Large language models\n\ufb01netuned with diverse medical data and comprehensive evalu ation. arXiv preprint arXiv:2306.09968 , 2023.\n[49] Jianquan Li, Xidong W ang, Xiangbo Wu, Zhiyi Zhang, Xiao long Xu, Jie Fu, Prayag Tiwari, Xiang W an, and\nBenyou W ang. Huatuo-26m, a large-scale chinese medical qa d ataset. arXiv preprint arXiv:2305.01526 , 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9337de9-5869-4490-9591-5d05bfb30f5a": {"__data__": {"id_": "a9337de9-5869-4490-9591-5d05bfb30f5a", "embedding": null, "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b74ff41-9bfd-488f-9319-a5f56fe276b0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "597864202a41ebf96c58358911291ab4d501dd7bf1c97582cd068fa3d0e1965e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ce040ea-1e5e-4b93-8546-b1cfc86ced1e", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2f4d66f8b9e7304368dc74e495b59dd1b9c707e859d7c62f6092b2774ba469d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:2306.09968 , 2023.\n[49] Jianquan Li, Xidong W ang, Xiangbo Wu, Zhiyi Zhang, Xiao long Xu, Jie Fu, Prayag Tiwari, Xiang W an, and\nBenyou W ang. Huatuo-26m, a large-scale chinese medical qa d ataset. arXiv preprint arXiv:2305.01526 , 2023.\n[50] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, M antas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.\n[51] Mark Chen, Jerry T worek, Heewoo Jun, Qiming Y uan, Henri que Ponde De Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Y uri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 , 2021.\n[52] Hazrat Ali, Junaid Qadir, T anvir Alam, Mowafa Househ, a nd Zubair Shah. Chatgpt and large language models\n(llms) in healthcare: Opportunities and risks. 2023.\n[53] Sandeep Reddy. Evaluating large language models for us e in healthcare: A framework for translational value\nassessment. Informatics in Medicine Unlocked , page 101304, 2023.\n[54] Giovanni Briganti. A clinician\u2019s guide to large langua ge models. Future Medicine AI , (0):FMAI, 2023.\n[55] Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaa r, Boris Delibasic, Pietro Lio, and Andrija Petrovic.\nInterpretable medical diagnostics with structured data ex traction by large language models. arXiv preprint\narXiv:2306.05052 , 2023.\n[56] Y an Jiang, Ruihong Qiu, Y i Zhang, and Peng-Fei Zhang. Ba lanced and explainable social media analysis for\npublic health with large language models. arXiv preprint arXiv:2309.05951 , 2023.\n[57] Jesutofunmi A Omiye, Haiwen Gui, Shawheen J Rezaei, Jam es Zou, and Roxana Daneshjou. Large language\nmodels in medicine: the potentials and pitfalls. arXiv preprint arXiv:2309.00087 , 2023.\n[58] Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, b ard, and large language models for biomedical research:\nOpportunities and pitfalls. Annals of Biomedical Engineering , pages 1\u20135, 2023.\n8", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 4794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d24aeab-16a9-41e5-90ca-4b69a2b031c1": {"__data__": {"id_": "1d24aeab-16a9-41e5-90ca-4b69a2b031c1", "embedding": null, "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14e3892b-29c1-4c5a-b868-790337ea1e4b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bdc0bd2fb6675e97778e68b5cd33928a991e03207b2294c07d434db39d20181d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[59] Shubo Tian, Qiao Jin, Lana Y eganova, Po-Ting Lai, Qingq ing Zhu, Xiuying Chen, Y ifan Y ang, Qingyu Chen,\nW on Kim, Donald C Comeau, et al. Opportunities and challenge s for chatgpt and large language models in\nbiomedicine and health. arXiv preprint arXiv:2306.10070 , 2023.\n[60] Claudio Novelli, Federico Casolari, Philipp Hacker, G iorgio Spedicato, and Luciano Floridi. Generative ai in eu\nlaw: liability, privacy, intellectual property, and cyber security. arXiv preprint arXiv:2401.07348 , 2024.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0eb12a84-69f4-45cc-9aee-72b0024cc6ae": {"__data__": {"id_": "0eb12a84-69f4-45cc-9aee-72b0024cc6ae", "embedding": null, "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd1b1618-e5d1-483a-ab0d-ae93ab9d300c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d45ee42a544cd56be83f60cb4d198ccb1f0d21cb0f70b80e57eb8925f0d40e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cba646f6-7fc1-401f-8645-0d5890dfb387": {"__data__": {"id_": "cba646f6-7fc1-401f-8645-0d5890dfb387", "embedding": null, "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b70994a-8c2a-42ae-b0c0-4a86ce7c3b57", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "22f2fa0e566ecce1b71e1d9c1e623ddef5bf6e657f78b2f68639b53545e03daf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28944c87-6729-4d1a-b73b-4a1058a5f0f1": {"__data__": {"id_": "28944c87-6729-4d1a-b73b-4a1058a5f0f1", "embedding": null, "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fef60c5-e84d-4ad2-bc6d-435da3bc56bd", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "57c0ce6fd4f492a668ef2e58a74a49ee94c5f86c3881f982b1c48ff266e6e7b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "224736fe-83b3-4a3a-852c-89853c34726f": {"__data__": {"id_": "224736fe-83b3-4a3a-852c-89853c34726f", "embedding": null, "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9999bb3b-5e4d-4ecf-8d53-ca9b2eb6ecce", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d1ecab826e3cc304682bad5f49b438a247e7de7e50db3bd7202a6ef63f30e361", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"models_parameter_years_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ebb82a7-dc3b-4fa0-a17b-9177fbcb5b4d": {"__data__": {"id_": "1ebb82a7-dc3b-4fa0-a17b-9177fbcb5b4d", "embedding": null, "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f667b7f-5f2b-4019-950d-aa6bb82831da", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "77a30f6dd69e402a1d108eb352415c1794a574f643b2544a83940568bb7ff5c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v3.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "beb44e96-1fee-48cd-89d3-fc71f9e58855": {"__data__": {"id_": "beb44e96-1fee-48cd-89d3-fc71f9e58855", "embedding": null, "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "306ed2f7-b6ed-4f35-83cc-977631112c8f", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "48d668e32c7e62162e51f76b6bdf2dd67fb966a5aad1d1e79df325c2e9ddca92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v3_old.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "87ccc4f3-14a7-40d1-a7bd-da676bb871c2": {"__data__": {"id_": "87ccc4f3-14a7-40d1-a7bd-da676bb871c2", "embedding": null, "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12794cab-7416-40f2-9cf3-9cc7b3c5aa3c", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6af54316744150a7425f2bd8ad895ff9c9fc927cc7f0c00322c097144a6ebe7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_challenges.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2ef4c5f3-3b4d-457e-ac47-fb2662ce3483": {"__data__": {"id_": "2ef4c5f3-3b4d-457e-ac47-fb2662ce3483", "embedding": null, "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "236a3bb2-64f2-4a86-b396-39fe2f4c6f8e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "936bb7e65df14bdbb68ea1256b7c3aca1c9278070f38626f1bb8fa8c476534cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_challenges_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c29ba2c-16aa-40bf-8629-de63ca5db99d": {"__data__": {"id_": "4c29ba2c-16aa-40bf-8629-de63ca5db99d", "embedding": null, "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "34342f5d-4243-4a80-85bd-87c8a0eb5db3", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cfaf0cd5fc93cc72e43f4828c63382bb4c9ec2fc431d76def5689bf2a856592c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"medllm_comp.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4a824857-e0ad-43d7-86b0-e4cac21a719d": {"__data__": {"id_": "4a824857-e0ad-43d7-86b0-e4cac21a719d", "embedding": null, "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64547a6a-f338-428b-92c3-49cb822749b0", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b74476f80c7a5b60755b5f49920698f2e946b75633b1c5eeae4a9627ca855da7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7e8d3475-8743-421a-b0f0-bf134ff63c4c": {"__data__": {"id_": "7e8d3475-8743-421a-b0f0-bf134ff63c4c", "embedding": null, "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3897af41-de98-441e-9fd4-7f93c7a3eb5f", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95e538c631d6b86b84ae381aecbedf608467fca832d0b61184925861faa74835", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size_v3.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b536a2a6-a5f8-4545-9f19-d5d547f552ed": {"__data__": {"id_": "b536a2a6-a5f8-4545-9f19-d5d547f552ed", "embedding": null, "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4dc3664-baa2-4bec-ad10-4aa114388b2c", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "39dcab11fd47d87ddb16a3b5f96c5f79b40718ce1bc3007f06dc7b05796696bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"multimodal.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60f09d8f-1086-4c8d-8a4a-673ab16b4eaf": {"__data__": {"id_": "60f09d8f-1086-4c8d-8a4a-673ab16b4eaf", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f39b935-39fe-4afb-bd30-8f998b21c4b8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f1d0d195eda002b6ad581bc25912bf5f7e7540c425e89d640ff1b804be51b520", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. J Biomed Inform. 2025 Apr 16:104819. doi: 10.1016/j.jbi.2025.104819. Online \r\nahead of print.\r\n\r\nRoBIn: A Transformer-based model for risk of bias inference with machine reading \r\ncomprehension.\r\n\r\nDias AC(1), Moreira VP(1), Comba JLD(2).\r\n\r\nAuthor information:\r\n(1)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, \r\nPorto Alegre, 91501-970, Rio Grande do Sul, Brazil.\r\n(2)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, \r\nPorto Alegre, 91501-970, Rio Grande do Sul, Brazil. Electronic address: \r\njoao.comba@gmail.com.\r\n\r\nOBJECTIVE: Scientific publications are essential for uncovering insights, \r\ntesting new drugs, and informing healthcare policies. Evaluating the quality of \r\nthese publications often involves assessing their Risk of Bias (RoB), a task \r\ntraditionally performed by human reviewers. The goal of this work is to create a \r\ndataset and develop models that allow automated RoB assessment in clinical \r\ntrials.\r\nMETHODS: We use data from the Cochrane Database of Systematic Reviews (CDSR) as \r\nground truth to label open-access clinical trial publications from PubMed. This \r\nprocess enabled us to develop training and test datasets specifically for \r\nmachine reading comprehension and RoB inference. Additionally, we created \r\nextractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to \r\nextract relevant evidence and classify the RoB effectively.\r\nRESULTS: RoBIn was evaluated across various settings and benchmarked against \r\nstate-of-the-art methods, including large language models (LLMs). In most cases, \r\nthe best-performing RoBIn variant surpasses traditional machine learning and \r\nLLM-based approaches, achieving a AUROC of 0.83.\r\nCONCLUSION: This work addresses RoB assessment in clinical trials by introducing \r\nRoBIn, two Transformer-based models for RoB inference and evidence retrieval, \r\nwhich outperform traditional models and LLMs, demonstrating its potential to \r\nimprove efficiency and scalability in clinical research evaluation. We also \r\nintroduce a public dataset that is automatically annotated and can be used to \r\nenable future research to enhance automated RoB assessment.\r\n\r\nCopyright \u00a9 2025. Published by Elsevier Inc.\r\n\r\nDOI: 10.1016/j.jbi.2025.104819\r\nPMID: 40250743\r\n\r\nConflict of interest statement: Declaration of competing interest The authors \r\ndeclare that they have no known competing financial interests or personal \r\nrelationships that could have appeared to influence the work reported in this \r\npaper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1104fabf-12ee-44e9-b592-9ee2f489702d": {"__data__": {"id_": "1104fabf-12ee-44e9-b592-9ee2f489702d", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca034df0-0341-47e7-bd59-180f46fa2774", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bed46a9b302fb775305509cebf7b12d2626f308449adab58d428dfdeb6e9d0a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Br J Oral Maxillofac Surg. 2025 Mar 24:S0266-4356(25)00059-2. doi: \r\n10.1016/j.bjoms.2025.03.006. Online ahead of print.\r\n\r\nThe impact of the large language model ChatGPT in oral and maxillofacial \r\nsurgery: a systematic review.\r\n\r\nde Menezes Torres LM(1), de Morais EF(2), Fernandes Almeida DRM(3), Pagotto \r\nLEC(4), de Santana Santos T(5).\r\n\r\nAuthor information:\r\n(1)Department of Oral and Maxillofacial Surgery, Universidade de Pernambuco, \r\nRecife, PE, Brazil.\r\n(2)Department of Oral Diagnosis, and Graduate Program in Oral Biology, \r\nPiracicaba Dental School, University of Campinas, Piracicaba, S\u00e3o Paulo, Brazil. \r\nElectronic address: evertonfreitas2@hotmail.com.\r\n(3)School of Dentistry, Federal University of Alfenas (Unifal-MG), Alfenas, \r\nMinas Gerais, Brazil.\r\n(4)Oral and Maxillofacial Surgeon, Hospital S\u00edrio-Libanes, S\u00e3o Paulo, Brazil.\r\n(5)Institute Maxillofacial Education, Aracaju, Sergipe, Brazil.\r\n\r\nThis systematic review evaluates the impact of the large language model (LLM) \r\nChatGPT in oral and maxillofacial surgery. Following PRISMA guidelines and \r\nregistered in PROSPERO (CRD42024625882), the study involved a comprehensive \r\nsearch across PubMed/Medline, Embase, Scopus, and Science Direct. Inclusion \r\ncriteria focused on ChatGPT's use in clinical decision-making, surgical \r\nplanning, patient education, and research. Ten studies were reviewed, assessing \r\nChatGPT's performance in diagnostic accuracy, surgical efficiency, and patient \r\nsatisfaction. GPT-4 achieved the highest accuracy (76.8%) in multiple-choice \r\nquestions but showed variability, with lower performance in pharmacology and \r\ncomplex clinical scenarios. It excelled in generating informed consent \r\ndocuments, outperforming other AI models and human residents in accuracy, \r\ncompleteness, and readability. It also provided accurate and supportive \r\nresponses in postoperative follow up. However, limitations were noted in \r\nhandling complex clinical decisions and providing personalised advice for cases \r\nsuch as oral cancer and orthognathic surgery. While ChatGPT shows potential in \r\nimproving patient communication, reducing healthcare workloads, and providing \r\nup-to-date information, its current limitations in accuracy and personalisation \r\nunderscore the need for human supervision and integration with clinical \r\ndatabases. AI tools like ChatGPT can complement, but should not replace, human \r\njudgment in specialised fields such as oral and maxillofacial surgery.\r\n\r\nCopyright \u00a9 2025 The British Association of Oral and Maxillofacial Surgeons. \r\nPublished by Elsevier Ltd. All rights reserved.\r\n\r\nDOI: 10.1016/j.bjoms.2025.03.006\r\nPMID: 40251084", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9d9fe77-3ccc-4a32-b8ee-93425486aa90": {"__data__": {"id_": "f9d9fe77-3ccc-4a32-b8ee-93425486aa90", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6d25e31-be38-4a95-bbc5-848b8e856517", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d56ef5574295784b966d431186670fbb64cd7922e29b7bcd981ab87505c907b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Adv Simul (Lond). 2025 Apr 18;10(1):22. doi: 10.1186/s41077-025-00350-6.\r\n\r\nArtificial intelligence-assisted academic writing: recommendations for ethical \r\nuse.\r\n\r\nCheng A(1), Calhoun A(2), Reedy G(3).\r\n\r\nAuthor information:\r\n(1)Departments of Pediatrics and Emergency Medicine, Alberta Children's \r\nHospital, Cumming School of Medicine, University of Calgary, 28 Oki Drive NW, \r\nCalgary, Alberta, T3B 6A8, Canada. chenger@me.com.\r\n(2)University of Louisville School of Medicine and Norton Children's Medical \r\nGroup, Louisville, KY, USA.\r\n(3)Faculty of Life Sciences and Medicine, King's College London, London, UK.\r\n\r\nGenerative artificial intelligence (AI) tools have been selectively adopted \r\nacross the academic community to help researchers complete tasks in a more \r\nefficient manner. The widespread release of the Chat Generative Pre-trained \r\nTransformer (ChatGPT) platform in 2022 has made these tools more accessible to \r\nscholars around the world. Despite their tremendous potential, studies have \r\nuncovered that large language model (LLM)-based generative AI tools have issues \r\nwith plagiarism, AI hallucinations, and inaccurate or fabricated references. \r\nThis raises legitimate concern about the utility, accuracy, and integrity of AI \r\nwhen used to write academic manuscripts. Currently, there is little clear \r\nguidance for healthcare simulation scholars outlining the ways that generative \r\nAI could be used to legitimately support the production of academic literature. \r\nIn this paper, we discuss how widely available, LLM-powered generative AI tools \r\n(e.g. ChatGPT) can help in the academic writing process. We first explore how \r\nacademic publishers are positioning the use of generative AI tools and then \r\ndescribe potential issues with using these tools in the academic writing \r\nprocess. Finally, we discuss three categories of specific ways generative AI \r\ntools can be used in an ethically sound manner and offer four key principles \r\nthat can help guide researchers to produce high-quality research outputs with \r\nthe highest of academic integrity.\r\n\r\n\u00a9 2025. The Author(s).\r\n\r\nDOI: 10.1186/s41077-025-00350-6\r\nPMCID: PMC12007126\r\nPMID: 40251634\r\n\r\nConflict of interest statement: Declarations. Ethics approval and consent to \r\nparticipate: Not applicable. Consent for publication: Not Applicable. Competing \r\ninterests: Adam Cheng is an editorial board member and a former Associate Editor \r\nof the journal Simulation in Healthcare. Aaron Calhoun is Associate \r\nEditor-in-Chief and an editorial board member of the journal Simulation in \r\nHealthcare. Gabriel Reedy is Editor-in-Chief and an editorial board member of \r\nthe journal Advances in Simulation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ac5f6796-cb0c-4fbc-9d9d-3080c12897da": {"__data__": {"id_": "ac5f6796-cb0c-4fbc-9d9d-3080c12897da", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae7359a-02d5-48c4-900b-5890bff51b9b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2eeeeb855dba89648403d8f65585aaaecc1259d112ef85d6caaf3b916e56e370", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. SLAS Technol. 2025 Apr 17:100295. doi: 10.1016/j.slast.2025.100295. Online\r\nahead  of print.\r\n\r\nNLP for Computational Insights into Nutritional Impacts on Colorectal Cancer \r\nCare.\r\n\r\nGong S(1), Jin X(2), Guo Y(3), Yu J(4).\r\n\r\nAuthor information:\r\n(1)The Affiliated Hospital of Nantong University, Nantong University, Nantong, \r\nJiangsu, 226001, China. Electronic address: gsn6574172@163.com.\r\n(2)The Affiliated Hospital of Nantong University, Nantong, Jiangsu, 226001, \r\nChina. Electronic address: 1554319715@qq.com.\r\n(3)Nantong University, Nantong, Jiangsu, 226001, China. Electronic address: \r\njyg@ntu.edu.cn.\r\n(4)Nantong University, Nantong, Jiangsu, 226001, China. Electronic address: \r\n1014147829@qq.com.\r\n\r\nColorectal cancer (CRC) is one of the most prominent cancers globally, with its \r\nincidence rising among younger adults due to improved screening practices. \r\nHowever, existing algorithms for CRC prediction are frequently trained on \r\ndatasets that primarily reflect older persons, thus limiting their usefulness in \r\nmore diverse populations. Additionally, the part of nutrition in CRC deterrence \r\nand management is gaining significant attention, although computational \r\napproaches to analyzing the impact of diet on CRC remain underdeveloped. This \r\nresearch introduces the Nutritional Impact on CRC Prediction Framework \r\n(NICRP-Framework), which combines Natural Language Processing (NLP) techniques \r\nwith Adaptive Tunicate Swarm Optimized Large Language Models (ATSO-LLMs) to \r\npresent important insights into the part of the diet in CRC care across diverse \r\npopulations. The colorectal cancer dietary and lifestyle dataset, encompassing \r\nmore than 1000 participants, is collected from multiple regions and sources. The \r\ndataset includes structured and unstructured data, including textual \r\ndescriptions of food ingredients. These descriptions are processed using \r\nstandardization techniques, such as stop word removal, lowercasing, and \r\npunctuation elimination. Relevant terms are then extracted and visualized in a \r\nword cloud. The dataset also contained an imbalanced binary CRC outcome, which \r\nis rebalanced utilizing the random oversampling. ATSO-LLMs are employed to \r\nanalyze the processed dietary data, identifying key nutritional factors and \r\nforecasting CRC and non-CRC phenotypes based on dietary patterns. The results \r\nshow that combining NLP-derived features with ATSO-LLMs significantly enhances \r\nprediction accuracy (98.4%), sensitivity (97.6%) specificity (96.9%) and \r\nF1-Score (96.2%), with minimal misclassification rates. This framework \r\nrepresents a transformative advancement in life science by offering a new, \r\ndata-driven approach to understanding the nutritional determinants of CRC, \r\nempowering healthcare professionals to make more precise predictions and adapted \r\ndietary interventions for diverse populations.\r\n\r\nCopyright \u00a9 2025. Published by Elsevier Inc.\r\n\r\nDOI: 10.1016/j.slast.2025.100295\r\nPMID: 40252975\r\n\r\nConflict of interest statement: Declaration of competing interest The authors \r\ndeclare that they have no known competing financial interests or personal \r\nrelationships that could have appeared to influence the work reported in this \r\npaper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4faf397b-f967-4bdc-8d48-04b309c7776a": {"__data__": {"id_": "4faf397b-f967-4bdc-8d48-04b309c7776a", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7d897be-d7a3-4dc4-9bab-84a1bc880122", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "282c8e59fa837ca5feeed92a5f0a02ee0202298bd69347e167131534d336a495", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Rheumatol Adv Pract. 2024 Sep 18;9(2):rkae119. doi: 10.1093/rap/rkae119. \r\neCollection 2025.\r\n\r\nLarge language models and rheumatology: are we there yet?\r\n\r\nBenavent D(1)(2), Madrid-Garc\u00eda A(3).\r\n\r\nAuthor information:\r\n(1)Rheumatology Department, Hospital Universitari de Bellvitge, Barcelona, \r\nSpain.\r\n(2)Medical Department, Savana Research SL, Madrid, Spain.\r\n(3)Grupo de Patolog\u00eda Musculoesquel\u00e9tica, Hospital Cl\u00ednico San Carlos, Instituto \r\nde Investigaci\u00f3n Sanitaria San Carlos (IdISSC), Madrid, Spain.\r\n\r\nThe last 2 years have marked the beginning of a golden age for natural language \r\nprocessing in medicine. The arrival of large language models (LLMs) and \r\nmultimodal models have raised new opportunities and challenges for research and \r\nclinical practice. In rheumatology, a specialty rich in data and requiring \r\ncomplex decision-making, the use of these tools may transform diagnostic \r\nprocedures, improve patient interaction and simplify data management, leading to \r\nmore personalized and efficient healthcare outcomes. The objective of this \r\narticle is to present an overview of the status of LLMs in the field of \r\nrheumatology while discussing some of the challenges ahead in this area of great \r\npotential.\r\n\r\n\u00a9 The Author(s) 2025. Published by Oxford University Press on behalf of the \r\nBritish Society for Rheumatology.\r\n\r\nDOI: 10.1093/rap/rkae119\r\nPMCID: PMC12007598\r\nPMID: 40256630", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "024a09f3-bd37-405c-9176-e02f67b122f1": {"__data__": {"id_": "024a09f3-bd37-405c-9176-e02f67b122f1", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f3568aa-4847-4e02-bc68-372a1d0fe3ff", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f52808e91b4adbc5290a810899400c51d1182902fe1a9751709b23fc84aa58e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d433150f-dfd9-4628-a5ee-6deba55d12c3", "node_type": "1", "metadata": {}, "hash": "dcf9985fef9140ed1a49e574049e819e0be574e35a65924a800e60a0525d9079", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Slide #0: \nPr\u00e9sentation de projet\nPr\u00e9senter par: Abdoul Fataou Hama et Mahamat Yaya Hissein\nTh\u00e8me: NLP-IA-AIA  \n\n\nSlide #1: \n\n Image: an alarm clock on a computer screen\n\nINTRODUCTION\nL'essor de l'intelligence artificielle et des grands mod\u00e8les de langage (LLM) offre des opportunit\u00e9s in\u00e9dites pour la recherche scientifique et le management d'entreprise. Pourtant, l'acc\u00e8s \u00e0 l'information pertinente et la gestion efficace des connaissances restent des d\u00e9fis majeurs pour les chercheurs, les doctorants, les ing\u00e9nieurs et les managers.\nPourquoi un Assistant IA NLP ?\nChercheurs et doctorants : Difficult\u00e9s \u00e0 trouver et analyser rapidement les travaux pertinents.\nIng\u00e9nieurs : Recherche d'informations techniques et support \u00e0 la r\u00e9daction de documentation.\nEntreprises : Besoin d'un syst\u00e8me de veille strat\u00e9gique et d'aide \u00e0 la prise de d\u00e9cision.\n\n\nSlide #2: \nProbl\u00e9matique et Solution propos\u00e9es\nD\u00e9fis actuels\n\nVolume massif de donn\u00e9es acad\u00e9miques et techniques.\nDifficult\u00e9s \u00e0 extraire des informations pr\u00e9cises et pertinentes.\nTemps n\u00e9cessaire pour analyser et synth\u00e9tiser des documents.\n\n\nSolutions propos\u00e9es\n\nR\u00e9sumer des articles acad\u00e9miques et documents techniques.\nAssister \u00e0 la r\u00e9daction et \u00e0 la reformulation de textes.\nAutomatiser la veille scientifique et technologique.\nOptimiser l'analyse de documents en entreprise.\n\n\n\nSlide #3: \n\n Image: a man holding a video game controller in his hand\n\nFonctionnalit\u00e9s Cl\u00e9s de l'Assistant IA\n\n1\nAnalyse et Recherche Documentaire\nRecherche avanc\u00e9e NLP, exploration s\u00e9mantique, analyse des tendances scientifiques.\n\n2\nR\u00e9sum\u00e9 et Extraction d'Informations\nG\u00e9n\u00e9ration de r\u00e9sum\u00e9s, extraction de citations et r\u00e9f\u00e9rences cl\u00e9s, cr\u00e9ation de fiches de lecture intelligentes.\n\n3\nAssistance \u00e0 la R\u00e9daction\nG\u00e9n\u00e9ration automatique d'introductions, correction grammaticale et am\u00e9lioration du style scientifique.\n\n\nSlide #4: \nSolutions existantes d\u2019assistant IA\nChatGPT (OpenAI) : G\u00e9n\u00e9ration de texte, assistance \u00e0 la r\u00e9daction, r\u00e9sum\u00e9s.\nDeepSeek (DeepSeek AI) : Sp\u00e9cialis\u00e9 dans l'analyse de documents techniques et scientifiques.\nGoogle Gemini : Recherche et traitement avanc\u00e9 d\u2019informations avec int\u00e9gration multimodale.\nMicrosoft Copilot : Assistance int\u00e9gr\u00e9e aux outils Microsoft pour l\u2019optimisation du travail.\n\n\n\nSlide #5: \nComparaison des Assistants IA Existants\u000b\n\n\nSlide #6: \nArchitecture et Technologies de l'IA\nMod\u00e8les NLP et LLM\nDeepSeek, Mistral, LLaMA, GPT, Claude pour l\u2019analyse et la synth\u00e8se. Sentence Transformers, BERT pour la recherche s\u00e9mantique. Spacy, NLTK pour l\u2019analyse linguistique et la tokenization.\nBackend et API\nFastAPI / Flask pour le backend. LangChain pour l\u2019orchestration des mod\u00e8les IA. ElasticSearch / FAISS pour l\u2019indexation et la recherche vectorielle.\n\n\nSlide #7: \n\n Image: a shelf filled with different types of toys\n\nTaxinomie du Projet\n\nExploration et Recherche Documentaire\nMoteur de recherche NLP, filtrage par sujet, analyse des tendances scientifiques.\n\nR\u00e9sum\u00e9 et Extraction d\u2019Informations\nR\u00e9sum\u00e9 automatique, analyse des citations et r\u00e9f\u00e9rences, synth\u00e8se par th\u00e9matique.\n\nAssistance \u00e0 la R\u00e9daction\nG\u00e9n\u00e9ration de contenu scientifique, reformulation et simplification, correction grammaticale.\n\n\nSlide #8: \n\n Image: people sitting at a table with laptops\n\nCas d\u2019Usage Concrets de l'Assistant IA\n\n Image: a black and white photo of a black and white airplane\n\nDoctorant\nUtilise l\u2019IA pour r\u00e9sumer rapidement plusieurs articles de recherche sur son sujet.\n\n Image: a black and white photo of a street sign\n\nChercheur en R&D\nExplore de nouvelles publications et g\u00e9n\u00e8re des revues de litt\u00e9rature automatis\u00e9es.\n\n Image: a black and white photo of a black and white sign\n\nAnalyste en entreprise\nExtrait des tendances strat\u00e9giques \u00e0 partir de milliers de documents de march\u00e9.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d433150f-dfd9-4628-a5ee-6deba55d12c3": {"__data__": {"id_": "d433150f-dfd9-4628-a5ee-6deba55d12c3", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f3568aa-4847-4e02-bc68-372a1d0fe3ff", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f52808e91b4adbc5290a810899400c51d1182902fe1a9751709b23fc84aa58e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "024a09f3-bd37-405c-9176-e02f67b122f1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "653471baac42af8909ab7dff1a267195a5037491a6285857597d9786a902e94e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cas d\u2019Usage Concrets de l'Assistant IA\n\n Image: a black and white photo of a black and white airplane\n\nDoctorant\nUtilise l\u2019IA pour r\u00e9sumer rapidement plusieurs articles de recherche sur son sujet.\n\n Image: a black and white photo of a street sign\n\nChercheur en R&D\nExplore de nouvelles publications et g\u00e9n\u00e8re des revues de litt\u00e9rature automatis\u00e9es.\n\n Image: a black and white photo of a black and white sign\n\nAnalyste en entreprise\nExtrait des tendances strat\u00e9giques \u00e0 partir de milliers de documents de march\u00e9.\n\n\nSlide #9: \nB\u00e9n\u00e9fices attendus\nGain de temps et am\u00e9lioration de la productivit\u00e9.\nAcc\u00e8s facilit\u00e9 \u00e0 des informations pertinentes.\nMeilleure prise de d\u00e9cision pour les chercheurs et les entreprises.\nPerspectives d'\u00e9volution\nInt\u00e9gration d'une synth\u00e8se vocale pour une interaction plus naturelle. \n Fine-tuning sur des corpus sp\u00e9cifiques pour am\u00e9liorer la pr\u00e9cision. \n Extension vers des modules de recommandation personnalis\u00e9s.\n\nCONCLUSION", "mimetype": "text/plain", "start_char_idx": 3235, "end_char_idx": 4183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"3971ea48-3f84-4e36-8e07-2cba5766bbd0": {"doc_hash": "8ae9340109534c967443281852f50fd2804db7004e91d2c10e4367cea2504bed", "ref_doc_id": "464da0fd-1c4b-416e-a69e-4d2665cead28"}, "a28f41d6-04ff-4068-8062-af73ed0ec56c": {"doc_hash": "2ea0360b456fdefe2cf15c0de3a1ff5daedb784c42142ec2d1747297c5c928a3", "ref_doc_id": "da847f53-93e8-4a6e-8dfc-a6107e68db88"}, "19773e18-7996-40aa-bb27-425f411c2ac4": {"doc_hash": "4ba240beb77dfa93e9545791c0a0e9b24abfc2a42e3a9eec8f207a26d99f9fe9", "ref_doc_id": "bfc256a1-d765-40cd-abfd-aeb0027a80df"}, "394523af-3ce3-4c15-89aa-32ca5282a11a": {"doc_hash": "bd243882d9ed6dadbc2935820507d955b439d5249d02494f87718c67555a5929", "ref_doc_id": "bfc256a1-d765-40cd-abfd-aeb0027a80df"}, "e5bdf5d1-d26f-487b-a671-1c2a3f5a4fd7": {"doc_hash": "10d7b554bd7527a33c7ea797c2c8f8222c9c4975f97fb1d192943cb0975e191e", "ref_doc_id": "013f11cf-c5c8-4d29-8a5e-7ecd98587909"}, "a918fb02-3a2d-48ad-9db9-4ed4e30e7d2c": {"doc_hash": "d50ec1a000ea5bc9719f0625029160ad7f272cabf8e5efc4825ad1e437e7b74c", "ref_doc_id": "013f11cf-c5c8-4d29-8a5e-7ecd98587909"}, "73d32840-e30b-40b3-bdd5-2a084a44186e": {"doc_hash": "879498b4944f2a3dadbe6201ef0f98bb1463f23bbbc76504c9871c7a541e1c80", "ref_doc_id": "2787ee49-8958-4540-addc-80f5b6d8dde7"}, "836e9473-7b27-4b48-8df6-dabf4d1e7aca": {"doc_hash": "d7c410a75691142e2a13e7e3d8529623a2f70fa52a19a2cd877b5426c6f70b4f", "ref_doc_id": "2787ee49-8958-4540-addc-80f5b6d8dde7"}, "a1da25fb-f75c-4662-b92d-1412fd58aec5": {"doc_hash": "c0873a3850eec1c312abd0b6fd654dde2be551670bf9d448b72963ceeb8589f8", "ref_doc_id": "5bc688e9-54d5-4bc6-8b7d-f2f87d213a7f"}, "9a6c95a7-906f-4554-80c4-c1a6cff2e352": {"doc_hash": "964ed70954ba94aa4a1f7d2112dbf154d5511f8f4c9938f70c83f6ce1dc263a0", "ref_doc_id": "9ac1ab6d-be3f-458a-8e5d-024316fa88ba"}, "44aaa760-cc26-4fc5-873b-4684ca7933f4": {"doc_hash": "406e4185dc1a7db8e37adbf26df27d23d0706cdcd9c7b751a7357083fd86f4c9", "ref_doc_id": "ebb35667-1ec2-4326-9877-959aa17912d2"}, "79dc9d7b-9d55-4d7b-85c8-1533adb594f6": {"doc_hash": "82e82e3e0dd883a244eb8ea5191a5fdf6a3ce79c087afb998ddfe6df6c3709c9", "ref_doc_id": "580dce2e-e952-45eb-b3c4-7f514020a4a5"}, "abab35fd-92b2-437f-8719-1294f2052564": {"doc_hash": "db21e5c793c542f6ffbb1cdf0edc522832b941be3aeb72a2c85e7cff41526445", "ref_doc_id": "60c5a7e4-4606-4469-b240-48651f33e468"}, "7f21d530-74ca-466a-a186-ae71b1d17987": {"doc_hash": "eac156bfff7ec407e951dd6f6ed6e82b1daaabd51bfbd1416304f1765ee6463c", "ref_doc_id": "214bf48e-035e-4d5d-8f0d-61318c7efd7d"}, "8ae73c34-4e7a-48db-b5ef-52e943f4c6d6": {"doc_hash": "b5dd31bd445af24f0e0fe4c611ad60e4eb93d96b91b8b90cfd2c564b25f32edb", "ref_doc_id": "214bf48e-035e-4d5d-8f0d-61318c7efd7d"}, "6a739065-5338-4e49-8a8f-8c832b0c56e0": {"doc_hash": "6aba8fbbbb461ac98391d0e15603e49c7ff828958c05303f78c4b18cb73066a2", "ref_doc_id": "9fd5b8dd-ca02-4ab2-a77b-c129442a2132"}, "d86a42af-29a1-41ed-943b-efe4fae1e6a3": {"doc_hash": "e3c0791ae78f199f36982f0e006444171fbdc09c36f28d6bd06672f765b00de1", "ref_doc_id": "078f9666-8d69-44b2-a401-ccb13079a1c9"}, "1ffb443b-66af-4698-895e-a67d879fd52f": {"doc_hash": "2e2f8a4283abeb8f655bba0216425d0fd3a6417dff05efd2cb8f393871273d99", "ref_doc_id": "bb96d0fd-d6a1-43ff-a421-45a5887c7864"}, "98b29d88-7334-4e28-aa14-41ef28ed897f": {"doc_hash": "1318515494a68012231c23e58f5cd2acb3eed0e4981d895b4f7ccfdee560490e", "ref_doc_id": "c7909d06-ff21-486f-ad57-c14a081af40f"}, "349c72ea-fc34-4ee8-ae7d-023fcdb1cb92": {"doc_hash": "89e48bf129e54fd325dfcf6f7d34f1b4929f9d0c8b6675a326558e45095ea876", "ref_doc_id": "e1722240-690a-4587-8efe-7b9e2c036c9c"}, "da39ba41-fb9c-467a-848a-72db614ca487": {"doc_hash": "a79ce2725d9893bd3e0a33bb9cf840182c118206c823c81f50be6f8acd1eb177", "ref_doc_id": "7d5371ca-8586-44a7-880f-34157b51292a"}, "540d861a-b73c-43ef-b233-d3c5ebbda741": {"doc_hash": "24d7c57671be202d6a9f7a103d93e4f749a215ef8815a5a58f8fe75b91037d6c", "ref_doc_id": "89a6b699-7041-42b6-8056-e20e7bb12875"}, "5907c7ca-640b-4971-86f0-0d2d62955da2": {"doc_hash": "a59cfecfc107df862c9e8b40b0be908cb49f9024d2de6711782df6b6113c8634", "ref_doc_id": "5e68eec0-45d7-4ca0-880b-8c3ef5d5ab48"}, "0e1981ef-40bc-4931-95bc-27c9ecdb3efd": {"doc_hash": "066ed6ec2eef872c75cd17d666e2b10b98b1e60a14e6eb47a36918f3e54b2451", "ref_doc_id": "8e78cdb2-a88a-4550-a0c1-14f85ce32f63"}, "1b871956-1ff5-4f57-8cce-e8a3be0c9fbf": {"doc_hash": "14aac7834c0d8c50dd097a756053f42abde8223793225e2d0bc1d3c85e60c500", "ref_doc_id": "b32f56c2-7db7-4166-83b0-8d0c79598ca2"}, "03fe14a5-e7c7-4ad0-8b63-7995f7257c41": {"doc_hash": "e11a92f7266109818ab72a15db4347d93cd98948e71c6c2b0399107522d76659", "ref_doc_id": "2a3ba3cc-f88c-4265-9514-ce5c189ed928"}, "89852993-9801-434b-8b2b-c49e34af79af": {"doc_hash": "0bdf28247df400501bbfa8c521bc6c81e02d5206aa3b219f15649c3d97991dd1", "ref_doc_id": "814a2403-6dfc-4673-9d31-b232a98267b4"}, "a9531aa2-aeac-4360-aee8-d53f280b9b9e": {"doc_hash": "5039b03a9588506aaf2cd0f10e9ba3c6e5e6f10052d25f8da3fc11cbcbfc76e2", "ref_doc_id": "7dd2e20e-d2c0-463d-a5ff-9d9ab53370db"}, "3f4f7563-31ca-42c3-95c5-6d132d346934": {"doc_hash": "18dfbe6c0959eb0a00eb9fec5c62341d7fddaa1e8e15b9ca638b863e4fa3442e", "ref_doc_id": "5d93fe54-1262-42f9-9dc3-af81f426b019"}, "fa96261a-2441-4f9a-ba76-18d0604d177c": {"doc_hash": "6dee7ff73b09933caea6cf88cf2ade9b9943cdf033c058ebad3ef60d0b9fda07", "ref_doc_id": "1c924199-a612-4a3f-a1f6-8165449807ae"}, "dc27b76d-beba-403a-9021-7a1b79b1948f": {"doc_hash": "2d5ecc484837658c71224f36f97df7c2d2334b78ac2cad9ba9a382d751d8ddb9", "ref_doc_id": "2e3770db-8a36-478c-a0e0-620b9c3a2bc1"}, "f5c0bcc2-2964-45a4-9e51-a56592124c02": {"doc_hash": "85841702afa7eb7068e217db638154c4b78fbc7db3d782ea1e50b73a11703534", "ref_doc_id": "26d91884-20ca-4606-8c54-4e246a86ff03"}, "68284171-9e3c-457c-b5f3-1306147d2a47": {"doc_hash": "92715940e11085f4d92624b0e07df0a88a43dc8112457dbe06d7b388d5ea5e8f", "ref_doc_id": "7ea65f56-0a8f-4a7d-9cbe-c4bcb1777595"}, "d782f8fc-1f6a-4b1a-8887-82ebea70c671": {"doc_hash": "03a41312ee854347e8159652100f4a5480dcd75f2a1b41f112e888936d1ae863", "ref_doc_id": "fc16011d-df63-4c64-956b-cf60dfe6a02c"}, "eda27de0-e840-42b9-8ec7-5f122017c53e": {"doc_hash": "c956df1ea4410eab11f462c8fb651bbe5076c5bd185b53f9e6edc43c082ab9d0", "ref_doc_id": "c2c1e965-199a-45a8-bb6d-6ab683cb8acc"}, "6e5e2d97-bbad-4179-af0a-fb1ec121817c": {"doc_hash": "5850e6910e120afcf35f6883eada3280063c2302949877e501fc22fee3897104", "ref_doc_id": "6503fd34-ae7b-4d44-a271-95e6cec6d187"}, "6aa0204d-04e6-4332-8fd0-44edb6a7d135": {"doc_hash": "c986e32bcb751d2be353f8355119bf2c9e52dacbf80fec4147edc944ada8ef65", "ref_doc_id": "e02d8246-f470-4045-9f61-b6ada5eb6f78"}, "efe62d02-f379-4502-9789-83173321422c": {"doc_hash": "2c29a1fadc21f2aeab323105f0dc89ce89fe9d5bbc385532ed745e5dcb515ea3", "ref_doc_id": "2c47c95e-b57d-4ca6-ac29-aa5cdf666e43"}, "33f3ede6-5df6-417d-abd6-ecb79d3e27a6": {"doc_hash": "ba410f2a82cfef6860617aec68eb0795ae6ad4f08f4f8fa6f5d6c2fde0d206b9", "ref_doc_id": "b4d6db63-83d1-4fb2-9c94-5d7fe7bcedeb"}, "55b3c148-de24-4729-969a-9b82d4afd343": {"doc_hash": "c05027dd48450837f21103debebd5aef6d3035690495d46f8d80ccf087e3e45a", "ref_doc_id": "7c0d2782-15c6-4d1f-b7f3-2e486a589ec7"}, "302f76de-3c44-475e-8a9e-822d1e210eb2": {"doc_hash": "dcc8c0114e5163afebb935e89f575ad90232e151028febcdff72b2accffa249d", "ref_doc_id": "dfcf7971-6e71-4a2b-aa91-fa46305049b6"}, "0ba989b9-230f-47b7-ad8c-095b25a9f3c8": {"doc_hash": "0e97444cdb841fedbd88cef3b6a787e2916e7a3e457fa771abc0ab45c2ea5d41", "ref_doc_id": "61d8fca3-4372-4dd2-ae91-4d18833a262e"}, "3980cd5b-4d4e-4e2e-99ec-d59472cf180c": {"doc_hash": "8cd3203654866e7adf7019a207e5c4ebd718a04c229e359c51763db025491c7f", "ref_doc_id": "61d8fca3-4372-4dd2-ae91-4d18833a262e"}, "29b8c972-e312-466c-aedc-55f224d6377b": {"doc_hash": "4b940c2421ce678939cd66fbdd5c7849bbe8fdc1a43198d6caf9b38377db8fac", "ref_doc_id": "7fc0fac2-b8d3-44b6-a807-f45a4bf151e6"}, "5334ad4f-35f8-4668-a4cd-9a7588c99ffc": {"doc_hash": "d56bd1f4bb0e4b63fc6bfbe025498bc89d3f4082253dabd77c4f4324837dc9b9", "ref_doc_id": "7fc0fac2-b8d3-44b6-a807-f45a4bf151e6"}, "e26f2687-c7a5-42cd-b3fa-938e7e694189": {"doc_hash": "64b3b0d548fa78ab761d276195f0311cbd8ef640843b0e6837fa65094c2dad4f", "ref_doc_id": "db02c5ee-cc26-4afa-9bf3-303b6d5a813f"}, "e629bc8e-da09-43f8-b064-8e59531dc214": {"doc_hash": "10174112322dd5f45208737823b9b9da32cef2c02a21adf556ff433e1024b782", "ref_doc_id": "9345d441-1a96-472d-9f8d-694c31af0438"}, "2ba8adce-8f30-4121-8787-7069020dd9d1": {"doc_hash": "79a355d0b56678915f13140f1ab124a451c1b6ea711d0a21b8aaad567534a1f9", "ref_doc_id": "f71088fc-52c6-480e-80a3-15d093574264"}, "e885adc5-dd7a-4633-a5e5-455b09c26c0b": {"doc_hash": "6e5588ffa77a39eb18adbd9e82d6272200becab20bb4c9ce7e0be2e00cfebb4c", "ref_doc_id": "080b97e4-1712-490e-a0ab-552a8bdef02b"}, "1328a3e8-8820-4b15-b9eb-fa661608bd93": {"doc_hash": "484d5b7f4e92a9b22ccf0f4fe828e68ccd69c86a82c17654d0a80d0200a385db", "ref_doc_id": "adcddc03-9e4b-4388-89fa-04aae2a9ccb9"}, "f6280781-059c-41c8-96b4-d70fb657b03f": {"doc_hash": "b5f9023a989d7cc7ee6a96e51902fc098a8e49947543c104409e1b979249a9b5", "ref_doc_id": "adcddc03-9e4b-4388-89fa-04aae2a9ccb9"}, "78485156-54a6-4e4b-950c-8630b10c039e": {"doc_hash": "217afe8df161559e8c99681d2f9fe4f7e81a34d16e002ed66fe0af123c2af5e2", "ref_doc_id": "a2d6b0b5-edf4-4e67-9363-f35aedd4721d"}, "b2a5bfc1-c413-414a-86e4-e3ccbfc5aee7": {"doc_hash": "881ecd6696f01c36fe0880d69eb2f6f9b77bdc1ca8da02be6a865c54451b4a34", "ref_doc_id": "8e401de1-7d6d-4811-95c8-4eab0d7a3f5e"}, "62e0e837-8ebc-4278-a7d8-013fe6d8fe4b": {"doc_hash": "a9b6dea86c466ea490861e191fa22c7172c519dc40188cd119a715c47febdef5", "ref_doc_id": "8e401de1-7d6d-4811-95c8-4eab0d7a3f5e"}, "cefd4005-86b5-4faf-a469-78c5a433cf62": {"doc_hash": "e9133fb2d0bedb78add9e19432f557d2b69b210c2378562f00ca6ce431f416ad", "ref_doc_id": "434cf912-7cf4-4349-8ed5-215c3ee4ee56"}, "486e2214-3f7c-417a-8c8b-4e87b5841238": {"doc_hash": "7b88cd1a7bbbe3222f36cf6916064f51d9abc2dee3d86b7b1dbd532626618b0f", "ref_doc_id": "39ff8ed8-5b2c-47d3-a1ab-c9292850e48f"}, "59917371-2612-4d68-85f1-f869cebb51de": {"doc_hash": "60955a99e0370053e3ed12426e16dec14890b4955e8a4e212d27a028a81ca8de", "ref_doc_id": "39ff8ed8-5b2c-47d3-a1ab-c9292850e48f"}, "7388d728-d29b-4b71-bff9-335b66cb0ee6": {"doc_hash": "dcf53b13e299a99d64681016252dab419ca303bf29c9d786a4d5dba9e342d542", "ref_doc_id": "1a3c2390-9797-41c6-b4ed-c32c4783bbf9"}, "177ea331-8ba9-454e-bcd0-1ea58b71d4e3": {"doc_hash": "6b1039b07a7079ff9353b7f1e61fa2564e35c842307e4394bafd1ffd6cd79a80", "ref_doc_id": "d99b3ec8-f5fc-4838-bd99-20bcfbc2f0ad"}, "3d2c2217-4101-4644-9c02-3a894a47a2c9": {"doc_hash": "1d689689e490c7bf3bd3692e4b96ee8c48af0baec66eee971d818aba691b6f04", "ref_doc_id": "981f68e3-8d13-49b9-a94f-e77afaf948cd"}, "7fd858dc-feea-46fc-82ae-fe386f056826": {"doc_hash": "83a95bfd89223a7f70a7ab3a3242be10857422c138c88267155a353b5b70b623", "ref_doc_id": "68120e16-101f-4af5-af63-aa3db48f0ecb"}, "014eae43-cb2b-41ca-a68f-ae5fd3588488": {"doc_hash": "4766732085421d298be717736447efbb0912e5065f79ad9cf81657b64bdf599b", "ref_doc_id": "68120e16-101f-4af5-af63-aa3db48f0ecb"}, "440d6004-7e26-4559-a775-99aab1416d29": {"doc_hash": "6dd8621ae36febb58d051b196dc7bcbf852bee500a84335c9f38a86877c7c8f5", "ref_doc_id": "c43deeb1-bc0a-4597-83ec-89fa05206ba8"}, "ec3edb3f-19d8-4df3-8b26-ef27f56b8c74": {"doc_hash": "c1118a8d856dd6cb3c25e04d0d86b1be87ab756a2e9d948a3d061f73033c2b8b", "ref_doc_id": "c43deeb1-bc0a-4597-83ec-89fa05206ba8"}, "5141534c-1abd-445a-af2b-c22a2e89f5bb": {"doc_hash": "23374ba6bb4cd372fe5498d4385e262355432ea7e626e130010bca09f0f1680a", "ref_doc_id": "39743898-7152-4110-9bbe-f1b04afaf30e"}, "ab3c3480-844f-4e77-9c1e-907ba0fafbed": {"doc_hash": "3fb461371116d3b40c3c8580fb6f6a263452dedbe3e1f90c78b7c690c48fba0d", "ref_doc_id": "39743898-7152-4110-9bbe-f1b04afaf30e"}, "466e8572-6fdf-41f3-959b-f5c74172ff62": {"doc_hash": "266f0c199bb3e796b586c8a09f354673ce85dbe0a78df41c56ac449ac546a4af", "ref_doc_id": "ae3a5731-6c14-4b81-a9d4-a4ed212cefbf"}, "bd8ceb3f-df48-44a4-aa83-b31cd4e2a76b": {"doc_hash": "3dbf8eb559ecad5a1a3f3fe07b58ced945b30eb2d293eb07960c6a60389a8fc0", "ref_doc_id": "ae3a5731-6c14-4b81-a9d4-a4ed212cefbf"}, "6761b260-4c9d-4627-8e00-76d7357d5b00": {"doc_hash": "78d187aff6f8c91a0a89a4604da0b462abaf609ea522c8b4364bb5ee62b30ffe", "ref_doc_id": "d6abc588-e112-4631-8d8e-215751aee4cf"}, "e8748485-e96e-46d9-afc5-877eb3e6ff1e": {"doc_hash": "459a7076a9f3d9ea9fd1632b9ff029628462e805de9a2b61c8f6bb6484d5d7ec", "ref_doc_id": "d6abc588-e112-4631-8d8e-215751aee4cf"}, "56f96d72-fa67-410c-a3b4-d505986798af": {"doc_hash": "4ee02bd5907fb75d900046b956a151f7e8c302322ec91d3d76311827d62ac23b", "ref_doc_id": "ea3d537f-0394-4e79-b93f-be94bddbc93f"}, "fbfe7dda-ac00-4bd7-945d-86eab268758c": {"doc_hash": "794b3e0278c0936f7a0ada1e6a91214fdfa9675ff5db2a28da7715c2ebb33684", "ref_doc_id": "ea3d537f-0394-4e79-b93f-be94bddbc93f"}, "f3c99cd2-cbbc-40b0-99f6-827b49d746af": {"doc_hash": "fd217ac116b201d1b5a011e1d59c229927abf4dc7dba768b2e1c71f7b53e2416", "ref_doc_id": "dfcb46db-f2d8-4946-ade8-08c13a3def5c"}, "77133867-0d0a-4b9c-92f0-59cab680b66b": {"doc_hash": "c0ad985e1c0709ab0e4659ee744d57f101f4d58d71cb4811df9439ea7e8713f0", "ref_doc_id": "cd563c1c-eb7d-4d1c-8720-86d9417a99dd"}, "2854761d-7fcd-4132-a526-e311a724336c": {"doc_hash": "f950866becccb9d8dd28b5803942fe0eb783aeb56427a3bfa6288bbd8204c07e", "ref_doc_id": "57adcca6-efe8-4cda-9aa3-f06f3e58f3b1"}, "b79d2240-b1ee-4f24-ba86-1cae3998984d": {"doc_hash": "22590a8d590116bb7137ec2a274d7a4e780bb70d25c386217044c7360370c5d8", "ref_doc_id": "4687383f-a972-47c8-a019-4d4c807c983b"}, "1d4e170a-3d04-4d74-a1fd-5aadb35dc4c1": {"doc_hash": "255c4c5106f3f16c859006f16a749385d25f152d460fb8a60c8f78ab57be9b46", "ref_doc_id": "c0973131-09f0-4f1c-96bc-9391ea9d1a8b"}, "852557fd-3301-4bfe-a61d-ca82c43e9c5f": {"doc_hash": "09f7550ab1755f3ba8175dbf75967f53a71d242f51737c3cd35fd699a4f41e33", "ref_doc_id": "236161d0-b7e8-49f0-bf54-b678f399062d"}, "d33cb7a8-f93c-4212-8850-e68009306bfe": {"doc_hash": "243fbe57034b6a8f85819cd9b8e32a707fb313e7612d253a82b66830d40291d1", "ref_doc_id": "d9169ea3-c2a1-4cc7-810e-5c4a84f6730d"}, "c2cdfda8-7a32-4701-abb9-5380ff5418c7": {"doc_hash": "6a8a211d2f0ec7963bfed92379d9121b934f250431346a2f6f0c058d7b21df09", "ref_doc_id": "0e9109f1-c1e6-4869-aa72-8a949997379a"}, "973fc9e2-5db9-43a0-a1a7-ae658d8f1055": {"doc_hash": "24c0fbd1f5c8b57542ef2d427371b1a2f839a1134aefdd135028eeb99b3baf3a", "ref_doc_id": "d90cf593-b9a6-4f03-ac91-a58dd0075c86"}, "fa700a76-ff9d-4ee1-a345-aad0ce7aaff2": {"doc_hash": "583cbe4c45d7a87d9efa4c42bc5de109b355e362b2cfb0512e4e3a07a5dc9903", "ref_doc_id": "0023bd73-95d5-4673-a31f-5f22ae536f29"}, "4e894968-ed5d-4a49-8f6a-1feb00651611": {"doc_hash": "282979a74d97c88b2d60f402ffaf9a9381e1c564fae6e419e0b20b3478a3223e", "ref_doc_id": "c662354f-eed1-457f-97ae-7d853352ce78"}, "a7c82d17-8f70-4f1d-ab48-67c000c27d27": {"doc_hash": "a67e690f36e9c65ad345169c8d2b57dd8875da6b974d6b38a94324be33ede96b", "ref_doc_id": "fbe517d5-2096-4f99-b8be-9513ee86091e"}, "ead7fc51-3cf2-48c8-b63d-d46628b0f849": {"doc_hash": "3f41669cbb2d0b05e0e09e0d0383437f02abfff8b748cbc187d66f4162593126", "ref_doc_id": "3f295428-5b56-4efb-a4de-e0f23cec05e1"}, "afb992d3-919b-457b-848b-40c91fdef6e6": {"doc_hash": "c6c819bc704ffa71252c5d43336fd527bd41d7e3588400022fafae632d71a0c7", "ref_doc_id": "78e983e3-5ca2-46d2-96c8-a2ad45268e07"}, "520cab2e-7869-4b67-b8e9-cfc9ae53e0c7": {"doc_hash": "e03805b417c0256bd31187682a7171ec1db09b36065bd0a62a9739e079dec84f", "ref_doc_id": "77984cd6-67c2-42c4-a432-5349a1a8022d"}, "49401694-e006-4f65-92e8-0ce28dfeb8d8": {"doc_hash": "3e596c3572058fb6b953d7567e30bfd92911b9e3f276aff764b6d2615034a93c", "ref_doc_id": "bfe68aca-4b89-4975-969e-e4df13a5f0cc"}, "4c276dad-47a1-4c9c-91af-a118fa86daaa": {"doc_hash": "19d81b6f9850717e4f3105db89975bad903572b5093f33b18b92bd6e2416b847", "ref_doc_id": "299a7be1-9f06-43c1-979c-7c22ae413117"}, "b6d200ec-9182-49a0-9837-c61edf0413de": {"doc_hash": "4be9f2b4650fafd3b08cb7f4b324193141f19b6d0e63b4189cd0345ee88a7586", "ref_doc_id": "204186e0-05b9-42fe-b3f8-cd482a7eb3d8"}, "704c9b2d-c2c7-459b-8d3c-cf09a36b1eae": {"doc_hash": "fc5fb8c2bb099cc6a09b22f29b90074f536b337c576828d061673316d37bfcd1", "ref_doc_id": "b1871c86-2ec3-4443-b8ff-432d7ecc887b"}, "aebb4f93-89e3-456f-8ed4-9a62578d09fe": {"doc_hash": "1fac911e960124b19727525fb20cfb502e10ade33e6a243d3cf728e0061bcdcb", "ref_doc_id": "455a1b36-879d-4cab-859d-ba7837494abe"}, "c03e7e5d-4b33-40ab-9633-9edab67cbb23": {"doc_hash": "f80fd28a8bdcdbcfb8687bc655ce31372cc179f153c63dbe95077d12d514a1c9", "ref_doc_id": "692b1c6b-80f5-4e74-8ad2-88169551f8dd"}, "f9993feb-f461-448f-b430-db581f5b543d": {"doc_hash": "a0bbeae990cdace0863765614ba3a8dad22b98f99acfd4b6c512b7dfb40d69ab", "ref_doc_id": "5faeaca7-9d4f-45f3-915c-6319dfcd58e6"}, "71428714-d8f6-46fd-a802-8cdd7fb84c82": {"doc_hash": "47050b96aa84482f328337873db42c06ceacc8b9c94eff5dc6c7e1973d5732dc", "ref_doc_id": "f37bc684-70d9-48ca-9a39-e95b925fdb24"}, "0c9156b8-8038-4fd7-a7dd-7b220b5e7c2a": {"doc_hash": "071d0c49378cd7dd983626158fa033b87bef6130a10c13707d66c77ac3437035", "ref_doc_id": "bfa34520-04f4-4089-bb66-4d3abdf2a2cb"}, "ebc1ee53-2e8c-423b-9413-5f8e385573b4": {"doc_hash": "368c378a67d084e47830d651b0d25978fcab6f853011cc6c4f6cdeeb20f3d945", "ref_doc_id": "21debbee-1d51-46b0-a172-697bf6d5d2e5"}, "eff31749-96ea-4fb3-9fbb-23af8f244693": {"doc_hash": "b0c7b1f464d5dd8b9213b52075bc505f5008ecd6521d26166c8a9973d95ffdfe", "ref_doc_id": "4fe9dfff-bf79-4b07-afc0-3ec00caf91c4"}, "5e4aa46c-c968-4769-9eb8-2c87675c49bb": {"doc_hash": "995e243f8ae4367816765f1e59a8caae250e7c49e85753b601d4fa9b0d834baf", "ref_doc_id": "dbda075b-6840-4c04-b4de-d56fd8eaa43c"}, "e566adc4-edaf-4f8b-964f-eb6be0252d2c": {"doc_hash": "eccf6235343f1090d8a8e0988e1eed4dfd9e0bf4f5f18c02461280a17bf3df0d", "ref_doc_id": "a437b24d-5323-4c43-a1dd-0e9eff58e51b"}, "b7976034-b756-4fa5-b20c-47413e9e492c": {"doc_hash": "a2ddcdb957a79f1e75ec47dc906ca3f21e4250d5245efd7e2a69cd8b04ab1d60", "ref_doc_id": "ff3ad51b-9b19-4f0d-ac01-dfd784629f2f"}, "a5afaab4-c60b-4b85-9368-5531f9005c1c": {"doc_hash": "26704d76e6b60ce537ae4bc6ed81de16f70c7a464c527885ff3c3d7127325752", "ref_doc_id": "a9cda855-5d81-482b-a385-bb9c488d130a"}, "8b16887e-d4d6-4077-8acc-67d7e89a0ac4": {"doc_hash": "e74278a6d2e8496e3b902fb17ab886681b5b62b5b6116503c1c71388af9d092c", "ref_doc_id": "589c5b12-4927-4cb1-b8f5-a5d021f7fe2e"}, "9fcad995-6fc0-421b-86ed-fb3538ead9f0": {"doc_hash": "ccd891880706678c291c1a65e04d50dd819183405fe3133727532966ff5199ba", "ref_doc_id": "eca59d18-33f3-4a2f-b059-7b89c3f325a6"}, "895f306f-0c03-4f11-9418-34792a0c7a35": {"doc_hash": "adfa01834cf8fdf5f2f28bf32826ad98c7dc4d5a573977881b03d4b71a1a27f3", "ref_doc_id": "52eb3eae-9d0d-4712-9197-64d8281f7aed"}, "d895f2fb-2d30-4093-87ff-4f9a477e5346": {"doc_hash": "a660afb9978f0b03775344f858cc95dd11ba20c04ff76e6ded0ab9a47fc9a2b6", "ref_doc_id": "233b941d-4b35-44a9-890f-7e9135c16b4d"}, "19294647-156a-470a-965e-f7f63c9117c8": {"doc_hash": "c0bfdf3188939473bfa1ab9670aa54fb096cccda646a08e6dd10b365033970a8", "ref_doc_id": "233b941d-4b35-44a9-890f-7e9135c16b4d"}, "80db4b67-ce50-427b-8322-c2144e823ca3": {"doc_hash": "35667643ee1fd3889d6f5a40a21ec13a4a3ba6efd4988420289047b401a56c65", "ref_doc_id": "3e35c7bd-27ac-46ca-b1bb-cce296931b82"}, "6779e741-70c4-4c3e-bbab-08bc83490797": {"doc_hash": "ffa268d2559ffb497d2fb325e59082af3bcd19e423696fd82ae7a844e851039a", "ref_doc_id": "80076866-e602-4250-818b-46ec6facb36f"}, "a25db37d-f01e-4170-a8e6-3bffd64191f3": {"doc_hash": "c7089467e7e79e4c8f116b55284c233f679f8c98b86c20c9eb70e5b769c4bc61", "ref_doc_id": "7b85b298-fd49-4ba8-9383-736c92d57466"}, "52841cf5-5677-4734-b950-c4c2d22a498a": {"doc_hash": "62c091481530a94cb96a7fcfb86d112fa45cb50a55eda1e803c180fd52690bb9", "ref_doc_id": "7b85b298-fd49-4ba8-9383-736c92d57466"}, "d51845d3-4b44-4344-9d00-dc96c6f86884": {"doc_hash": "4bd8d07c5f37237cc6e6fbabe56b4664bd0c09de6698cb9e057fedca83d4ca21", "ref_doc_id": "279328f0-56f2-427d-ae9e-01db9f67472c"}, "704d03f9-b78f-479f-b0c0-7b0c1576ae53": {"doc_hash": "d7d24b4e425b67635ad0c937249c6b334f231e74971922aadb1b54fed1101147", "ref_doc_id": "350146c2-9a3c-400f-873f-92b3b564abe3"}, "de8bd408-a7ff-46b0-8615-27b3262fc1bc": {"doc_hash": "594a40fffc5a72fe95e73987f9e29ae73f288efdc0b032240a393c93e6b5369a", "ref_doc_id": "63146ebb-319f-461f-8f5a-86cc7adc1498"}, "f02b3c1a-6580-4633-b9b9-c28dfa27a808": {"doc_hash": "e6f2ebf98bd8f55aaab785e3d1f950f72937f096f0adbad847170ce42096a15a", "ref_doc_id": "c2cf2e60-32c6-498b-bdc6-785575e4f765"}, "4ae011c2-b13b-4c81-9f02-d01836c5b13d": {"doc_hash": "1dc780462e88fb81e1dbeb4d09ab4a0b69204deac056efbdc6386abdf227163f", "ref_doc_id": "c2cf2e60-32c6-498b-bdc6-785575e4f765"}, "b9b03e82-7837-484a-adcd-3c8d49d330aa": {"doc_hash": "d229650a63533ea189dbaceeb92b874beec828abea520ba6d230ab308f5c65f6", "ref_doc_id": "8db6264f-0b28-4326-88db-6038bc565edc"}, "f9d3b38d-2100-4cca-bac8-dd421ca3556a": {"doc_hash": "e47a3a806451176076993e9b176bcb562f1a74eff7977bdcc577db9251d32121", "ref_doc_id": "8db6264f-0b28-4326-88db-6038bc565edc"}, "8ce040ea-1e5e-4b93-8546-b1cfc86ced1e": {"doc_hash": "2f4d66f8b9e7304368dc74e495b59dd1b9c707e859d7c62f6092b2774ba469d3", "ref_doc_id": "0b74ff41-9bfd-488f-9319-a5f56fe276b0"}, "a9337de9-5869-4490-9591-5d05bfb30f5a": {"doc_hash": "d79566f68be7b2efc3838adc951300751709980749ab7cd5a5e44fc81413c289", "ref_doc_id": "0b74ff41-9bfd-488f-9319-a5f56fe276b0"}, "1d24aeab-16a9-41e5-90ca-4b69a2b031c1": {"doc_hash": "8246ebb9a9ba0469c54920b2b2e4e2323564566bd431784a216bd255852251ad", "ref_doc_id": "14e3892b-29c1-4c5a-b868-790337ea1e4b"}, "0eb12a84-69f4-45cc-9aee-72b0024cc6ae": {"doc_hash": "178db03c4d914756df25c28ec1959913c427a215ebe72a5d758d0bee62d78e3b", "ref_doc_id": "fd1b1618-e5d1-483a-ab0d-ae93ab9d300c"}, "cba646f6-7fc1-401f-8645-0d5890dfb387": {"doc_hash": "95144b31abd1bda4c80e728e805a7a75745b7990a12ee2a55c56e55f0c52ec26", "ref_doc_id": "3b70994a-8c2a-42ae-b0c0-4a86ce7c3b57"}, "28944c87-6729-4d1a-b73b-4a1058a5f0f1": {"doc_hash": "ab9fc7bb4d118612b3d9cec290dd76de156c1c665d60c829842f69d44d45150f", "ref_doc_id": "4fef60c5-e84d-4ad2-bc6d-435da3bc56bd"}, "224736fe-83b3-4a3a-852c-89853c34726f": {"doc_hash": "d28bf7b9b86445f83b7de608edc128dd1e77de3f115cf40df38737e1dba524d9", "ref_doc_id": "9999bb3b-5e4d-4ecf-8d53-ca9b2eb6ecce"}, "1ebb82a7-dc3b-4fa0-a17b-9177fbcb5b4d": {"doc_hash": "1b8a33f6abbcdf4dbc3f41bed0b62a3eb2813c9eafc1516bdcd1c98d01cbea88", "ref_doc_id": "3f667b7f-5f2b-4019-950d-aa6bb82831da"}, "beb44e96-1fee-48cd-89d3-fc71f9e58855": {"doc_hash": "aff03896d04686aa4af6fe3cec4d73a1ed437767d0b04ddac52aaaf0972379df", "ref_doc_id": "306ed2f7-b6ed-4f35-83cc-977631112c8f"}, "87ccc4f3-14a7-40d1-a7bd-da676bb871c2": {"doc_hash": "9345a2b64e3c91eda98c029222552e3dcff1a3064c3c9fae36259be3de564ced", "ref_doc_id": "12794cab-7416-40f2-9cf3-9cc7b3c5aa3c"}, "2ef4c5f3-3b4d-457e-ac47-fb2662ce3483": {"doc_hash": "820d00a0f6ee8094e1c64ab6bf1bd4d419ce85185a2f47a5efe732a6367f5d1e", "ref_doc_id": "236a3bb2-64f2-4a86-b396-39fe2f4c6f8e"}, "4c29ba2c-16aa-40bf-8629-de63ca5db99d": {"doc_hash": "6ee9ca198ebf44b8fb3a49dd7b45e6a90f835c2b9d8f1564810e41aff0ae0e84", "ref_doc_id": "34342f5d-4243-4a80-85bd-87c8a0eb5db3"}, "4a824857-e0ad-43d7-86b0-e4cac21a719d": {"doc_hash": "36d690c616917ba5703d483a8e57160cb7b14779dbcb17bacc7d983857ffc1fe", "ref_doc_id": "64547a6a-f338-428b-92c3-49cb822749b0"}, "7e8d3475-8743-421a-b0f0-bf134ff63c4c": {"doc_hash": "7042fcf39319a177f7a2b09161e3ba8002f3a2059b1f24063f0e649564c1213f", "ref_doc_id": "3897af41-de98-441e-9fd4-7f93c7a3eb5f"}, "b536a2a6-a5f8-4545-9f19-d5d547f552ed": {"doc_hash": "96b1d036133cd693c6eb7e91330f02a0e27adf6bafa137d194759e451b562920", "ref_doc_id": "c4dc3664-baa2-4bec-ad10-4aa114388b2c"}, "60f09d8f-1086-4c8d-8a4a-673ab16b4eaf": {"doc_hash": "defb402ad658cc4715d8dd9a86a6ba19b3f3fdd3e49dbea520ee61cd7deff3bc", "ref_doc_id": "7f39b935-39fe-4afb-bd30-8f998b21c4b8"}, "1104fabf-12ee-44e9-b592-9ee2f489702d": {"doc_hash": "be957b9882cf7d27b1cb71ea816d6c6b41e91d47fad83c405a6e86788be651bc", "ref_doc_id": "ca034df0-0341-47e7-bd59-180f46fa2774"}, "f9d9fe77-3ccc-4a32-b8ee-93425486aa90": {"doc_hash": "c1179fbcf8aabee5d1180b7d7008e27bcb6fc3429af12298791fbbf745747389", "ref_doc_id": "d6d25e31-be38-4a95-bbc5-848b8e856517"}, "ac5f6796-cb0c-4fbc-9d9d-3080c12897da": {"doc_hash": "124de8365516c44a3f4cb850fd4df3044463f6f4acd2da96df449edb7a31db93", "ref_doc_id": "dae7359a-02d5-48c4-900b-5890bff51b9b"}, "4faf397b-f967-4bdc-8d48-04b309c7776a": {"doc_hash": "921414532de6e5c8af02f6fc0597daefeec9824ddc621d43823f46700eccad17", "ref_doc_id": "a7d897be-d7a3-4dc4-9bab-84a1bc880122"}, "024a09f3-bd37-405c-9176-e02f67b122f1": {"doc_hash": "653471baac42af8909ab7dff1a267195a5037491a6285857597d9786a902e94e", "ref_doc_id": "6f3568aa-4847-4e02-bc68-372a1d0fe3ff"}, "d433150f-dfd9-4628-a5ee-6deba55d12c3": {"doc_hash": "ad598c19eb4a049fa6425a241757ff21b6becb11bc6f67287f23f1018e674e0c", "ref_doc_id": "6f3568aa-4847-4e02-bc68-372a1d0fe3ff"}}, "docstore/ref_doc_info": {"464da0fd-1c4b-416e-a69e-4d2665cead28": {"node_ids": ["3971ea48-3f84-4e36-8e07-2cba5766bbd0"], "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "da847f53-93e8-4a6e-8dfc-a6107e68db88": {"node_ids": ["a28f41d6-04ff-4068-8062-af73ed0ec56c"], "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bfc256a1-d765-40cd-abfd-aeb0027a80df": {"node_ids": ["19773e18-7996-40aa-bb27-425f411c2ac4", "394523af-3ce3-4c15-89aa-32ca5282a11a"], "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "013f11cf-c5c8-4d29-8a5e-7ecd98587909": {"node_ids": ["e5bdf5d1-d26f-487b-a671-1c2a3f5a4fd7", "a918fb02-3a2d-48ad-9db9-4ed4e30e7d2c"], "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2787ee49-8958-4540-addc-80f5b6d8dde7": {"node_ids": ["73d32840-e30b-40b3-bdd5-2a084a44186e", "836e9473-7b27-4b48-8df6-dabf4d1e7aca"], "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5bc688e9-54d5-4bc6-8b7d-f2f87d213a7f": {"node_ids": ["a1da25fb-f75c-4662-b92d-1412fd58aec5"], "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9ac1ab6d-be3f-458a-8e5d-024316fa88ba": {"node_ids": ["9a6c95a7-906f-4554-80c4-c1a6cff2e352"], "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ebb35667-1ec2-4326-9877-959aa17912d2": {"node_ids": ["44aaa760-cc26-4fc5-873b-4684ca7933f4"], "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "580dce2e-e952-45eb-b3c4-7f514020a4a5": {"node_ids": ["79dc9d7b-9d55-4d7b-85c8-1533adb594f6"], "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "60c5a7e4-4606-4469-b240-48651f33e468": {"node_ids": ["abab35fd-92b2-437f-8719-1294f2052564"], "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "214bf48e-035e-4d5d-8f0d-61318c7efd7d": {"node_ids": ["7f21d530-74ca-466a-a186-ae71b1d17987", "8ae73c34-4e7a-48db-b5ef-52e943f4c6d6"], "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9fd5b8dd-ca02-4ab2-a77b-c129442a2132": {"node_ids": ["6a739065-5338-4e49-8a8f-8c832b0c56e0"], "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "078f9666-8d69-44b2-a401-ccb13079a1c9": {"node_ids": ["d86a42af-29a1-41ed-943b-efe4fae1e6a3"], "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bb96d0fd-d6a1-43ff-a421-45a5887c7864": {"node_ids": ["1ffb443b-66af-4698-895e-a67d879fd52f"], "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c7909d06-ff21-486f-ad57-c14a081af40f": {"node_ids": ["98b29d88-7334-4e28-aa14-41ef28ed897f"], "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e1722240-690a-4587-8efe-7b9e2c036c9c": {"node_ids": ["349c72ea-fc34-4ee8-ae7d-023fcdb1cb92"], "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7d5371ca-8586-44a7-880f-34157b51292a": {"node_ids": ["da39ba41-fb9c-467a-848a-72db614ca487"], "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "89a6b699-7041-42b6-8056-e20e7bb12875": {"node_ids": ["540d861a-b73c-43ef-b233-d3c5ebbda741"], "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5e68eec0-45d7-4ca0-880b-8c3ef5d5ab48": {"node_ids": ["5907c7ca-640b-4971-86f0-0d2d62955da2"], "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "8e78cdb2-a88a-4550-a0c1-14f85ce32f63": {"node_ids": ["0e1981ef-40bc-4931-95bc-27c9ecdb3efd"], "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b32f56c2-7db7-4166-83b0-8d0c79598ca2": {"node_ids": ["1b871956-1ff5-4f57-8cce-e8a3be0c9fbf"], "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2a3ba3cc-f88c-4265-9514-ce5c189ed928": {"node_ids": ["03fe14a5-e7c7-4ad0-8b63-7995f7257c41"], "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "814a2403-6dfc-4673-9d31-b232a98267b4": {"node_ids": ["89852993-9801-434b-8b2b-c49e34af79af"], "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7dd2e20e-d2c0-463d-a5ff-9d9ab53370db": {"node_ids": ["a9531aa2-aeac-4360-aee8-d53f280b9b9e"], "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5d93fe54-1262-42f9-9dc3-af81f426b019": {"node_ids": ["3f4f7563-31ca-42c3-95c5-6d132d346934"], "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "1c924199-a612-4a3f-a1f6-8165449807ae": {"node_ids": ["fa96261a-2441-4f9a-ba76-18d0604d177c"], "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2e3770db-8a36-478c-a0e0-620b9c3a2bc1": {"node_ids": ["dc27b76d-beba-403a-9021-7a1b79b1948f"], "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "26d91884-20ca-4606-8c54-4e246a86ff03": {"node_ids": ["f5c0bcc2-2964-45a4-9e51-a56592124c02"], "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7ea65f56-0a8f-4a7d-9cbe-c4bcb1777595": {"node_ids": ["68284171-9e3c-457c-b5f3-1306147d2a47"], "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "fc16011d-df63-4c64-956b-cf60dfe6a02c": {"node_ids": ["d782f8fc-1f6a-4b1a-8887-82ebea70c671"], "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c2c1e965-199a-45a8-bb6d-6ab683cb8acc": {"node_ids": ["eda27de0-e840-42b9-8ec7-5f122017c53e"], "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "6503fd34-ae7b-4d44-a271-95e6cec6d187": {"node_ids": ["6e5e2d97-bbad-4179-af0a-fb1ec121817c"], "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e02d8246-f470-4045-9f61-b6ada5eb6f78": {"node_ids": ["6aa0204d-04e6-4332-8fd0-44edb6a7d135"], "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2c47c95e-b57d-4ca6-ac29-aa5cdf666e43": {"node_ids": ["efe62d02-f379-4502-9789-83173321422c"], "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b4d6db63-83d1-4fb2-9c94-5d7fe7bcedeb": {"node_ids": ["33f3ede6-5df6-417d-abd6-ecb79d3e27a6"], "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7c0d2782-15c6-4d1f-b7f3-2e486a589ec7": {"node_ids": ["55b3c148-de24-4729-969a-9b82d4afd343"], "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dfcf7971-6e71-4a2b-aa91-fa46305049b6": {"node_ids": ["302f76de-3c44-475e-8a9e-822d1e210eb2"], "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "61d8fca3-4372-4dd2-ae91-4d18833a262e": {"node_ids": ["0ba989b9-230f-47b7-ad8c-095b25a9f3c8", "3980cd5b-4d4e-4e2e-99ec-d59472cf180c"], "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7fc0fac2-b8d3-44b6-a807-f45a4bf151e6": {"node_ids": ["29b8c972-e312-466c-aedc-55f224d6377b", "5334ad4f-35f8-4668-a4cd-9a7588c99ffc"], "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "db02c5ee-cc26-4afa-9bf3-303b6d5a813f": {"node_ids": ["e26f2687-c7a5-42cd-b3fa-938e7e694189"], "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9345d441-1a96-472d-9f8d-694c31af0438": {"node_ids": ["e629bc8e-da09-43f8-b064-8e59531dc214"], "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f71088fc-52c6-480e-80a3-15d093574264": {"node_ids": ["2ba8adce-8f30-4121-8787-7069020dd9d1"], "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "080b97e4-1712-490e-a0ab-552a8bdef02b": {"node_ids": ["e885adc5-dd7a-4633-a5e5-455b09c26c0b"], "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "adcddc03-9e4b-4388-89fa-04aae2a9ccb9": {"node_ids": ["1328a3e8-8820-4b15-b9eb-fa661608bd93", "f6280781-059c-41c8-96b4-d70fb657b03f"], "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a2d6b0b5-edf4-4e67-9363-f35aedd4721d": {"node_ids": ["78485156-54a6-4e4b-950c-8630b10c039e"], "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "8e401de1-7d6d-4811-95c8-4eab0d7a3f5e": {"node_ids": ["b2a5bfc1-c413-414a-86e4-e3ccbfc5aee7", "62e0e837-8ebc-4278-a7d8-013fe6d8fe4b"], "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "434cf912-7cf4-4349-8ed5-215c3ee4ee56": {"node_ids": ["cefd4005-86b5-4faf-a469-78c5a433cf62"], "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "39ff8ed8-5b2c-47d3-a1ab-c9292850e48f": {"node_ids": ["486e2214-3f7c-417a-8c8b-4e87b5841238", "59917371-2612-4d68-85f1-f869cebb51de"], "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "1a3c2390-9797-41c6-b4ed-c32c4783bbf9": {"node_ids": ["7388d728-d29b-4b71-bff9-335b66cb0ee6"], "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d99b3ec8-f5fc-4838-bd99-20bcfbc2f0ad": {"node_ids": ["177ea331-8ba9-454e-bcd0-1ea58b71d4e3"], "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "981f68e3-8d13-49b9-a94f-e77afaf948cd": {"node_ids": ["3d2c2217-4101-4644-9c02-3a894a47a2c9"], "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "68120e16-101f-4af5-af63-aa3db48f0ecb": {"node_ids": ["7fd858dc-feea-46fc-82ae-fe386f056826", "014eae43-cb2b-41ca-a68f-ae5fd3588488"], "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c43deeb1-bc0a-4597-83ec-89fa05206ba8": {"node_ids": ["440d6004-7e26-4559-a775-99aab1416d29", "ec3edb3f-19d8-4df3-8b26-ef27f56b8c74"], "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "39743898-7152-4110-9bbe-f1b04afaf30e": {"node_ids": ["5141534c-1abd-445a-af2b-c22a2e89f5bb", "ab3c3480-844f-4e77-9c1e-907ba0fafbed"], "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ae3a5731-6c14-4b81-a9d4-a4ed212cefbf": {"node_ids": ["466e8572-6fdf-41f3-959b-f5c74172ff62", "bd8ceb3f-df48-44a4-aa83-b31cd4e2a76b"], "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d6abc588-e112-4631-8d8e-215751aee4cf": {"node_ids": ["6761b260-4c9d-4627-8e00-76d7357d5b00", "e8748485-e96e-46d9-afc5-877eb3e6ff1e"], "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ea3d537f-0394-4e79-b93f-be94bddbc93f": {"node_ids": ["56f96d72-fa67-410c-a3b4-d505986798af", "fbfe7dda-ac00-4bd7-945d-86eab268758c"], "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dfcb46db-f2d8-4946-ade8-08c13a3def5c": {"node_ids": ["f3c99cd2-cbbc-40b0-99f6-827b49d746af"], "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "cd563c1c-eb7d-4d1c-8720-86d9417a99dd": {"node_ids": ["77133867-0d0a-4b9c-92f0-59cab680b66b"], "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "57adcca6-efe8-4cda-9aa3-f06f3e58f3b1": {"node_ids": ["2854761d-7fcd-4132-a526-e311a724336c"], "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "4687383f-a972-47c8-a019-4d4c807c983b": {"node_ids": ["b79d2240-b1ee-4f24-ba86-1cae3998984d"], "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c0973131-09f0-4f1c-96bc-9391ea9d1a8b": {"node_ids": ["1d4e170a-3d04-4d74-a1fd-5aadb35dc4c1"], "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "236161d0-b7e8-49f0-bf54-b678f399062d": {"node_ids": ["852557fd-3301-4bfe-a61d-ca82c43e9c5f"], "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d9169ea3-c2a1-4cc7-810e-5c4a84f6730d": {"node_ids": ["d33cb7a8-f93c-4212-8850-e68009306bfe"], "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0e9109f1-c1e6-4869-aa72-8a949997379a": {"node_ids": ["c2cdfda8-7a32-4701-abb9-5380ff5418c7"], "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d90cf593-b9a6-4f03-ac91-a58dd0075c86": {"node_ids": ["973fc9e2-5db9-43a0-a1a7-ae658d8f1055"], "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0023bd73-95d5-4673-a31f-5f22ae536f29": {"node_ids": ["fa700a76-ff9d-4ee1-a345-aad0ce7aaff2"], "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c662354f-eed1-457f-97ae-7d853352ce78": {"node_ids": ["4e894968-ed5d-4a49-8f6a-1feb00651611"], "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "fbe517d5-2096-4f99-b8be-9513ee86091e": {"node_ids": ["a7c82d17-8f70-4f1d-ab48-67c000c27d27"], "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3f295428-5b56-4efb-a4de-e0f23cec05e1": {"node_ids": ["ead7fc51-3cf2-48c8-b63d-d46628b0f849"], "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "78e983e3-5ca2-46d2-96c8-a2ad45268e07": {"node_ids": ["afb992d3-919b-457b-848b-40c91fdef6e6"], "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "77984cd6-67c2-42c4-a432-5349a1a8022d": {"node_ids": ["520cab2e-7869-4b67-b8e9-cfc9ae53e0c7"], "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bfe68aca-4b89-4975-969e-e4df13a5f0cc": {"node_ids": ["49401694-e006-4f65-92e8-0ce28dfeb8d8"], "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "299a7be1-9f06-43c1-979c-7c22ae413117": {"node_ids": ["4c276dad-47a1-4c9c-91af-a118fa86daaa"], "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "204186e0-05b9-42fe-b3f8-cd482a7eb3d8": {"node_ids": ["b6d200ec-9182-49a0-9837-c61edf0413de"], "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b1871c86-2ec3-4443-b8ff-432d7ecc887b": {"node_ids": ["704c9b2d-c2c7-459b-8d3c-cf09a36b1eae"], "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "455a1b36-879d-4cab-859d-ba7837494abe": {"node_ids": ["aebb4f93-89e3-456f-8ed4-9a62578d09fe"], "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "692b1c6b-80f5-4e74-8ad2-88169551f8dd": {"node_ids": ["c03e7e5d-4b33-40ab-9633-9edab67cbb23"], "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5faeaca7-9d4f-45f3-915c-6319dfcd58e6": {"node_ids": ["f9993feb-f461-448f-b430-db581f5b543d"], "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f37bc684-70d9-48ca-9a39-e95b925fdb24": {"node_ids": ["71428714-d8f6-46fd-a802-8cdd7fb84c82"], "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bfa34520-04f4-4089-bb66-4d3abdf2a2cb": {"node_ids": ["0c9156b8-8038-4fd7-a7dd-7b220b5e7c2a"], "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "21debbee-1d51-46b0-a172-697bf6d5d2e5": {"node_ids": ["ebc1ee53-2e8c-423b-9413-5f8e385573b4"], "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "4fe9dfff-bf79-4b07-afc0-3ec00caf91c4": {"node_ids": ["eff31749-96ea-4fb3-9fbb-23af8f244693"], "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dbda075b-6840-4c04-b4de-d56fd8eaa43c": {"node_ids": ["5e4aa46c-c968-4769-9eb8-2c87675c49bb"], "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a437b24d-5323-4c43-a1dd-0e9eff58e51b": {"node_ids": ["e566adc4-edaf-4f8b-964f-eb6be0252d2c"], "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ff3ad51b-9b19-4f0d-ac01-dfd784629f2f": {"node_ids": ["b7976034-b756-4fa5-b20c-47413e9e492c"], "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a9cda855-5d81-482b-a385-bb9c488d130a": {"node_ids": ["a5afaab4-c60b-4b85-9368-5531f9005c1c"], "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "589c5b12-4927-4cb1-b8f5-a5d021f7fe2e": {"node_ids": ["8b16887e-d4d6-4077-8acc-67d7e89a0ac4"], "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "eca59d18-33f3-4a2f-b059-7b89c3f325a6": {"node_ids": ["9fcad995-6fc0-421b-86ed-fb3538ead9f0"], "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "52eb3eae-9d0d-4712-9197-64d8281f7aed": {"node_ids": ["895f306f-0c03-4f11-9418-34792a0c7a35"], "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "233b941d-4b35-44a9-890f-7e9135c16b4d": {"node_ids": ["d895f2fb-2d30-4093-87ff-4f9a477e5346", "19294647-156a-470a-965e-f7f63c9117c8"], "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3e35c7bd-27ac-46ca-b1bb-cce296931b82": {"node_ids": ["80db4b67-ce50-427b-8322-c2144e823ca3"], "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "80076866-e602-4250-818b-46ec6facb36f": {"node_ids": ["6779e741-70c4-4c3e-bbab-08bc83490797"], "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7b85b298-fd49-4ba8-9383-736c92d57466": {"node_ids": ["a25db37d-f01e-4170-a8e6-3bffd64191f3", "52841cf5-5677-4734-b950-c4c2d22a498a"], "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "279328f0-56f2-427d-ae9e-01db9f67472c": {"node_ids": ["d51845d3-4b44-4344-9d00-dc96c6f86884"], "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "350146c2-9a3c-400f-873f-92b3b564abe3": {"node_ids": ["704d03f9-b78f-479f-b0c0-7b0c1576ae53"], "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "63146ebb-319f-461f-8f5a-86cc7adc1498": {"node_ids": ["de8bd408-a7ff-46b0-8615-27b3262fc1bc"], "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c2cf2e60-32c6-498b-bdc6-785575e4f765": {"node_ids": ["f02b3c1a-6580-4633-b9b9-c28dfa27a808", "4ae011c2-b13b-4c81-9f02-d01836c5b13d"], "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "8db6264f-0b28-4326-88db-6038bc565edc": {"node_ids": ["b9b03e82-7837-484a-adcd-3c8d49d330aa", "f9d3b38d-2100-4cca-bac8-dd421ca3556a"], "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0b74ff41-9bfd-488f-9319-a5f56fe276b0": {"node_ids": ["8ce040ea-1e5e-4b93-8546-b1cfc86ced1e", "a9337de9-5869-4490-9591-5d05bfb30f5a"], "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "14e3892b-29c1-4c5a-b868-790337ea1e4b": {"node_ids": ["1d24aeab-16a9-41e5-90ca-4b69a2b031c1"], "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "fd1b1618-e5d1-483a-ab0d-ae93ab9d300c": {"node_ids": ["0eb12a84-69f4-45cc-9aee-72b0024cc6ae"], "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3b70994a-8c2a-42ae-b0c0-4a86ce7c3b57": {"node_ids": ["cba646f6-7fc1-401f-8645-0d5890dfb387"], "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "4fef60c5-e84d-4ad2-bc6d-435da3bc56bd": {"node_ids": ["28944c87-6729-4d1a-b73b-4a1058a5f0f1"], "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9999bb3b-5e4d-4ecf-8d53-ca9b2eb6ecce": {"node_ids": ["224736fe-83b3-4a3a-852c-89853c34726f"], "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3f667b7f-5f2b-4019-950d-aa6bb82831da": {"node_ids": ["1ebb82a7-dc3b-4fa0-a17b-9177fbcb5b4d"], "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "306ed2f7-b6ed-4f35-83cc-977631112c8f": {"node_ids": ["beb44e96-1fee-48cd-89d3-fc71f9e58855"], "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "12794cab-7416-40f2-9cf3-9cc7b3c5aa3c": {"node_ids": ["87ccc4f3-14a7-40d1-a7bd-da676bb871c2"], "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "236a3bb2-64f2-4a86-b396-39fe2f4c6f8e": {"node_ids": ["2ef4c5f3-3b4d-457e-ac47-fb2662ce3483"], "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "34342f5d-4243-4a80-85bd-87c8a0eb5db3": {"node_ids": ["4c29ba2c-16aa-40bf-8629-de63ca5db99d"], "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "64547a6a-f338-428b-92c3-49cb822749b0": {"node_ids": ["4a824857-e0ad-43d7-86b0-e4cac21a719d"], "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3897af41-de98-441e-9fd4-7f93c7a3eb5f": {"node_ids": ["7e8d3475-8743-421a-b0f0-bf134ff63c4c"], "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c4dc3664-baa2-4bec-ad10-4aa114388b2c": {"node_ids": ["b536a2a6-a5f8-4545-9f19-d5d547f552ed"], "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7f39b935-39fe-4afb-bd30-8f998b21c4b8": {"node_ids": ["60f09d8f-1086-4c8d-8a4a-673ab16b4eaf"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ca034df0-0341-47e7-bd59-180f46fa2774": {"node_ids": ["1104fabf-12ee-44e9-b592-9ee2f489702d"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d6d25e31-be38-4a95-bbc5-848b8e856517": {"node_ids": ["f9d9fe77-3ccc-4a32-b8ee-93425486aa90"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dae7359a-02d5-48c4-900b-5890bff51b9b": {"node_ids": ["ac5f6796-cb0c-4fbc-9d9d-3080c12897da"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a7d897be-d7a3-4dc4-9bab-84a1bc880122": {"node_ids": ["4faf397b-f967-4bdc-8d48-04b309c7776a"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "6f3568aa-4847-4e02-bc68-372a1d0fe3ff": {"node_ids": ["024a09f3-bd37-405c-9176-e02f67b122f1", "d433150f-dfd9-4628-a5ee-6deba55d12c3"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}}}