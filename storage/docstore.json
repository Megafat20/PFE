{"docstore/data": {"44aae20f-64f6-4313-9d4e-1007f36541d5": {"__data__": {"id_": "44aae20f-64f6-4313-9d4e-1007f36541d5", "embedding": null, "metadata": {"page_label": "1", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cf0bc84-4f19-48ed-8fb1-6928bc429853", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "287adec51e5d7ea95f46738022d119c4d4c4a9b66d9c9c4d917429b604444936", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CEDILLE :\nA LARGE AUTOREGRESSIVE LANGUAGE MODEL IN FRENCH\nMartin M\u00fcller\u2217 Florian Laurent\u2217\nCedille AI1\nhello@cedille.ai\nABSTRACT\nScaling up the size and training of autoregressive language models has enabled novel ways of solving\nNatural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale\nlanguage models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages\nother than English remain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, speci\ufb01cally trained for the French language. Our results show that\nCedille outperforms existing French language models and is competitive with GPT-3 on a range\nof French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity\nexhibited by these models, showing that Cedille marks an improvement in language model safety\nthanks to dataset \ufb01ltering.\n1 Introduction\nLarge autoregressive language models have drawn wide\nattention due to their zero-shot and few-shot capabilities,\nallowing them to be used for a wide variety of Natural Lan-\nguage Processing tasks without the need for task-speci\ufb01c\n\ufb01netuning or annotation data [1, 2]. Additionally, previ-\nous work highlights the improved sample and compute\nef\ufb01ciency of larger models, generally justifying the move\ntowards larger models [3].\nAlthough large language models, such as GPT-3 [2], have\nbeen trained on multilingual corpuses, the performance on\nNLP tasks may vary signi\ufb01cantly between languages. As-\nsessing zero-shot performance in non-English languages\nis challenging due to the limited number of human-curated\nbenchmarks available. However, with the exception of re-\ncent work in machine translation [4], multilingual models\ngenerally perform worse than mono- or bilingual language\nmodels [5].\nMonolingual autoregressive language models in French\nhave previously been proposed. GPT-fr [6] and PAGnol [7]\nhave been trained on \ufb01ltered versions of Common Crawl2\nand CCNet [8], respectively. Both works highlight the im-\nportance of deduplicating and \ufb01ltering of pre-training data\nand use decoder-only transformer architectures, closely\nfollowing the GPT models with model sizes reaching 1B\nand 1.5B parameters, respectively. It\u2019s worth noting that\nthese works do not directly compare performance against\nextreme-scale large multilingual models, such as GPT-3,\nin particular with regard to zero-shot tasks.\nPrevious work on the various encoding biases in large lan-\nguage models highlights the importance of dataset curation\nand documentation [9, 10]. Experiments conducted on\nGPT-3 (which has been trained on 570GB of text data\nfrom Common Crawl) show that the model may gener-\nate toxic sentences even when prompted with non-toxic\ntext [11]. Although applying \ufb01ltering of training data using\nautomated toxicity scores may introduce classi\ufb01er-speci\ufb01c\nbiases [12], this technique remains more effective than\n\u2217Authors contributed equally, order is random\n1Coteries SA, EPFL Innovation Park, Lausanne, Switzerland\n2https://commoncrawl.org/\narXiv:2202.03371v1  [cs.CL]  7 Feb 2022", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ea269a8-7360-4d2d-ad75-8f0dd75aca58": {"__data__": {"id_": "7ea269a8-7360-4d2d-ad75-8f0dd75aca58", "embedding": null, "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc49187f-8138-4140-b291-bab8c53111c7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "53af3ed08f91c404261c837742804e31e262ecf1c7c9372d96b31c448f763f7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a05de66a-a4cd-46c9-925a-f8c71f4d87d0", "node_type": "1", "metadata": {}, "hash": "2b5ee8dd5057a3f27bc036338034bc71c70ede3d2973a03ddee3d35da2d48833", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "decoder-based detoxi\ufb01cation using methods such as swear\nword \ufb01lters, PPLM [13], soft prompt tuning [14] or toxicity\ncontrol tokens [15].\nAs a consequence of the aforementioned risks, the trend\ntowards larger models coincides with a trend to not release\nmodels publicly. Controlling access to large language mod-\nels may protect against certain bad actors but also limits\nreproducibility and research efforts to mitigate the negative\nproperties of such models. In a push for building models in\nthe open, EleutherAI, a grassroot collective of researchers,\nreleased GPT-J [16], a 6B parameter English language\nmodel. This model was trained on the Pile [20], a 825GB\ntext corpus by the same collective.\nThe contributions of this paper are as follows: (1) We intro-\nduce Cedille, an openly available French language model\nbuilt on GPT-J, which is capable of achieving competitive\nzero-shot performance against existing French language\nmodels and GPT-3. (2) We release the toxicity scores\nof the complete French C4 dataset, and (3) we provide a\ncomparison of Cedille\u2019s toxicity to other language models\n(including GPT-3).\n2 Methods\n2.1 Model architecture\nOur model architecture is identical to GPT-J [16]. GPT-J\nuses a similar transformer architecture to the one used in\n6.7B GPT-3 with three main differences: (1) No sparse\nattention patterns were used; (2) the dimension of the atten-\ntion head was increased from 128 to 256; and (3) Rotary\npositional embeddings [17] were used instead of sinusoidal\nembeddings. See Table 1 for more details.\nNumber of parameters 6,053,381,344\nNumber of layers N 28\nModel dimensions dmodel 4096\nFeed-forward dimension dff 16,384\nNumber of attention heads nheads 16\nHead dimension dhead 256\nContext size 2048\nV ocab size 50,257\nTable 1:Cedille model details.\n2.2 Training data\nCedille is trained on a \ufb01ltered version of the French part\nof the multilingual C4 (mC4) dataset [18], which contains\n332M documents or 1.1TB of uncompressed text. mC4 is\nextracted from 71 Common Crawl snapshots (years 2013\nto 2020) and uses CLD33, a small feed-forward neural net-\nwork, for language identi\ufb01cation. mC4 \ufb01ltered out pages\nof less than three lines of at least 200 characters.\nWe apply two different forms of \ufb01ltering to the dataset 1)\ntoxicity \ufb01ltering using the Detoxify model [19] and 2) loss\n\ufb01ltering using the FlauBERT model [20]. For both \ufb01ltering\nsteps we compute the metric on a per document level of the\nentire base dataset. In some cases chunking the documents\ninto splits of 1200 characters was necessary due to the\n\ufb01xed context size of the used models. Chunks smaller than\n600 characters were not evaluated. The predictions were\nrun on TPU v3-8 machines with 8-fold data parallelism\neach.\nEach percentile as well as the tails of both the loss and the\ntoxicity distribution were sampled and manually inspected\nto \ufb01nd suitable cut-off values for \ufb01ltering. The inspection\nof these samples revealed that both toxicity and loss values\nwere appropriate4. We removed documents correspond-\ning to a toxicity score higher than 0.5, corresponding to\n0.25% of the content (0.8M documents). For the loss \ufb01l-\ntering we considered the loss distribution of each of the\n2048 \ufb01les and removed documents below a 0.2 percentile\nloss (corresponding to a loss value of roughly 4.5) and\nabove an absolute loss value of 10. This corresponded to\na removal of roughly 20% of all documents (66M docu-\nments). The combined \ufb01ltering led to a \ufb01nal training set of\n265M documents, which corresponds to roughly 773GB\nof uncompressed text.\nThe text was then run through the fix_text method of\nthe Python library ftfy [21] using NFKC normalization\nand encoded using the unmodi\ufb01ed GPT-2 tokenizer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a05de66a-a4cd-46c9-925a-f8c71f4d87d0": {"__data__": {"id_": "a05de66a-a4cd-46c9-925a-f8c71f4d87d0", "embedding": null, "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc49187f-8138-4140-b291-bab8c53111c7", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "53af3ed08f91c404261c837742804e31e262ecf1c7c9372d96b31c448f763f7b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ea269a8-7360-4d2d-ad75-8f0dd75aca58", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "07f6c4d28b2dd319e4b631e8f55f5d193cc8e2260eb8b49dfd5c39f12ffc8e41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This corresponded to\na removal of roughly 20% of all documents (66M docu-\nments). The combined \ufb01ltering led to a \ufb01nal training set of\n265M documents, which corresponds to roughly 773GB\nof uncompressed text.\nThe text was then run through the fix_text method of\nthe Python library ftfy [21] using NFKC normalization\nand encoded using the unmodi\ufb01ed GPT-2 tokenizer. Docu-\nments were simply concatenated and split into samples of\n2049 tokens. The \ufb01nal training set yielded a total of 130M\nsamples corresponding to 268B tokens.\n2.3 Training process\nCedille was trained starting from the of\ufb01cial GPT-J model\ncheckpoint using the mesh-transformer-jax codebase [22].\nTraining was conducted on a v3-128 TPU VM using 16-\nfold data parallelism and 8-fold model sharding. For all\nour experiments we used an effective batch size of 256.\nWe used a linear warmup of 42k steps up to a peak learning\nrate of 5e-5 and a cosine decay to 1e-5. Weight decay was\nset to 0.1. Cedille was trained for 150k steps, which corre-\nsponds to 0.3 epochs on the training set or 78.7B tokens.\nThe starting and \ufb01nal training perplexities were 6.13 and\n3.89, respectively. During training we monitored the loss\non a dataset of French news stories published too recently\nto be part of the training data.\n3https://github.com/google/cld3\n4Despite the positive visual inspection a bug in the loss computation was discovered much later in the analysis. Further investiga-\ntion revealed that roughly 10% of samples were wrongly included in the \ufb01nal dataset as a result. Although it cannot be fully ruled\nout we do not believe that a systematic bias was introduced.\n2", "mimetype": "text/plain", "start_char_idx": 3331, "end_char_idx": 4956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2b7f3b72-f1a5-49dc-968c-4732c031ec90": {"__data__": {"id_": "2b7f3b72-f1a5-49dc-968c-4732c031ec90", "embedding": null, "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cad8c60e-57da-4b63-9b61-54b93306b7b8", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "176c7426381f344f60771c67cc0ee63a5c5546d0edec0af13eb9c0fbe1a8f2f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62ed9e17-9110-429f-b071-a875b0b5b23f", "node_type": "1", "metadata": {}, "hash": "851d16ed995f1300ae2789b3b91801f7350857c38fa3ff44be28192346246c65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.4 Evaluation\nZero-shot performance was evaluated using a forked ver-\nsion of the lm-evaluation-harness codebase [23]. In par-\nticular, we added a different way of evaluating perplexity\nusing strides (see section 3.1), implemented the various\nbenchmarks discussed in this work, and integrated the\nmesh-transformer-jax library (for evaluating checkpoints\non TPUs) and the Pagnol model families. Benchmarking\nwas conducted on v3-8 TPU VMs and on A100 GPUs.\nToxicity evaluation was conducted using a modi\ufb01ed ver-\nsion of the real-toxicity-prompts codebase 5. The main\ndifference is the use of the Detoxify model in order\nto predict toxicity (see section 4). Our adapted code-\nbase is available at https://github.com/coteries/\nreal-toxicity-prompts.\n3 Tasks\n3.1 Perplexity\nModel #params Byte-PPL Token-PPL\nGPT-3 (ada) 1.3B a 1.930 7.952\nGPT-3 (babbage) 6.7B 1.973 6.447\nGPT-3 (curie) 13B 1.809 5.082\nGPT-3 (davinci) 175B 1.656 3.993\nGPT-J 6.05B 1.746 5.797\nCedille 6.05B 1.646 3.932\nPagnol (small) 124M 1.852 17.802\nPagnol (medium) 335M 1.775 14.623\nPagnol (large) 773M 1.725 12.791\nGPT-fr (base) 1B 2.090 11.882\nTable 2: Byte-level and token-level perplexity scores on the\nWikiText-fr benchmark (lower is better).\naOpenAI hasn\u2019t of\ufb01cially disclosed the size of the models\nprovided by their API, however recent experiments suggest the\nmapping presented in the table [24].\nZero-shot perplexity was evaluated on the test subset of\nthe WikiText-fr6 dataset [6], containing articles from the\nFrench Wikipedia which are part of the \u201cquality articles\u201d or\n\u201cgood articles\u201d categories, similar to the English WikiText-\n103 dataset [25]. The test set contains 589k words or 3.7M\ncharacters of cleaned French text from 60 articles. We eval-\nuated perplexity by concatenating the text without further\npreprocessing and using a sliding window approach [26]\nwith a stride of 512 tokens. Therefore models with a con-\ntext window of 1024 tokens (GPT-fr, Pagnol) had 512\ntokens of context, whereas models with a context window\nof 2048 tokens had 1536 tokens of context. Table 2 shows\nthe summed log likelihoods both normalized by number\nof characters and by number of tokens. Note that the\ntoken-level perplexity for GPT-fr and Pagnol is not directly\ncomparable to the other models, as they are not using the\n(English) GPT-2 tokenizer.\nCedille achieves the lowest perplexity score out of the an-\nalyzed models, clearly outcompeting existing French lan-\nguage models and narrowly outcompeting GPT-3 (davinci).\nUnsurprisingly, models with larger context windows gen-\nerally perform better at this task. It is noteworthy that the\ntest dataset is likely contained in the training data as no\ndataset-speci\ufb01c \ufb01ltering of the training data was conducted\nas part of this work.\n3.2 Summarization\nWe evaluated the summarization capabilities on the Orange-\nSum benchmark, as introduced in the BARThez work [27]\nas a French equivalent of XSum [28]. The benchmark con-\ntains news articles published between February 2011 and\nSeptember 2020, scraped from the French website \u201cOrange\nActu\u201d. The models were given the news article in the test\nsubset using the following prompt:\n{article text}\\nPour r\u00e9sumer :\nThe models were tasked to generate 100 tokens using top-k\nof 2 and a temperature of 1, following the methodology\nin [1]. We used greedy decoding (top-k = 1) for GPT-3,\nsince at the time of this work being conducted, the API\ndidn\u2019t allow for other top-k values.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62ed9e17-9110-429f-b071-a875b0b5b23f": {"__data__": {"id_": "62ed9e17-9110-429f-b071-a875b0b5b23f", "embedding": null, "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cad8c60e-57da-4b63-9b61-54b93306b7b8", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "176c7426381f344f60771c67cc0ee63a5c5546d0edec0af13eb9c0fbe1a8f2f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b7f3b72-f1a5-49dc-968c-4732c031ec90", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "9cb38311827537a180cfc8aa7a3546062256744ac6125b7e9c44abb6c53d616f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The models were given the news article in the test\nsubset using the following prompt:\n{article text}\\nPour r\u00e9sumer :\nThe models were tasked to generate 100 tokens using top-k\nof 2 and a temperature of 1, following the methodology\nin [1]. We used greedy decoding (top-k = 1) for GPT-3,\nsince at the time of this work being conducted, the API\ndidn\u2019t allow for other top-k values. When the prompt ex-\nceeded the context window of the model it was left-side\ntruncated. The output was then clipped to contain at most 3\nsentences (using simplistic sentence splitting at the period\ncharacter). Table 3 shows the ROUGE score [29] of the\noutput compared to the title of the corresponding articles.\nModel R1 R2 RL\nGPT-3 (ada) 13.95 4.75 11.59\nGPT-3 (babbage) 4.62 1.76 3.86\nGPT-3 (curie) 5.28 2.21 4.42\nGPT-3 (davinci) 15.49 5.82 13.05\nGPT-J 14.46 4.72 11.68\nCedille 14.74 4.83 11.86\nPagnol (small) 8.52 1.61 7.24\nPagnol (medium) 8.98 1.86 7.55\nPagnol (large) 9.19 1.85 7.71\nGPT-fr (base) 10.15 2.60 8.27\nTable 3:Performance of summarization in French. Shown are\nthe ROUGE scores on the OrangeSum dataset (higher is better).\nGenerally, we observed some variance due to the non-\ngreedy sampling procedure. However, computational limi-\n5https://github.com/allenai/real-toxicity-prompts\n6https://huggingface.co/datasets/asi/wikitext_fr\n3", "mimetype": "text/plain", "start_char_idx": 3055, "end_char_idx": 4379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dadec924-b3e5-4ac9-9d9d-08ce26e51c32": {"__data__": {"id_": "dadec924-b3e5-4ac9-9d9d-08ce26e51c32", "embedding": null, "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c36ef3b-ec00-4991-8993-fc0fa6c9af08", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7abe246e1a4c4f6ad42781b294dccc357795ecc51eb691c2a21d907e068b51ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "431846f2-a241-491f-a310-f87428bd313f", "node_type": "1", "metadata": {}, "hash": "995a1a7857e57960634f514e149e0778d1e591ddffd3aa2d0c55669d5ceb96fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tations and cost made it dif\ufb01cult to estimate this variance.\nWe also observed that the choice of the pre\ufb01x (\u201cPour r\u00e9-\nsumer :\u201d) strongly in\ufb02uences the scores. Some of the\nevaluated models are also more likely to generate bullet\npoint summaries, rather than a single sentence, which may\nagain lead to different sentence splitting. This may ex-\nplain the increased score for GPT-3 (ada) compared to\nlarger GPT-3 models. Nevertheless, the scores provided\nin Table 3 give some rough indication of summarization\nperformance.\n3.3 Question Answering (QA)\nQuestion answering (QA) was evaluated on FQuAD\n(French Question Answering Dataset) [30], a dataset in-\nspired by the English SQuAD equivalent [31]. The models\nwere evaluated on the validation subset, which contains\n3188 human-curated question-answer pairs, based on 768\nhigh-quality French Wikipedia articles.\nModel F1 Exact match (%)\nGPT-3 (ada) 19.09 4.48\nGPT-3 (babbage) 26.16 8.81\nGPT-3 (curie) 39.49 17.84\nGPT-3 (davinci) - -\nGPT-J 26.14 6.96\nCedille 34.59 12.23\nPagnol (small) 10.66 0.43\nPagnol (medium) 13.80 0.84\nPagnol (large) 17.67 2.72\nGPT-fr (base) 15.15 2.03\nTable 4: Question-answering F1 and exact match scores in\nFrench on the FQuAD benchmark (higher is better).\nThe models were evaluated using the SQuAD v2 met-\nric [31], which also takes into consideration \u201cno answer\u201d\nprobabilities, i.e. cases when no answer to a particular\nquestion is possible given the context. The models were\ntasked to generate 100 tokens and at most 1 sentence using\ngreedy sampling and the following prompt:\nTitre: {title}\\nContexte: {context}\\n\\n\nQuestion: {question}\\n\\nR\u00e9ponse:\nThe \u201cno answer\u201d probabilities were calculated against the\nstring:\n{prompt} Sans r\u00e9ponse.\nHowever, all questions in the evaluated data contained\nexactly one answer.\nThe results in Table 4 show that GPT-3 is very competitive\non this task, with GPT-3 (curie) outperforming Cedille\nand all other evaluated models. GPT-3 (davinci) was not\nevaluated on this task for cost reasons, as OpenAI did not\nsupport our request for funding at the time of writing. The\nresults may be contrasted to a \ufb01netuned version of Camem-\nBERT [32] which yields F1 of 88% and best match of 78%\non this dataset [30].\n3.4 Translation\nZero-shot translation was evaluated for the language pair\nEnglish and French on the WMT14 dataset [33]. Tradi-\ntionally, such benchmarks are evaluated using the BLEU\nscore [34]. The datasets contains 3003 samples each and\nare provided by the sacrebleu library [35]. The zero-shot\ntask is formulated using the following pattern:\n{source_lang} phrase: {text}\\n{target_lang}\nphrase:\nWhere source_lang and target_lang are French and\nEnglish, respectively, depending on the direction. Greedy\nsampling is used to generate 256 tokens. The output was\nclipped to at most 1 sentence.\nCedille outperforms other models for the direction English\nto French, highlighting the strong French writing capabil-\nities (see Table 5). Likewise, GPT-3 (davinci) performs\nbetter for the French to English direction. Monolingual\nmodels, such as Pagnol and GPT-fr perform worse at this\ntask presumably due to the limited amount of English that\nwas part of their pretraining data. Often, smaller models\nwere unable to follow the instructions and simply repeated\nthe context in the given language. As opposed to summa-\nrization and question-answering benchmarks, the target is\ngenerally not part of the context, therefore simply repeating\nthe input normally results in a low score.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "431846f2-a241-491f-a310-f87428bd313f": {"__data__": {"id_": "431846f2-a241-491f-a310-f87428bd313f", "embedding": null, "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c36ef3b-ec00-4991-8993-fc0fa6c9af08", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7abe246e1a4c4f6ad42781b294dccc357795ecc51eb691c2a21d907e068b51ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dadec924-b3e5-4ac9-9d9d-08ce26e51c32", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e11cc431edbca232630ad7ba219e1f3e296f3b893bbf617bb4807b2c92e83098", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Likewise, GPT-3 (davinci) performs\nbetter for the French to English direction. Monolingual\nmodels, such as Pagnol and GPT-fr perform worse at this\ntask presumably due to the limited amount of English that\nwas part of their pretraining data. Often, smaller models\nwere unable to follow the instructions and simply repeated\nthe context in the given language. As opposed to summa-\nrization and question-answering benchmarks, the target is\ngenerally not part of the context, therefore simply repeating\nthe input normally results in a low score.\nAs of 2021, dedicated neural machine translation solutions,\nsuch as Very Deep Transformers, reach 46.4 BLEU for\nEnglish to French translation [36].\nModel BLEU (en \u2192fr) BLEU (fr \u2192en)\nGPT-3 (ada) 2.71 16.64\nGPT-3 (babbage) 3.20 24.56\nGPT-3 (curie) 13.45 27.15\nGPT-3 (davinci) 20.40 27.70\nGPT-J 14.71 26.06\nCedille 24.89 20.59\nPagnol (small) 0.76 1.20\nPagnol (medium) 1.07 1.48\nPagnol (large) 1.06 3.47\nGPT-fr (base) 1.47 1.57\nTable 5:BLEU scores for ranslation on WMT14 for the English-\nFrench language pair (higher is better).\n4 Toxicity analysis\nIn order to evaluate the toxicity of the model we closely\nfollowed the work conducted in [11]. We studied the case\n4", "mimetype": "text/plain", "start_char_idx": 2939, "end_char_idx": 4142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "302ddf55-dfde-4527-a4bd-236e2de9f884": {"__data__": {"id_": "302ddf55-dfde-4527-a4bd-236e2de9f884", "embedding": null, "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c35c144-0e93-4fbd-b142-36072f24c9b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "97d31a28c7f9a05d28c1d144e8211ee1a569a7d7551e0453048e902d36d55e90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b73e086-deb0-4ec8-8561-72207e630872", "node_type": "1", "metadata": {}, "hash": "e952502686cf8ab1b02a67f775b562d62394d165311c92246d08f44943db5e1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of unprompted (i.e. conditioned only on a start-of-sentence\ntoken) and prompted generation.\nThe original work in [11] used the Perspective API, a ser-\nvice that uses machine learning classi\ufb01ers to estimate the\nperceived toxicity of text. In this work, we employ the\nDetoxify tool [19] instead. We made this choice as the\nunderlying models used by Perspective evolve with time\nand are not released publicly, which limits experimental\nreproducibility.\nDetoxify assigns a toxicity score between 0 and 1, with 1\ndenoting \u201ca very hateful, aggressive, or disrespectful com-\nment\u201d. We refer to content with a score> 0.5 as \u201ctoxic\u201d.\nWe use the \u201cmultilingual\u201d Detoxify model from release\nv0.4.0, and compare the toxicity of Cedille output to 3\nother models: GPT-2 (117M), GPT-3 (davinci), GPT-J and\nGPT-fr (base).\n4.1 Unprompted toxicity\nFor the unprompted toxicity we analyze the expected max-\nimum toxicity, i.e. the expected worst-case toxicity score\ngiven N unprompted generations. Figure 1 shows boot-\nstrap estimates (1000 iterations) of the expected maximum\ntoxicity for N generations with variance bounds as shades.\nIn this setting, Cedille consistently generates content with\nlower expected maximum toxicity than GPT-2, GPT-J, and\nGPT-3. After 100 generations, this value is under 0.5 for\nGPT-fr and Cedille (0.41 and 0.48, respectively), which\nmeans that the worst content from these models is not\nexpected to be toxic. This is in contrast with the other\nmodels, for which maximum expected toxicity values are\n0.64, 0.54 and 0.56.\nAfter 10K generations, Cedille and GPT-fr are the only\nmodels for which the expected worst outputs don\u2019t reach\na toxicity level of 1.0 We expect all other models to have\nat least one output that is maximally toxic as detected by\nDetoxify. Generally the two models that perform best are\nGPT-fr and Cedille, which were both trained on carefully\n\ufb01ltered datasets, pointing to the importance of dataset cu-\nration when considering the safety of language models.\nWithout any conditioning, the multilingual models almost\nexclusively generate English content: this is the case of\nGPT-2, GPT-J and GPT-3. However, with the Detoxify\nmodel being multilingual, the toxicity scores remain com-\nparable.\n4.2 Prompted toxicity\nFor prompted toxicity we used a set of 50 French prompts\nwith values of toxicity spanning the full range, with a mean\nof 0.34. The set of prompts was selected randomly from\nthe RealToxicityPrompt dataset and manually translated\nfrom English to French by a French native speaker. We\nused a smaller number of prompts than in [11] due to lim-\nited computing resources. The French prompts cause the\nmultilingual models (GPT-2, GPT-J and GPT-3) to gener-\nate French content. For each prompt, each model generates\n50 completions. We used nucleus sampling with p = 0.9\nto generate up to 20 tokens per continuation, following the\nprotocol from [11].\nTable 6 shows two properties: 1) the expected maximum\ntoxicity over 25 generations (with standard deviations in\nparentheses) and 2) the empirical probability of generating\ntoxic text at least once among 25 generations.\nModel Exp. max tox. Prob. toxicity\nGPT-2a 0.63 (0.23) 0.66\nGPT-3 (davinci) 0.68 (0.27) 0.74\nGPT-J 0.73 (0.26) 0.78\nCedille 0.66 (0.27) 0.72\nGPT-fr (base) 0.73 (0.27) 0.78\nTable 6:Toxicity of prompted generations.\naUpon manual inspection, it appeared that GPT-2 is unable\nto generate sensible French content, and as such the resulting\ntoxicity values can\u2019t be compared to other models.\nFor both properties, Cedille outperforms the other models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b73e086-deb0-4ec8-8561-72207e630872": {"__data__": {"id_": "3b73e086-deb0-4ec8-8561-72207e630872", "embedding": null, "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c35c144-0e93-4fbd-b142-36072f24c9b9", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "97d31a28c7f9a05d28c1d144e8211ee1a569a7d7551e0453048e902d36d55e90", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "302ddf55-dfde-4527-a4bd-236e2de9f884", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0909fc0ca685d50bc8702aaf8b601205beddfa4929a87e7fb9ece4afab80ce16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "aUpon manual inspection, it appeared that GPT-2 is unable\nto generate sensible French content, and as such the resulting\ntoxicity values can\u2019t be compared to other models.\nFor both properties, Cedille outperforms the other models.\nWe can see again that Cedille is less toxic than GPT-J,\nindicating that the training not only improved the model\u2019s\nFrench capabilities, but also increased its safety.\n5 Conclusions\nIn this work we introduced Cedille, a large auto-regressive\nFrench language model. Our work shows that mono-\nlingual models such as Cedille, can be competitive com-\npared to extreme scale multilingual language models, i.e.\nGPT-3. Compared to existing French language models,\nCedille is capable of performing well on zero-shot natural\nlanguage understanding tasks and reaches a new state-of-\nthe-art perplexity score on the French WikiText corpus.\nLastly, our approach of toxicity \ufb01ltering of the training\ndata led to a decrease in both maximum toxicity as well as\nthe likelihood of toxic output.\nAs a result of the \ufb01netuning approach starting from GPT-J,\nCedille has been exposed to a large amount of both English\nand French language data from the Pile and French mC4.\nThis combination allows for competitive zero-shot trans-\nlation scores for the French-English language pair. Early\nexperiments indicate that \ufb01netuning an existing English\nlanguage model and adapting it to French is more ef\ufb01cient\neven with considerable compute and data investments (see\nappendix).\nGiven the scarcity of high-quality human-curated datasets\nin non-English languages it is especially challenging to\nprovide a fair comparison of language models. For the\nzero-shot benchmarks we observed a high degree of sen-\nsitivity towards evaluation settings such as pre\ufb01xes, sam-\npling parameters, and type of evaluation metric. The scores\n5", "mimetype": "text/plain", "start_char_idx": 3319, "end_char_idx": 5141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f7c0e0a0-0161-4430-85c8-30fd55ad4a9e": {"__data__": {"id_": "f7c0e0a0-0161-4430-85c8-30fd55ad4a9e", "embedding": null, "metadata": {"page_label": "6", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e0304c1-0d9e-4aa1-afd6-24ecdc69460e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e739ab887f2efe4f7e350846d39aec9158ed1382a78c2653762ec8caf061d3e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 100 1K 10K\nNumber of Generations\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Expected Maximum Toxicity\nGPT-2\nGPT-3\nGPT-J\nGPT-fr\nCedille\nFigure 1:Unprompted expected maximum toxicity against increasing numbers of generations.\nshould therefore only be considered as a rough guidance\nand model performance may be highly task speci\ufb01c. In this\nwork we haven\u2019t provided performance metrics for other\nNLP tasks such as text classi\ufb01cation or word sense disam-\nbiguation. Furthermore, this work focused on zero-shot\nevaluation, ignoring few-shot or \ufb01netuning approaches.\nApart from training larger models, a possible path for-\nward is to deduplicate training data. This method has been\nshown to improve end-task performance signi\ufb01cantly [8,\n37] but was not conducted as part of this work. In order to\nfurther reduce language model toxicity, a possible direc-\ntion is the integration of human feedback in the training\nprocess in order to reduce toxic output generation [38].\nData availability. Cedille is available under the MIT\nLicense on the Hugging Face model hub: https:\n//huggingface.co/Cedille/fr-boris, and on our\nGitHub repository: https://github.com/coteries/\ncedille-ai. Regarding the French mC4 toxicity scores\nand toxicity analysis code, please refer to: https://\ngithub.com/coteries/real-toxicity-prompts.\nFunding. This work was funded by, and conducted at,\nCoteries SA7. The model was trained on Cloud TPUs pro-\nvided by Google\u2019s TPU Research Cloud program.\nAcknowledgments. We thank S\u00e9bastien Flury and\nFran\u00e7ois Bochatay for their guidance and feedback. Tiago\nCastanheiro, Flavien Bonvin and Livio Gamassia imple-\nmented the web-based Playground used to evaluate the\nmodel. Tiago Castanheiro, Flavien Bonvin, Sacha To-\nufani, Livio Gamassia, and Kasper Andkjaer tested out\nmultiple versions of the model. S\u00e9bastien V on Roth de-\nsigned the Cedille logo as well as the visual design of the\nPlayground and Cedille website8. Sonja Dossenbach as-\nsembled the dataset of recent French news. We are grateful\nto EleutherAI for publicly releasing the GPT-J model and\noffering us support on their Discord server9. We thank the\nTPU Research Cloud team for their access to Cloud TPUs\nand their support.\nReferences\n[1] Alec Radford et al. \u201cLanguage models are unsu-\npervised multitask learners\u201d. In: OpenAI blog 1.8\n(2019), p. 9.\n[2] Tom B Brown et al. \u201cLanguage models are few-\nshot learners\u201d. In: arXiv preprint arXiv:2005.14165\n(2020).\n[3] Jared Kaplan et al. \u201cScaling laws for neu-\nral language models\u201d. In: arXiv preprint\narXiv:2001.08361 (2020).\n[4] Chau Tran et al. \u201cFacebook AI WMT21 news\ntranslation task submission\u201d. In: arXiv preprint\narXiv:2108.03265 (2021).\n[5] Naveen Arivazhagan et al. \u201cMassively multilingual\nneural machine translation in the wild: Findings and\nchallenges\u201d. In: arXiv preprint arXiv:1907.05019\n(2019).\n7https://coteries.com\n8https://cedille.ai\n9https://discord.gg/zBGx3azzUn\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bbc9c4f-4cbc-45b5-92c2-d115c20a7b65": {"__data__": {"id_": "5bbc9c4f-4cbc-45b5-92c2-d115c20a7b65", "embedding": null, "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c450a3e-aa63-423d-a940-7237b75165ec", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "4bf596553fda0bd3af8edee8be95964d297d4d9093dab60cecb1f264a228be79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d36f84bd-d341-4972-8494-e520da286a0d", "node_type": "1", "metadata": {}, "hash": "033606ec7104f42770077dc30c4f6459129922956268002c99da3f298394b1ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] Antoine Simoulin and Benoit Crabb\u00e9. \u201cUn mod-\n\u00e8le Transformer G\u00e9n\u00e9ratif Pr\u00e9-entrain\u00e9 pour le _\nfran\u00e7ais\u201d. In: Traitement Automatique des Langues\nNaturelles. ATALA. 2021, pp. 245\u2013254.\n[7] Julien Launay et al. \u201cPAGnol: An Extra-Large\nFrench Generative Model\u201d. In: arXiv preprint\narXiv:2110.08554 (2021).\n[8] Guillaume Wenzek et al. \u201cCcnet: Extracting high\nquality monolingual datasets from web crawl data\u201d.\nIn: arXiv preprint arXiv:1911.00359 (2019).\n[9] Emily M Bender et al. \u201cOn the Dangers of Stochas-\ntic Parrots: Can Language Models Be Too Big?\u201d\nIn: Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency. 2021,\npp. 610\u2013623.\n[10] Isaac Caswell et al. \u201cQuality at a glance: An au-\ndit of web-crawled multilingual datasets\u201d. In: arXiv\npreprint arXiv:2103.12028 (2021).\n[11] Samuel Gehman et al. \u201cRealToxicityPrompts: Evalu-\nating neural toxic degeneration in language models\u201d.\nIn: arXiv preprint arXiv:2009.11462 (2020).\n[12] Johannes Welbl et al. \u201cChallenges in detox-\nifying language models\u201d. In: arXiv preprint\narXiv:2109.07445 (2021).\n[13] Sumanth Dathathri et al. \u201cPlug and play language\nmodels: A simple approach to controlled text gener-\nation\u201d. In: arXiv preprint arXiv:1912.02164 (2019).\n[14] Brian Lester, Rami Al-Rfou, and Noah Constant.\n\u201cThe power of scale for parameter-ef\ufb01cient prompt\ntuning\u201d. In: arXiv preprint arXiv:2104.08691\n(2021).\n[15] Nitish Shirish Keskar et al. \u201cCtrl: A conditional\ntransformer language model for controllable gener-\nation\u201d. In: arXiv preprint arXiv:1909.05858 (2019).\n[16] Ben Wang and Aran Komatsuzaki.GPT-J-6B: A 6\nBillion Parameter Autoregressive Language Model.\nhttps : / / github . com / kingoflolz / mesh -\ntransformer-jax. May 2021.\n[17] Jianlin Su et al. \u201cRoformer: Enhanced transformer\nwith rotary position embedding\u201d. In: arXiv preprint\narXiv:2104.09864 (2021).\n[18] Linting Xue et al. \u201cmT5: A massively multilin-\ngual pre-trained text-to-text transformer\u201d. In: arXiv\npreprint arXiv:2010.11934 (2020).\n[19] Laura Hanu and Unitary team. Detoxify. https :\n//github.com/unitaryai/detoxify. 2020.\n[20] Hang Le et al. \u201cFlaubert: Unsupervised language\nmodel pre-training for french\u201d. In: arXiv preprint\narXiv:1912.05372 (2019).\n[21] Robyn Speer. ftfy. Zenodo. Version 5.5. 2019. DOI :\n10.5281/zenodo.2591652. URL : https://doi.\norg/10.5281/zenodo.2591652.\n[22] Ben Wang. Mesh-Transformer-JAX: Model-Parallel\nImplementation of Transformer Language Model\nwith JAX. https://github.com/kingoflolz/\nmesh-transformer-jax. May 2021.\n[23] Leo Gao et al. A framework for few-shot language\nmodel evaluation. Version v0.0.1. Sept. 2021. DOI :\n10.5281/zenodo.5371628. URL : https://doi.\norg/10.5281/zenodo.5371628.\n[24] Leo Gao. On the Sizes of OpenAI API Models .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d36f84bd-d341-4972-8494-e520da286a0d": {"__data__": {"id_": "d36f84bd-d341-4972-8494-e520da286a0d", "embedding": null, "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c450a3e-aa63-423d-a940-7237b75165ec", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "4bf596553fda0bd3af8edee8be95964d297d4d9093dab60cecb1f264a228be79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bbc9c4f-4cbc-45b5-92c2-d115c20a7b65", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "29f782fe487fae09d78c68af621c798f708bf559ce634e77647449067fe26c07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "https://github.com/kingoflolz/\nmesh-transformer-jax. May 2021.\n[23] Leo Gao et al. A framework for few-shot language\nmodel evaluation. Version v0.0.1. Sept. 2021. DOI :\n10.5281/zenodo.5371628. URL : https://doi.\norg/10.5281/zenodo.5371628.\n[24] Leo Gao. On the Sizes of OpenAI API Models .\nhttps://blog.eleuther.ai/gpt3- model-\nsizes/. May 2021.\n[25] Stephen Merity et al. \u201cPointer sentinel mixture mod-\nels\u201d. In: arXiv preprint arXiv:1609.07843 (2016).\n[26] Perplexity of \ufb01xed-length models . https : / /\nhuggingface . co / docs / transformers /\nperplexity. Accessed: 2022-02-04.\n[27] Moussa Kamal Eddine, Antoine J-P Tixier, and\nMichalis Vazirgiannis. \u201cBARThez: a skilled pre-\ntrained french sequence-to-sequence model\u201d. In:\narXiv preprint arXiv:2010.12321 (2020).\n[28] Shashi Narayan, Shay B Cohen, and Mirella La-\npata. \u201cDon\u2019t give me the details, just the sum-\nmary! topic-aware convolutional neural networks\nfor extreme summarization\u201d. In: arXiv preprint\narXiv:1808.08745 (2018).\n[29] Chin-Yew Lin. \u201cRouge: A package for automatic\nevaluation of summaries\u201d. In: Text summarization\nbranches out. 2004, pp. 74\u201381.\n[30] Martin d\u2019Hoffschmidt et al. \u201cFQuAD: French\nquestion answering dataset\u201d. In: arXiv preprint\narXiv:2002.06071 (2020).\n[31] Pranav Rajpurkar et al. \u201cSQuAD: 100,000+ ques-\ntions for machine comprehension of text\u201d. In: arXiv\npreprint arXiv:1606.05250 (2016).\n[32] Louis Martin et al. \u201cCamemBERT: a tasty\nfrench language model\u201d. In: arXiv preprint\narXiv:1911.03894 (2019).\n[33] Ond\u02c7rej Bojar et al. \u201cFindings of the 2014 workshop\non statistical machine translation\u201d. In: Proceedings\nof the ninth workshop on statistical machine trans-\nlation. 2014, pp. 12\u201358.\n[34] Kishore Papineni et al. \u201cBleu: a method for auto-\nmatic evaluation of machine translation\u201d. In: Pro-\nceedings of the 40th annual meeting of the Associa-\ntion for Computational Linguistics. 2002, pp. 311\u2013\n318.\n[35] Matt Post. \u201cA Call for Clarity in Reporting BLEU\nScores\u201d. In: Proceedings of the Third Conference\non Machine Translation: Research Papers. Belgium,\nBrussels: Association for Computational Linguis-\ntics, Oct. 2018, pp. 186\u2013191. URL : https://www.\naclweb.org/anthology/W18-6319.\n[36] Xiaodong Liu et al. \u201cVery deep transformers for\nneural machine translation\u201d. In: arXiv preprint\narXiv:2008.07772 (2020).\n[37] Katherine Lee et al. \u201cDeduplicating training data\nmakes language models better\u201d. In: arXiv preprint\narXiv:2107.06499 (2021).\n[38] Long Ouyang et al. Training language models to\nfollow instructions with human feedback. https://\nopenai.com/blog/instruction-following/ .\nJan. 2022.\n7", "mimetype": "text/plain", "start_char_idx": 2443, "end_char_idx": 5026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea8279ec-6956-463b-8eca-e65f45f30e80": {"__data__": {"id_": "ea8279ec-6956-463b-8eca-e65f45f30e80", "embedding": null, "metadata": {"page_label": "8", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae00076f-441a-4c9b-a5e1-de6e5841889f", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b6e53b23e8021832035fe0e3e45f26ae3605ca4d913b66e8b5e960624e4e9d10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SUPPLEMENTARY MATERIAL\n1 Experiments training from scratch\nGiven the amount of compute and data available, training from scratch rather than \ufb01netuning was considered. We\nexperimented training Cedille from scratch using both the GPT-2 tokenizer (Cedille-fs-GPT2, vocab size50,400) and\nthe GPT-fr tokenizer (Cedille-fs-GPTfr, vocab size 50.000) for 60k steps using a peak learning rate of 1.2e-4 end\nlearning rate 1.2e-5, and 7281 warm-up steps. These two variants are therefore only trained on one third of the data\ncompared to the released Cedille model (150k steps). In order to have a fair comparison we show the result of Cedille\nafter the same amount of steps (Cedille-60k). All models were trained on the same \ufb01ltered mC4 dataset, as described in\nthis work.\nAs shown in Table S1, Cedille-60k outperforms the from-scratch variants on the WikiText-fr benchmark. However,\ndue to compute limitations we did not run the variants for longer than 60k steps and it is possible that we could\u2019ve\nreached similar performance after 150k steps. Furthermore, both variants perform similarly, even though they are using\na different tokenizer. Due to the variants performing very similarly, we conclude that even though a dedicated French\ntokenizer is a lot more ef\ufb01cient at encoding French text compared to the GPT-2 tokenizer, its bene\ufb01t with regard to\nend-task performance was minimal in our experiments.\nModel PPL (byte) PPL (token)\nGPT-J 1.746 5.797\nCedille-60k 1.673 4.112\nCedille-fs-GPT2 1.794 4.972\nCedille-fs-GPTfr 1.775 6.856\nTable S1:Byte-level and token-level perplexities for the WikiText-fr benchmark. Cedille-60k is the Cedille model at checkpoint 60k\n(out of 150k), Cedille-fs-GPT2 and Cedille-fs-GPTfr are models trained for 60k steps on the same dataset, but with random weight\ninitialization.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d4f40c4-bf31-4496-986d-aa1649cbf507": {"__data__": {"id_": "4d4f40c4-bf31-4496-986d-aa1649cbf507", "embedding": null, "metadata": {"page_label": "1", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3864b547-5f35-47c3-bd66-9140804d6dd3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "8947908db0788c9277d62ebd83d38ee8ee505e1a5a94034986e4b7191e370053", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nMay 2023\nA report from\nGabriel Nicholas \nAliya Bhatia\nLarge Language Models in \nNon-English Content Analysis", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7abb6a27-e84b-411a-ac37-d3b2a0feb79d": {"__data__": {"id_": "7abb6a27-e84b-411a-ac37-d3b2a0feb79d", "embedding": null, "metadata": {"page_label": "2", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b8c8275-cb6c-426b-b691-7d7c3b606aff", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d59ba9d586a41daf51206a4c2c6b08f783996729f53df9d6b56951f0833dba56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GABRIEL NICHOLAS\nResearch Fellow at the Center for Democracy & T echnology.\nALIYA BHATIA\nPolicy Analyst, Free Expression Project at the Center for \nDemocracy & T echnology.\nThe Center for Democracy & T echnology (CDT) is the leading \nnonpartisan, nonprofit organization fighting to advance civil rights and \ncivil liberties in the digital age. W e shape technology policy, governance, \nand design with a focus on equity and democratic values. Established in \n1996, CDT has been a trusted advocate for digital rights since the earliest \ndays of the internet. The organization is headquartered in W ashington, \nD.C., and has a Europe Office in Brussels, Belgium.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9eb6d914-1d45-4049-b338-557573b25ef9": {"__data__": {"id_": "9eb6d914-1d45-4049-b338-557573b25ef9", "embedding": null, "metadata": {"page_label": "3", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73733ab9-eb31-41c1-9e8a-ec7a760ba395", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "79573ce4bb01a97d5885ef369f25122e88093a663105181b1004cfbb89facdec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A report from\nGabriel Nicholas and Aliya Bhatia\nWITH CONTRIBUTIONS BY\nSamir Jain, Mallory Knodel, Emma Llans\u00f3, Michal Luria, Nathalie Mar\u00e9chal, Dhanaraj Thakur, and \nCaitlin V ogus.\nACKNOWLEDGMENTS \nW e thank Pratik Joshi, Sebastin Santy, and Aniket Kesari for their invaluable feedback on the technical \naspects of this report. W e also thank Jacqueline Rowe, Damini Satija, and \u00c1ngel D\u00edaz for their \ninsightful comments and suggestions. The translation of our executive summary is made possible by \nGlobal V oices T ranslations and with the help of Iverna McGowan, Maria Villamar, Oph\u00e9lie Stockhem, \nand T om\u00e1s Pomar. All views in this report are those of CDT. \nThis work is made possible through a grant from the John S. and James L. Knight Foundation.\nSuggested Citation: Nicholas, G. and Bhatia, A. (2023) Lost in T ranslation: Large Language Models \nin Non-English Content Analysis. Center for Democracy & T echnology. https://cdt.org/insights/lost-\nin-translation-large-language-models-in-non-english-content-analysis/\nReferences in this report include original links and links archived and shortened by the Perma.cc service. \nThe Perma.cc links also contain information on the date of retrieval and archive. \nThis report is licensed under a Creative Commons Attribution 4.0 International License.\nLost in Translation\nLarge Language Models in Non-\nEnglish Content Analysis", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e74bb63-bcd4-4b31-9caf-7e39e0556847": {"__data__": {"id_": "1e74bb63-bcd4-4b31-9caf-7e39e0556847", "embedding": null, "metadata": {"page_label": "4", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aef93de9-8bc6-4d31-8d67-7ab14ba49674", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "17629397d00a740fa02d4aa0276b1e1b7f6ab46f4f320759bc5c0619894783e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n4\nCDT Research\n4\nContents\nExecutive Summary 5\nIntroduction 8\nI. Background 12\nA.\t How\tLarge\tL anguage\tM odels\tW ork\t 12\nB.\t The\tR esourcedness \tG ap:\tW hy\tt he\tL argest\tL anguage\t\t\nModels\tare\tin\tE nglish\t 15\nC.\t Multilingual\tLanguage\tM odels:\tE fforts\tt o\tB ridge\tt he\t\t\nResourcedness\tGap\t 19\nII. Limitations of Language Models in English and  \nNon-English Contexts 23\nA.\t Concerns\twith\tBuilding\tand\tDeploying\tLarge\t\t\nLanguage\tM odels\t 23\nB.\t Limitations\tof\tM ultilingual\tL anguage\tM odels\t 25\nIII. Recommendations 31\nA.\t Companies\t 31\nB.\t Researchers\tand\tFunders\t 33\nC.\t Governments\t 36\nWorks Cited 39", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17a668bd-752d-48ab-a8fa-8973d6de5e8e": {"__data__": {"id_": "17a668bd-752d-48ab-a8fa-8973d6de5e8e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aca900af-3be4-4d8a-afd6-e68003af2128", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "1cef3e2ebf950ec32794547c12114b272688e58f2e92661f66ada3a44c01d750", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\nLost in Translation\nExecutive \nSummary\n T\nhe internet is the primary source of information, economic \nopportunity, and community for many around the world. \nHowever, the automated systems that increasingly mediate our \ninteractions online \u2014 such as chatbots, content moderation \nsystems, and search engines \u2014 are primarily designed for and work far \nmore effectively in English than in the world\u2019s other 7,000 languages.\nIn recent years, large language models have become the dominant \napproach for building AI systems to analyze and generate language \nonline, but again, they have been built primarily for the English \nlanguage. A large language model (e.g., Open AI\u2019s GPT-4, Meta\u2019s \nLLaMa, Google\u2019s PaLM) is a machine learning algorithm that scans \nenormous volumes of text to learn which words and sentences \nfrequently appear near one another and in what context. Large language \nmodels can be adapted to perform a wide range of tasks across different \ndomains. They are most known for being used to build chatbots, \nsuch as ChatGPT, but researchers and technology companies also \nuse them for content analysis tasks, such as sentiment analysis, text \nsummarization, and hate speech detection. Google, Meta, Microsoft, \nand other companies have already incorporated large language models \ninto their core product functions, such as content moderation and \nsearch. Other vendors soon may incorporate them into automated \ndecision-making systems, such as resume scanners.\nRecently though, researchers and technology companies have attempted \nto extend the capabilities of large language models into languages other \nthan English by building what are called multilingual language models. \nInstead of being trained on text from only one language, multilingual \nlanguage models are trained on text from dozens or hundreds of \nlanguages at once. Researchers posit that multilingual language models \ninfer connections between languages, allowing them to apply word \nassociations and underlying grammatical rules learned from languages \nwith more text data available to train on (in particular English) to \nthose with less. In some applications, multilingual language models \noutperform models trained on only one language \u2014 for instance, a \nmodel trained on lots of text from lots of languages, including Hindi, \nmight perform better in Hindi contexts than a model just trained on \nHindi text.\nMultilingual language models give technology companies a way to scale \ntheir AI systems to many languages at once, and some have already \nbegun to integrate them into their products. Online service providers \nin particular have deployed multilingual language models to moderate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e12e51ca-83b9-4596-aca8-38344ae5111f": {"__data__": {"id_": "e12e51ca-83b9-4596-aca8-38344ae5111f", "embedding": null, "metadata": {"page_label": "6", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b99a776c-cc1f-4e87-8916-3ac9330f9d24", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "66bfdcc173c89fd6ed1c41c2fe3f67125a5f108f79b1c5e342ea863fd67f8578", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n6\ncontent: Meta uses a multilingual language model to detect harmful content on its \nplatforms in over 100 languages; Alphabet\u2019s Perspective API uses one to detect toxic \ncontent in eighteen different languages; Bumble uses one to detect and take action on \nunwanted sexual messages around the world.\nMultilingual language models allow technologists to attempt to build models in languages \nf\nor which they otherwise might not have enough digitized text. Languages vary widely \nin resourcedness, or the volume, quality, and diversity of text data they have available to \ntrain language models on. English is the highest resourced language by multiple orders of \nmagnitude, but Spanish, Chinese, German, and a handful of other languages are sufficiently \nhigh resource enough to build language models in. Medium resource languages, with fewer \nbut still high-quality data sets, such as Russian, Hebrew, and Vietnamese, and low resource \nlanguages, with almost no training data sets, such as Amharic, Cherokee, and Haitian \nCreole, have too little text for training their own large language models. Language data in \nlow resource languages is also often of particularly poor quality: either it is mistranslated or \neven nonsensical language scraped from the internet, or is limited to sources with narrow \ndomains, such as religious texts and Wikipedia. This gap in data availability between \nlanguages is known as the resourcedness gap.\nMultilingual language models are designed to address these gaps in data availability by \ninferring semantic and grammatical connections between higher- and lower-resource \nlanguages, allowing the former to bootstrap the latter. However, this architecture \nraises its own concerns. Multilingual language models are still usually trained \ndisproportionately on English language text and thus end up transferring values and \nassumptions encoded in English into other language contexts where they may not \nbelong. For example, a multilingual model might associate the word \u201cdove\u201d in all \nlanguages with \u201cpeace\u201d even though the Basque word for dove (\u201cuso\u201d) can be an insult. \nThe disparity in available data also means multilingual language models work far better \nin higher resource languages and languages similar to them than lower resource ones. \nModel developers will sometimes try to fill in these gaps with machine-translated text, \nbut translation errors may further compound language misrepresentation. And when \nmultilingual language models do fail, their unintuitive connections between languages \ncan make those problems harder to identify, diagnose, and fix.\nLarge language models\u2019 general use in content analysis raises further concerns. \nComputational linguists argue that large language models are limited in their capacity \nto analyze forms of expression not included in their training data, meaning they may \nstruggle to perform in new contexts. They may also reproduce any biases present in \ntheir training data. Often, this text is scraped from the internet, meaning that large \nlanguage models may encode and reinforce dominant views expressed online.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f218c722-6185-4fbc-b1c4-fc5c944eee5b": {"__data__": {"id_": "f218c722-6185-4fbc-b1c4-fc5c944eee5b", "embedding": null, "metadata": {"page_label": "7", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a2addc3-6c7b-4685-b0cb-6a31df257332", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7e03f63aea45776e6bbdd533a4d0f3cb3221d2706475decf1e6498b843b70848", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n7\n  Companies, researchers, and governments each have a role to play in protecting the \npublic from the potential dangers of multilingual language model content analysis \nsystems. T o ensure better public accountability, companies that deploy large language \nmodels should always be transparent about how they use them and in which languages. \nCompanies should deploy language models with narrow remits and adequate channels \nfor human review.\nResearchers and research funders meanwhile should invest in efforts to improve the \nuse and performance of language models in languages other than English, in particular, \nto reduce failures that disparately impact speakers of lower-resourced languages. The \nbest way to do this is by supporting language-specific research communities, who can \npromote the virtuous cycle of collecting data, curating datasets, training language \nmodels, publishing, and building applications. Local language speakers and context \nexperts need to be part of each step of this process and also be curating the data and \nassessing the language models deployed by large, global online services.\nFinally, governments need to be careful about how they use or encourage the use of \nlarge language models. Large language models should never power systems used to make \nhigh-stakes decisions without oversight, such as decisions about immigration status or \nhealthcare, nor should governments mandate or inadvertently require by law the use of \nlarge language model-powered systems to moderate content from online services. Instead, \ngovernments should convene different stakeholders to align on what norms and guardrails \nshould be around developing and deploying large language models.\nLarge language models in general and multilingual language models in particular \nhave the potential to create new economic opportunities and improve the web for \nall. However, mis- or over-application of these technologies poses real threats to \nindividuals\u2019 rights, such as undermining their right to free expression by inaccurately \ntaking down a person\u2019s post on social media or their right to be free of discrimination \nby misinterpreting an individual\u2019s job or visa application. Multilingual language \nmodels specifically can inadvertently further entrench the Anglocentrism they are \nintended to address. In light of these limitations, technology companies, researchers, \nand governments must consider potential human and civil rights risks when studying, \nprocuring, developing, or using multilingual language models to power systems, in \nparticular when they are used to make critical information available or play a role \nin decisions affecting people\u2019s access to economic opportunities, liberty, or other \nimportant interests or rights.\nExecutive Summary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f5521f12-f02b-4334-9b01-f472d8ddae7e": {"__data__": {"id_": "f5521f12-f02b-4334-9b01-f472d8ddae7e", "embedding": null, "metadata": {"page_label": "8", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3d27641-f4da-41a8-b37a-671561b9682c", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "2928b966f7e0360c68ecfd66245903e49c6269b2aa8bfe381702545674f3f393", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n8\nIntroduction\nD\nespite the modern internet\u2019s power to mobilize and connect \npeople around the world, the web still does not reflect the \nlinguistic diversity of its users. In particular, the automated \nsystems that increasingly mediate our interactions online \u2014 \nsuch as chatbots, search engines, and content moderation systems \u2014 \nare built using and perform far better on English-language text than \nthe world\u2019s other 7,000 languages (Kornai, 2013; Sengupta, 2022). \nIndividuals speaking languages other than English face barriers to \nexpressing themselves freely online and may face greater challenges \nwhen it comes to accessing critical information, public services, and \neven asylum and safety (T orbati, 2019).\nIn the last few years, however, there have been rapid advancements in \ndeveloping machine learning tools that can analyze content in a wide \nvariety of languages and across different domains. Large language \nmodels, machine learning tools trained on enormous amounts of text \nto recognize patterns in language, power many of these systems. Large \nlanguage models already underlie translation apps, search autocomplete, \nand chatbots such as ChatGPT. They are known for being adaptable to \nmany different language tasks, and today, researchers and technologists \nare constantly on the lookout for new applications and contexts \nin which to deploy them. Since the late 2010s, major U.S.-based \ntechnology companies have mostly invested in building large language \nmodels that work primarily for English, such as Open AI\u2019s GPT-4, \nMeta\u2019s LLaMa, and Google\u2019s PaLM.\nRecently, companies and researchers have begun building and researching \nmultilingual language models, large language models trained on text \ndata from several different languages at once. Meta\u2019s XLM-RoBER T a \n(XLM-R) for instance is trained on text from 100 languages (Meta AI, \n2019) at once. Google\u2019s mBER T, a multilingual version of its popular \nBER T model, is trained on 104 languages. Researchers claim that these \nmodels extend the multifaceted capabilities of large language models to \nlanguages other than English, even to languages for which there is little or \nno text data for the model to learn from (Artetxe & Schwenk, 2019; Wu \n& Dredze, 2019).\nT echnology companies have their own interests in improving how \nwell large language models work in different languages. Some may \nwant to make their products available in multiple languages to gain a \ncompetitive edge in emerging and populous markets. Online services", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a82f34de-5905-47d8-8fb6-6d3b8b023df2": {"__data__": {"id_": "a82f34de-5905-47d8-8fb6-6d3b8b023df2", "embedding": null, "metadata": {"page_label": "9", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51e8b4de-84b5-4c2c-959f-2a25de83158d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "cee4b4bd3fde243cad7c561dc1a818f182feb4c623130524067b1f23297a56fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n9\nthat host user-generated content may especially be interested in using multilingual \nlanguage models to detect and take action on hate speech, disinformation, and other \ncontent that violates their policies or the law (Dulhanty et al., 2019). This is top of \nmind for services after facing criticism for not taking more aggressive action against \ncontent that incited violence and genocide in Ethiopia, Nigeria, and Myanmar, among \nothers. Services have begun to deploy multilingual language models into their content \nmoderation systems: Meta claims their XLM-R model can detect harmful content \nin all 100 languages it is trained on (Meta AI, 2021); Alphabet\u2019s Perspective API uses \na large language model to detect toxic content in eighteen different languages (Lees \net al., 2022); Bumble uses one to detect rude and abusive messages in at least fifteen \nlanguages (Belloni, 2021). T echnology companies are also repurposing these models to \nmake health care information available and soon may reach into other domains as well \n(Lunden, 2023).\nIn the future, governments could also seek to use automated systems built using \nlarge language models to make information available, answer questions in languages \nspoken by their constituents (in the form of chatbots), or, more dangerously, analyze \ninformation to make critical decisions such as benefits allocation or refugee status \ndeterminations (Kinchin & Mougouei, 2022).\nStill, studies show that even multilingual language models struggle to deal with the \nwide disparities between different languages in how much text data they have available \nto train and test language models. English has, by multiple orders of magnitude, more \ntext data available than any other language and commands most of the attention of the \nnatural language processing research community. The abundance of English language \ndata stems from its position as the official or de facto language of international business, \npolitics, and media, itself a legacy of British colonialism and American neocolonialism \nand the subsequent erasure of regional and indigenous languages. American technology \ncompanies have further entrenched English as the predominant language of the internet \nby rolling out early standards, coding languages, and social media platforms in English \nlong before other languages.\nThe hegemony of English data means that most large language models, even \nmultilingual ones, are built predominantly using Standard English language text and \nwork best in Standard English language contexts. Spanish, Chinese, Arabic, and a few \nother \u201chigh resource\u201d languages also have significant amounts of text data available, but \nmany \u201cmedium resource\u201d languages, such as Hindi and Portuguese, and \u201clow resource\u201d \nlanguages, such as Haitian Creole and Swahili, have hardly any data available at all, and \nmultilingual language models perform much worse in those languages. This skewed \nemphasis fails to reflect the diversity of languages spoken by the world\u2019s internet users \nand further perpetuates the dominance of the English language.\nIntroduction", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e340042d-bcb0-4d71-ab9d-0d1d2c9a0bca": {"__data__": {"id_": "e340042d-bcb0-4d71-ab9d-0d1d2c9a0bca", "embedding": null, "metadata": {"page_label": "10", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bff5288-dec1-4e62-a90a-cb3d551b2957", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c17b02c660cff79bdbf6abc9ef6fed6f6bbfe39a6aade5e3840fcb3f1543177d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n10\nDespite being deployed in real-world systems, multilingual language models have largely \nbeen absent from public discourse, particularly about digital rights and public policy, \nand have instead been relegated to computer science academia and tech company public \nrelations. This paper seeks to address this gap by offering several resources to bolster \npolicy discussions. Part I provides a simple technical explanation of how large language \nmodels work in general, why there is a gap in available data between English and other \nlanguages, and how multilingual language models attempt to bridge that gap. Part II \naccounts for the challenges of doing content analysis with large language models in \ngeneral and multilingual language models in particular, namely:\n1. Multilingual language models often rely on machine-translated text that can \ncontain errors or terms native language speakers don\u2019t actually use. \n2. When multilingual language models fail, their problems are hard to identify, \ndiagnose, and fix.\n3. Multilingual language models do not and cannot work equally well in all languages.\n4. Multilingual language models fail to account for the contexts of local language \nspeakers.\nFinally, Part III provides recommendations for companies, researchers, and \npolicymakers to keep in mind when considering studying, developing, and deploying \nlarge and multilingual language models to do content analysis. These recommendations \noffer guidance concerning when large language models should or should not be \ndeployed, how to improve their performance in non-English languages, and how to \nensure better accountability and transparency to local language stakeholders.\nBefore proceeding, two notes on the terminology used in this primer. First, this paper \nfocuses specifically on one category of applications for large language models: content \nanalysis, or, the inference and extraction of information, themes, and concepts from \ntext. The Center for Democracy & T echnology (CDT) has written many times about \nthe limitations of automated content analysis systems (Duarte et al., 2017; Shenkman \net al., 2021) and the civil liberty risks they can pose, particularly in areas such as content \nmoderation, student activity monitoring, hiring and more (Grant-Chapman et al., \n2021; Nicholas, 2022; V allee & Duarte, 2019). Large language models are already deeply \nintegrated into many of these technical systems, particularly content moderation, and will \nsoon become part of many more. Public discourse about large language models has so far \ndisproportionately focused on text generation, an important area but not the only one. \nMany of the shortcomings of large language models presented in this report also apply \nto text generation. As such, this report can be read as a primer on some of the limits of \ngenerative AI systems as well. However, we choose to focus on content analysis for this \nreport because of the potential dangers associated with using these models to host and \nmake information available and the impacts on free expression rights.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed321e9b-0c17-4887-8cbc-08e80694c702": {"__data__": {"id_": "ed321e9b-0c17-4887-8cbc-08e80694c702", "embedding": null, "metadata": {"page_label": "11", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3cc3918-a8b2-48ac-91e1-52a8f7e11e59", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "ba2a822b7983d2e9ed5b6dc15f7bf00087e3a13a42476fb67e0acf9be6aaad39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n11\nSecond, this paper focuses on how multilingual language models perform in languages \nother than English. W e use the shorthand of \u201cnon-English languages\u201d for easy reading \nand because it is the terminology used in the machine learning and policy literature. \nW e recognize the irony that this term centers the English language and misleadingly \nimplies all other languages are a monolith. Where possible, we elaborate upon the types \nof languages we are writing about and make distinct references to specific languages and \ncultural contexts that will elude models trained primarily in English. In some instances, \nwe think the term \u201cnon-English\u201d captures the sheer Anglocentrism of many of these \nmodels well by articulating the limited scope in which they are trained and tested.\nIntroduction", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09ceeb29-1dbe-488b-bdc3-f1056a023a6b": {"__data__": {"id_": "09ceeb29-1dbe-488b-bdc3-f1056a023a6b", "embedding": null, "metadata": {"page_label": "12", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dfca318-5a22-4272-bc12-332d28ce54c9", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "735803bfbfa4bb6045982135c183af94ab9944aafea5921a996e5e218c81a92c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n12\nI. Background A. How Large Language Models Work\nNatural language processing (NLP) is a subfield of artificial intelligence \nand linguistics concerned with building computer systems that can \nprocess and analyze language. NLP underlies many technologies we \nencounter every day \u2014 spellcheck, voice assistants like Siri or Alexa, \nresume scanners, language translators, and automated hate speech \ndetection tools, to name a few. Until only a few years ago, when \ntechnologists wanted to teach a computer to perform a given NLP task, \nthey would build a system specifically tailored to that task. T o create a \nspam detection system for instance, a technologist might gather many \nemails, mark which ones are and are not spam, use some of those emails to \ntrain an algorithm and use others to test how well that algorithm works.\nT oday though, the field has fundamentally reoriented itself around \nrepurposing large language models to solve nearly every problem. \nA language model is a mathematical function trained to solve a text \nprediction task like the following, \u201cGiven a sequence of words, predict \nwhat word will likely come next.\u201d For example, a language model might be \ngiven the phrase \u201cI was a bad student, I used to skip ____,\u201d and generate \nas an output that there is a high percent chance the missing word is \n\u201cclass,\u201d a low percent it is \u201crope,\u201d and a near zero percent it is \u201cclamoring.\u201d\nThe distribution of language that the model learns in the process can \neasily be repurposed to many different language tasks. The most often \ndiscussed application is text generation: conversational agents like \nChatGPT can repurpose this text prediction task to answer questions, \nsummarize text, and generate overall \u201chuman\u201d-sounding speech. \nHowever, chatbots are just one application of large language models. \nOnce a large language model is built, it can be further trained on a \nsmaller dataset to improve its performance in a specific task, a process \ncalled fine-tuning. T oday, for example, a developer building a spam \ndetection system might take a general large language model already \nbuilt by someone else \u2014 say Google\u2019s BER T \u2014 and fine-tune it to the \nspecific task of spam detection using a handful of emails already labeled \nspam or not spam. By building it on top of a language model, the spam \ndetection system will do a better job of detecting spam that doesn\u2019t \nperfectly match the language available in the email dataset.\nLanguage models are not new. Computational linguists have used \nstatistical models to try to infer rules about language since the 1980s \n(Nadkarni et al., 2011) and have used \u201cneural networks\u201d (an algorithm \nloosely modeled on how neurons connect in the brain) to do so since \nthe early 2010s (Mikolov et al., 2013). What is new though is their", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62a1f5dc-7e95-4c1f-9507-c55080f8a1e6": {"__data__": {"id_": "62a1f5dc-7e95-4c1f-9507-c55080f8a1e6", "embedding": null, "metadata": {"page_label": "13", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce7c14e5-0e9d-46d3-8773-1260cb331065", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "4845cad0189d1d50652c8c193b4560a47e4ec11f13e449c13aa72f6ce613106c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n13\nlargeness. Early language models could not be trained on as much data, since they had \nto read text in sequence, a process that could not be sped up by using more computing \npower. These early language models struggled to analyze words within the broader \ncontext of a sentence or document: for instance, one fine-tuned to detect suicidal \nideation might have difficulty distinguishing between expressions of self-harm (\u201cI \njust wish I was dead\u201d) and humor (\u201comg I\u2019m dead\u201d). But in 2017, Google researchers \nreleased a paper on a new architecture called transformers, which allowed language \nmodels to train on lots of data at the same time, in parallel rather than in sequence \n(V aswani et al., 2017). These transformer-based language models could ingest so much \ndata simultaneously that they could learn associations between entire sequences of \nwords, not just individual words. Instead of being shown just {\u201cdead\u201d}, the model \nwould see a word in its entire context, {\u201cdead\u201d, [\u201comg\u201d, \u201cI\u2019m\u201d, \u201c_____\u201d]}, thus creating \na much richer representation of language. T oday, the only limit on the size of a language \nmodel \u2014 how much data it ingests and how many connections it makes between \ndifferent sequences of words (i.e. parameters) \u2014 is how much data one can find and \nhow much developers are willing to spend on processing power.\nThe output a language model produces is called a representation space, a map of the \nsequences of words that commonly appear near one another in the training text. For \nexample, the phrases \u201cIt\u2019s so cold outside!\u201d and \u201cI better wear a jacket\u201d may be near one \nanother in a language model\u2019s representation space, since those sentences often appear \nclose to one another in writing. This kind of proximity can lead to language models \ninferring patterns within language that can then help them conduct tasks that it is not \nexplicitly trained in. In this case, sentences about cold weather being mapped near each \nother mean the large language model could be trained to detect whether a given phrase \nis about temperature.\nWith enough data, a large language model may have such a rich and multifaceted \nrepresentation of a language that it can learn to do new tasks with only a few, or even \nzero examples to fine-tune on. For instance, the spam detection system described earlier \ncould be built with little to no spam to fine-tune on. This capability is called \u201cfew-shot\u201d \nor \u201czero-shot learning\u201d and is one of the greatest promises of large language models, so \nmuch so that the original GPT-3 white paper is entitled \u201cLanguage Models are Few-\nShot Learners\u201d (Brown et al., 2020).\nImportantly though, large language models only learn the distribution of language, not \nits meaning (Bender & Koller, 2020). In the previous \u201ccold\u201d example, the model has not \nlearned that when one is cold, one puts on a jacket or anything about the deeper meanings \nof \u201ccold\u201d and \u201cjacket,\u201d only that the words often appear near one another. If one of the \ndocuments a large language model trains on is a humorous blogpost about the best shorts \nto wear in cold temperatures, the model could just as easily learn that \u201cshorts\u201d and \u201ccold\u201d \nare related. Similarly, if a model is trained only on very formal language data, it may never \nlearn that \u201cnippy\u201d or \u201cbrick\u201d (New Y ork City slang) can refer to cold as well.\nI. Background", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42c32953-702a-4fad-8b80-3a2b86038006": {"__data__": {"id_": "42c32953-702a-4fad-8b80-3a2b86038006", "embedding": null, "metadata": {"page_label": "14", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2aacb4cf-4b39-49eb-ac3e-eb423aebc7d9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "10191ac74dc623066650d47f26c88a975fbb317b798b8a0787446a75fa9f88fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation14\nTechnologists often try to address these shortcomings by training language models \non more and more data. If a model is exposed to more data, the idea is that it will be \nfamiliar with more contexts, and outliers like the ironic cold-weather shorts blogpost \nwill be outweighed by more representative data. This has led to ballooning in the size \nof large language models. BER T, a popular open-source model built by Google in \n2018, was trained on 800 million words from free books and 2.5 billion words from \nEnglish Wikipedia (Devlin et al., 2019). T wo years later, OpenAI released its closed \nsource GPT-3, which was trained on half a trillion mostly-English words crawled from \nthe internet (Brown et al., 2020). Google\u2019s PaLM, released in 2022, trained on 780 \nbillion words, mostly from English-language websites and social media conversations \n(Chowdhery et al., 2022). As models have grown in size, so have the computation costs \nof training them. While BER T costs a few thousand dollars in computing power to \ntrain from scratch and is often trained by academics to build new topic- or language-\nspecific models (Izsak et al., 2021), GPT-3 and PaLM-sized models cost millions or \ntens of millions of dollars to train (Sharir et al., 2020). Future models will only be \nmore expensive, leaving only the most well-off companies able to afford to build them \n(Bommasani et al., 2021).\nFigure 1. Language model \nrepresentation space. A langauge model\u2019s \nreprsentation space, collapsed into two \ndimensions. In reality, these models often \nhave thousands or tens of thousands of \ndimensions.\nSource: (Amer, 2022)\nLost in Translation14\nWhen is \nBoxing Day?\nWhat is the date \nof Boxing Day?\nHow many species \nof sharks are there?\nHow many species of the \nGreat White shark are there?\nIt\u2019s so cold \noutside!\nI better wear \na jacket.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e024b56-1659-437a-9700-11cd9b23ff28": {"__data__": {"id_": "1e024b56-1659-437a-9700-11cd9b23ff28", "embedding": null, "metadata": {"page_label": "15", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a808c423-24ef-4e78-978c-3c8102cb821b", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c42da93f18054224e6f35a20c052c94213f81ce6de648f9cac89e2871b5cc35a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n15\nModels are expensive to initially train, but once built, their representations are relatively \ncheap to use and be fine-tuned for different tasks. Thus, many technologists simply \nuse pretrained large language models built by others (usually large companies, with the \nexpertise and resources) instead of paying to create their own. The few big pretrained \nmodels that exist have thus become a sort of infrastructure, known as \u201cfoundation \nmodels\u201d (Bommasani et al., 2021). This gives many technologists access to the state of \nthe art capabilities, but it also creates a single point of failure for the sector as a whole: \nif a foundation model has a problem, it will persist across many applications. And these \nmodels are so large and complicated that even when they are open source, researchers \ncannot understand the underlying logic they use to come up with individual decisions.\nMany of the largest and most advanced of these foundation models \u2014 such as \nOpenAI\u2019s GPT-4, Google\u2019s PaLM, and Meta\u2019s LLaMa \u2014 are trained primarily on \nEnglish language data. In the next section, we explore one reason why that may be: the \nresourcedness gap.\nB. The Resourcedness Gap: Why the Largest \nLanguage Models are in English\nEnglish is the closest thing there is to a global lingua franca. It is the dominant language \nin science, popular culture, higher education, international politics, and global \ncapitalism; it has the most total speakers and the third-most first-language speakers \n(Ethnologue, 2023b). It is the primary language spoken on the internet, accounting \nfor 63.7% of websites, despite being spoken by only 16% of the world\u2019s population \n(Richter, n.d.). This dominance does not stem from any sort of inherent linguistic \nsuperiority: rather it is the colonial and neocolonial legacy of nearly three hundred \nyears of the preeminent global superpower speaking English \u2014 first Great Britain, \nthen the United States. The British government prioritized the English language \nthrough official language policies to facilitate trade and in an attempt to \u201cmodernize\u201d \nits colonies, and as British, and later American trade became globally dominant, so too \ndid English (Corradi, 2017; Phillipson, 1992). Prioritization of the English language \ncame at the expense of other regional and indigenous languages and accelerated \nlanguage endangerment and economic marginalization, which still impedes digital \ninvestment into these languages worldwide (Rowe, 2022; S. Zhang et al., 2022). \nAmerican companies continue to perpetuate the dominance of the English language in \na new more insidious form, by making online services available to global users without \ncomparable investment into the languages they speak (Amrute et al., 2022; Kupfer & \nMuyumba, 2022).\nI. Background", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb6b518b-90ad-45c7-9e3a-0e4998f99022": {"__data__": {"id_": "eb6b518b-90ad-45c7-9e3a-0e4998f99022", "embedding": null, "metadata": {"page_label": "16", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9e72ea7-21ee-4a1c-a710-a1b9ddfb7674", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "70412f86829cd80bc1394e6542089653ca36eeed049addd8c2aa4ce76d4a0556", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n16\nAs a result of these forces, English also dominates the field of natural language \nprocessing, and there is vastly more raw text data available in English than in any other \nlanguage by orders of magnitude (Joshi et al., 2020).\u00a0English has the most digitized \nbooks and patents, the largest Wikipedia, and the biggest internet presence. English is \nalso by far the language paid the most attention by the global NLP research community. \nIt is so hegemonic within the field that NLP papers about the English language \ntypically do not even mention the language in the title or abstract (Bender, 2019). As \nFigure 2 shows, even among NLP papers that do mention a language in the abstract, \nEnglish is mentioned over ten times as often as the next most mentioned language, \nGerman (ACL Rolling Review Dashboard, 2022).\nThis wealth of data and research makes it significantly easier to build large language \nmodels in English than in any other language. More raw text data, also known as \nunlabeled data, means more data for the model to be trained on; more research means \nthat there are more datasets annotated with information, also known as labeled data, \nthat can be used to test how well models complete different types of language tasks. \nThis creates a virtuous cycle for English-language NLP \u2014\u00a0more labeled and unlabeled \ndata leads to more research attention, which leads to increased demand for labeled and \nunlabeled data, and so on.\nEnglish is the prime example of a high resource language, a language for which a lot of \nhigh-quality data resources exist. Though it has the most data available of any language \n(English could be called an \u201cextremely\u201d high resource language), there are six other \nlanguages that could be considered high resource \u2014 the official UN languages list, \nminus Russian, plus Japanese (see T able 1). There are also a few dozen medium resource \nlanguages, such as Urdu, Italian, and T agalog, with another one or two orders of \nmagnitude less data, or about one hundredth or one-thousandth of available English data. \nThe rest of the world\u2019s 6,000 plus languages can be considered low resource or extremely \nlow resource, with only small amounts of written text available (Joshi et al., 2020).\nResourcedness can vary within languages as well. Languages such as Arabic and Spanish \ndiffer so much between dialects that many are mutually incomprehensible, even if \nthey mostly use the same written form. Languages can also have different sociolects, \nvarying across different social groups, identity groups, and contexts (e.g. formal versus \ninformal). Regional dialects and sociolects can vary in degrees of difference from \nhaving different vocabulary and grammatical structures (e.g. Australian English or \nAfrican American English versus Standard American English) to make extensive use of \nborrowed words from other languages (e.g. Nigerian English, Indian English), to fully \nhybrid bilingual dialects (e.g. Spanglish, Hinglish). However, the available digitized \ntext of language often doesn\u2019t reflect the full spectrum of variation that exists within a \nlanguage. (Bergman & Diab, 2022). Data scraped from the internet in particular over-\nindexes Standard English spoken by younger people in developed countries (Luccioni \n& Viviano, 2021). Other languages have just as much dialectical diversity as English and \nalso likely over-index on certain dialects.\nFigure 2. Languages mentioned in \npaper abstracts. T op most mentioned \nlanguages in abstracts of papers published \nby the Association for Computational \nLinguistics, May 2022-January 2023.\nSource: (Santy et al., 2023)\nPaper Abstracts", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1f2fedc-c5fb-4b60-8a69-edf0ce4ba64a": {"__data__": {"id_": "e1f2fedc-c5fb-4b60-8a69-edf0ce4ba64a", "embedding": null, "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed344fa9-1b7b-4c6c-b38f-dad78e4591c8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "34508ff23959cc4f3d4f12a1e69987396c67e894cc019df5040f334456021c73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5be40d05-4b33-4e39-a03e-b19688f5d676", "node_type": "1", "metadata": {}, "hash": "0f8ec55ee9bbbff0c4e6e9b4e6c92da53d7050ef001201f48e470948036b082e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n17\nAs a result of these forces, English also dominates the field of natural language \nprocessing, and there is vastly more raw text data available in English than in any other \nlanguage by orders of magnitude (Joshi et al., 2020).\u00a0English has the most digitized \nbooks and patents, the largest Wikipedia, and the biggest internet presence. English is \nalso by far the language paid the most attention by the global NLP research community. \nIt is so hegemonic within the field that NLP papers about the English language \ntypically do not even mention the language in the title or abstract (Bender, 2019). As \nFigure 2 shows, even among NLP papers that do mention a language in the abstract, \nEnglish is mentioned over ten times as often as the next most mentioned language, \nGerman (ACL Rolling Review Dashboard, 2022).\nThis wealth of data and research makes it significantly easier to build large language \nmodels in English than in any other language. More raw text data, also known as \nunlabeled data, means more data for the model to be trained on; more research means \nthat there are more datasets annotated with information, also known as labeled data, \nthat can be used to test how well models complete different types of language tasks. \nThis creates a virtuous cycle for English-language NLP \u2014\u00a0more labeled and unlabeled \ndata leads to more research attention, which leads to increased demand for labeled and \nunlabeled data, and so on.\nEnglish is the prime example of a high resource language, a language for which a lot of \nhigh-quality data resources exist. Though it has the most data available of any language \n(English could be called an \u201cextremely\u201d high resource language), there are six other \nlanguages that could be considered high resource \u2014 the official UN languages list, \nminus Russian, plus Japanese (see T able 1). There are also a few dozen medium resource \nlanguages, such as Urdu, Italian, and T agalog, with another one or two orders of \nmagnitude less data, or about one hundredth or one-thousandth of available English data. \nThe rest of the world\u2019s 6,000 plus languages can be considered low resource or extremely \nlow resource, with only small amounts of written text available (Joshi et al., 2020).\nResourcedness can vary within languages as well. Languages such as Arabic and Spanish \ndiffer so much between dialects that many are mutually incomprehensible, even if \nthey mostly use the same written form. Languages can also have different sociolects, \nvarying across different social groups, identity groups, and contexts (e.g. formal versus \ninformal). Regional dialects and sociolects can vary in degrees of difference from \nhaving different vocabulary and grammatical structures (e.g. Australian English or \nAfrican American English versus Standard American English) to make extensive use of \nborrowed words from other languages (e.g. Nigerian English, Indian English), to fully \nhybrid bilingual dialects (e.g. Spanglish, Hinglish). However, the available digitized \ntext of language often doesn\u2019t reflect the full spectrum of variation that exists within a \nlanguage. (Bergman & Diab, 2022). Data scraped from the internet in particular over-\nindexes Standard English spoken by younger people in developed countries (Luccioni \n& Viviano, 2021). Other languages have just as much dialectical diversity as English and \nalso likely over-index on certain dialects.\nFigure 2. Languages mentioned in \npaper abstracts. T op most mentioned \nlanguages in abstracts of papers published \nby the Association for Computational \nLinguistics, May 2022-January 2023.\nSource: (Santy et al., 2023)\nPaper Abstracts\nLanguages with less data available also often have lower quality data available, either \nbecause it is mislabeled or otherwise not representative of how people actually speak \nthe language. This is particularly true with web-crawled data, a key data source for \nlarge language models (Khan & Hanna, 2023). Non-English language data scraped \nfrom the internet is more often machine translated, scanned from an image, or both, \nand each of those processes introduces opportunities for error (Dodge et al., 2021). \nLow- and medium-resource language data on the internet is more often pornographic, \nnonsensical, or non-linguistic content (Kreutzer et al., 2022).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5be40d05-4b33-4e39-a03e-b19688f5d676": {"__data__": {"id_": "5be40d05-4b33-4e39-a03e-b19688f5d676", "embedding": null, "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed344fa9-1b7b-4c6c-b38f-dad78e4591c8", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "34508ff23959cc4f3d4f12a1e69987396c67e894cc019df5040f334456021c73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1f2fedc-c5fb-4b60-8a69-edf0ce4ba64a", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a867742f59be9a1918ae1d92fc4948b6b5fc01b08ce372914dc4c0d47f85454c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is particularly true with web-crawled data, a key data source for \nlarge language models (Khan & Hanna, 2023). Non-English language data scraped \nfrom the internet is more often machine translated, scanned from an image, or both, \nand each of those processes introduces opportunities for error (Dodge et al., 2021). \nLow- and medium-resource language data on the internet is more often pornographic, \nnonsensical, or non-linguistic content (Kreutzer et al., 2022). It is also often labeled as \nthe incorrect language \u2013 around 95% of the time for many low resource languages \u2013 \nbecause automatic language identification works much more poorly with insufficient \ndata, thus creating a circular problem (Caswell et al., 2020). Languages with the worst \nquality web data are disproportionately those written in non-Latin scripts (e.g. Urdu, \nJapanese, Arabic) and those spoken in the Global South (e.g. African languages, \nminority languages in the Middle East, non-Mandarin Chinese languages) (Kreutzer et \nal., 2022).\n17\n0 200 300100\nEnglish\nKorean\nIndonesian\nThai\nFrench\nGreek\nTurkish\nFinnish\nGerman\nSpanish\nSwahili\nClassical Chinese\nHindi\nHebrew\nPolish\nItalian\nKinyarwanda\nArabic\nRussian\nTalugu\nDutch\nJapanese\nVietnamese\nPortuguese\nLatin\nMarathi\n311\n27\n18\n16\n16\n16\n16\n13\n10\n7\n7\n7\n6\n5\n5\n5\n4\n4\n4\n4\n4\n3\n3\n3\n3\n3\nI. Background", "mimetype": "text/plain", "start_char_idx": 3880, "end_char_idx": 5206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e548b25-ab47-4732-b6d5-92fe237cf2ba": {"__data__": {"id_": "2e548b25-ab47-4732-b6d5-92fe237cf2ba", "embedding": null, "metadata": {"page_label": "18", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99792d17-8ba7-4669-a571-8039beeaae9a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "318c1708709db24cfee9987fa188cc83dc76f90108d8dce8b99d38ef615f689f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n18\nLow resource languages also tend to have data that comes from a less diverse set of \nsources. The clean data that does exist often comes from places such as Wikipedia, the \nBible, and parliamentary proceedings, particularly in large language models that depend \non drawing parallels between low and high resource languages (see III.B and III.C) \n(Nekoto et al., 2020). None of these data sources is representative of a language as a \nwhole. For example, there is a significant gender gap when it comes to who contributes \nto Wikipedia, with studies finding that the percentage of women who edit Wikipedia \narticles remains \u201cdismally low\u201d (Callahan & Herring, 2011; Vitulli, 2018), and it \ndoesn\u2019t reflect a more casual style of speech. Some text on Wikipedia is also machine-\ntranslated \u2014 Cebuano, Swedish, and W aray for instance are some of the Wikipedia \nlanguages with the most articles, but most are translated by the same bot (Lokhov, \n2021). The Bible is similarly its own unique domain, unrepresentative of language at \nlarge, but is overrepresented in the training data for non-English large language models. \nThis can lead to errors in the tone and substance of language. For example, for a period \nof time, running a word repeated enough times through Google translate produced a \nreligious-sounding text: the word \u201cdog\u201d pasted two dozen times and translated from \nMaori to English produced text about Jesus\u2019 return at the end of days (Christian, 2018).\nThe resourcedness of a language is often out of sync with the number of speakers or \ninternet users that language has. Hindi, Bengali, and Indonesian are medium-resource \nlanguages yet each has hundreds of millions of speakers (Joshi et al., 2020). Guaran\u00ed, \nan Indigenous language spoken by most of the ~7 million-person population of \nParaguay, hardly has any data resources at all (G\u00f3ngora et al., 2021). Fula, a language \nspoken by tens of millions of W est Africans, also has few data sets (Nguer et al., 2020). \nDespite over 600 million internet users across the African continent, nearly all African \nlanguages remain low-resourced.\nTable 1. Categories of language \nresourcedness. Languages divided into \ndifferent levels of resourcedness, according \nto labeled and unlabeled datasets available \nas of 2020.\nSource: (Joshi et al., 2020)\nResourcedness Languages Number of Languages Number of Speakers\nExtremely\tHigh\tResource English 1 1.1B\nHigh\tResource Arabic,\tFrench,\tJapanese,\tGerman,\t\nSpanish,\tMandarin 6 2.7B\nMedium\tResource\nDutch,\tVietnamese,\tKorean,\t\nPortuguese,\tHindi,\tSlovak,\tHebrew,\t\nIndonesian,\tAfrikaans,\tBengali,\tetc.\nDozens 2.7B\nLow\tResource\nHaitian\tCreole,\tTigrinya,\tSwahili,\t\nBavarian,\tCherokee,\tZulu,\tBurmese,\t\nTelugu,\tMaltese,\tAmharic,\tetc.\nHundreds 0.5B\nExtremely\tLow\tResource Dahalo,\tWarlpiri,\tPopoloca,\t\nWallisian,\tBora,\tetc. Thousands 1.1B\nLost in Translation18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8cf56dc-da54-4541-89ee-b66233b0c7e1": {"__data__": {"id_": "c8cf56dc-da54-4541-89ee-b66233b0c7e1", "embedding": null, "metadata": {"page_label": "19", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee0485e9-1d43-409c-b790-6169c05ec167", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e23b159d64cf357442b400baf6dcb3a37216a7860171e2db7b20dc433348f878", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n19\nMany scholars have worked to try to close this resourcedness gap between high and low \nresource languages. Individual NLP communities have formed around many languages in \norder to kickstart and perpetuate the virtuous cycle of research attention and benchmark \ndevelopment, including collectives such as IndoNLP for languages spoken in Indonesia \nand Masakhane for African languages (Cahyawijaya et al., 2022; Nekoto et al., 2020; Orife \net al., 2020), and conferences such as the Association for Computational Linguistics\u2019 \nlow resource language track, and AmericasNLP for indigenous languages (ACL, 2021; \nAmericasNLP, 2022; Masakhane, n.d.). T ech companies have also sought to expand the \nnumber of language models their models work in, in part by creating more data sets, \nincluding with projects like Facebook\u2019s No Language Left Behind project and Google\u2019s \n1000 Languages Initiative (NLLB T eam et al., 2022; Vincent, 2022). DARPA even \nfunded the Low Resource Languages for Emergent Incidents (LORELEI) program in \n2014 to improve translation about emergency incidents into low resource languages \n(Corvey, 2014). But the gaps between English, other high resource languages, and low \nresource languages remain large and are growing exponentially greater by the day, at least \nin terms of available, raw digitized data.\nThe response by the NLP community has not just been to collect more language \ndata but also to employ technical tricks to help language models squeeze the most \nperformance out of the little data they have. In the next section, we discuss the primary \ntechnical architecture developers use to do this: multilingual language models.\nC. Multilingual Language Models: Efforts to \nBridge the Resourcedness Gap\nIn English, most large language models are monolingual, meaning that they train mostly \non data from one language. Researchers have also built monolingual models in non-\nEnglish languages: for instance, the architecture for Google\u2019s BER T model \u2014 one of \nthe most popular and cheapest to train \u2014 has been utilized for French (CamemBER T), \nItalian (AlBER T o), Arabic (AraBER T), Dutch (BER Tje), Basque (BER T eus), Maltese \n(BER Tu), and Swahili (SwahBER T), to name a few (Agerri et al., 2020; Antoun et al., \n2020; de Vries et al., 2019; G. Martin et al., 2022; L. Martin et al., 2020; Micallef et al., \n2022; Polignano et al., 2019). However, in general, these monolingual models perform \nworse in their respective languages than the best English models do in English because \nthey don\u2019t have as much data to train on.\nThis lack of data manifests in different ways depending on the specific task a model is \nfine-tuned to perform. Some language model capabilities \u2014 usually ones that depend \non fact retrieval \u2014 improve linearly with size. For instance, the more data a language \nmodel is exposed to, the better it is at answering trivia questions or reformatting \ndata (Srivastava et al., 2022). Other capabilities \u2014 usually ones with multiple steps \nI. Background", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "307fa96c-69a4-41e8-beaa-7bfb4cc5f8b8": {"__data__": {"id_": "307fa96c-69a4-41e8-beaa-7bfb4cc5f8b8", "embedding": null, "metadata": {"page_label": "20", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f037292a-85ff-4105-a24b-7455076cea27", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a9ea80fdd192ff62b40aaa4c346c9cfe0dd89aff56bd871004782e55ed969b10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n20\nor components \u2014 exhibit a \u201cbreakthrough\u201d behavior, where once a model reaches a \ncertain size, it improves sharply at the task. For instance, language models typically are \nunable to write code or add three digit numbers until they train on a certain amount \nof data, at which point their performance improves dramatically (Ganguli et al., 2022). \nLow and extremely low resource languages often do not have enough data to train a \nlarge language model at all, but medium and even high resource languages may not \nhave the hundreds of millions, or billions of words of text data necessary to achieve the \nbreakthroughs that English can (Y. Zhang et al., 2021).\nBesides technical limitations, companies may not be interested in deploying a different \nmonolingual model for every language their product is available in for business reasons \nas well. Maintaining and debugging one large language model for each language \nintroduces costs that scale per language introduced, introducing complexity and \nadditional overhead costs. Companies that seek to expand into new global markets \nwill likely try to keep their costs fixed by reusing as much infrastructure as possible, \nincluding language models.\nTherefore, instead of using monolingual models to do NLP tasks in non-English \nlanguages, researchers and developers most often use multilingual language models, \nsuch as Google\u2019s mBER T and Meta\u2019s XLM-R, which are trained on texts from \nmany different languages at once. Like their monolingual counterparts, multilingual \nlanguage models are trained on a fill-in-the-blank task. However, by training on text \nfrom several different languages, multilingual language models can, at least in theory, \ninfer connections between languages, acting as a sort of bridge between high and low \nresource languages, allowing the former to bootstrap the latter.\nFor instance, imagine that an Indian climate change researcher wants to use a language \nmodel to collect all Hindi-language tweets about the weather. A monolingual language \nmodel trained on just Hindi text may not have enough data to have seen the words \n\u201cthaand\u201d (\u201ccold\u201d in Hindi) and \u201cshaal\u201d or (\u201cshawl\u201d in Hindi) appear near one another \nin text, so it may miss that tweets to the effect of \u201cMain Agast mein shaal pahanta \nhoon\u201d (\u201cI put a shawl on in August\u201d) is a sentence about cold weather.1 A multilingual \nmodel, trained on data from English, Hindi, and many other languages may have seen \ntext where \u201cthaand\u201d appears near \u201ccold,\u201d \u201cshaal\u201d appears near \u201cshawl,\u201d and \u201ccold\u201d \nappears near \u201cshawl,\u201d thereby allowing the model to infer that \u201cthaand\u201d and \u201cshaal\u201d are \ninterrelated terms.\nMultilingual language models are usually not trained on equal volumes of data from \neach language: mBER T for instance is trained on 15.5 GB of English text but as little \nas 10 MB of Y oruba text (Wu & Dredze, 2020). Even BLOOM, a popular multilingual \nmodel by BigScience with a particular focus on language representation, has 30% of its \n1  T ransliterated into Roman script for ease of reading for an English-language reader.\nFigure 3. Monolingual vs multilingual \nlanguage model representation \nspace. A visualization of a monolingual \nand a multilingual langauge model\u2019s \nrepresentation space, collapsed into three \ndimensions.\nSource: (Schwenk, 2019)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3332, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6131f19e-cb58-4718-81e4-09698ab9451a": {"__data__": {"id_": "6131f19e-cb58-4718-81e4-09698ab9451a", "embedding": null, "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "400f475d-5691-4a67-841b-36688db2a4cf", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "585a2020b3e165256815d2ee56ab9b7619f55948baf7e025decdf1653360d8a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16043eea-1e0f-4d5f-b52b-67ac1a5e8bae", "node_type": "1", "metadata": {}, "hash": "41505e9cc204c6d090305656a7782290d4b068a987da0489319f7bc4836d685e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n21\nor components \u2014 exhibit a \u201cbreakthrough\u201d behavior, where once a model reaches a \ncertain size, it improves sharply at the task. For instance, language models typically are \nunable to write code or add three digit numbers until they train on a certain amount \nof data, at which point their performance improves dramatically (Ganguli et al., 2022). \nLow and extremely low resource languages often do not have enough data to train a \nlarge language model at all, but medium and even high resource languages may not \nhave the hundreds of millions, or billions of words of text data necessary to achieve the \nbreakthroughs that English can (Y. Zhang et al., 2021).\nBesides technical limitations, companies may not be interested in deploying a different \nmonolingual model for every language their product is available in for business reasons \nas well. Maintaining and debugging one large language model for each language \nintroduces costs that scale per language introduced, introducing complexity and \nadditional overhead costs. Companies that seek to expand into new global markets \nwill likely try to keep their costs fixed by reusing as much infrastructure as possible, \nincluding language models.\nTherefore, instead of using monolingual models to do NLP tasks in non-English \nlanguages, researchers and developers most often use multilingual language models, \nsuch as Google\u2019s mBER T and Meta\u2019s XLM-R, which are trained on texts from \nmany different languages at once. Like their monolingual counterparts, multilingual \nlanguage models are trained on a fill-in-the-blank task. However, by training on text \nfrom several different languages, multilingual language models can, at least in theory, \ninfer connections between languages, acting as a sort of bridge between high and low \nresource languages, allowing the former to bootstrap the latter.\nFor instance, imagine that an Indian climate change researcher wants to use a language \nmodel to collect all Hindi-language tweets about the weather. A monolingual language \nmodel trained on just Hindi text may not have enough data to have seen the words \n\u201cthaand\u201d (\u201ccold\u201d in Hindi) and \u201cshaal\u201d or (\u201cshawl\u201d in Hindi) appear near one another \nin text, so it may miss that tweets to the effect of \u201cMain Agast mein shaal pahanta \nhoon\u201d (\u201cI put a shawl on in August\u201d) is a sentence about cold weather.1 A multilingual \nmodel, trained on data from English, Hindi, and many other languages may have seen \ntext where \u201cthaand\u201d appears near \u201ccold,\u201d \u201cshaal\u201d appears near \u201cshawl,\u201d and \u201ccold\u201d \nappears near \u201cshawl,\u201d thereby allowing the model to infer that \u201cthaand\u201d and \u201cshaal\u201d are \ninterrelated terms.\nMultilingual language models are usually not trained on equal volumes of data from \neach language: mBER T for instance is trained on 15.5 GB of English text but as little \nas 10 MB of Y oruba text (Wu & Dredze, 2020). Even BLOOM, a popular multilingual \nmodel by BigScience with a particular focus on language representation, has 30% of its \n1  T ransliterated into Roman script for ease of reading for an English-language reader.\nFigure 3. Monolingual vs multilingual \nlanguage model representation \nspace. A visualization of a monolingual \nand a multilingual langauge model\u2019s \nrepresentation space, collapsed into three \ndimensions.\nSource: (Schwenk, 2019) \ntraining text in English (BigScience W orkshop et al., 2023). In large part, this is because \nof the lack of available data in these languages, which come disproportionately from \nWikipedia and religious texts, as discussed earlier (see Part I.C).\nJust as a monolingual language model can be fine-tuned to work better on an individual \ntask, a multilingual language model can be fine-tuned to work better in an individual \nlanguage. Imagine for instance a developer who wants to use a multilingual language \nmodel to detect Indonesian election disinformation on social media. One way they \ncould do it is by using an out-of-the-box multilingual model, such as BLOOM, and \nfine-tuning it by showing examples of false narratives circulated in Indonesian related \nto the local election.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16043eea-1e0f-4d5f-b52b-67ac1a5e8bae": {"__data__": {"id_": "16043eea-1e0f-4d5f-b52b-67ac1a5e8bae", "embedding": null, "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "400f475d-5691-4a67-841b-36688db2a4cf", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "585a2020b3e165256815d2ee56ab9b7619f55948baf7e025decdf1653360d8a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6131f19e-cb58-4718-81e4-09698ab9451a", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "56ce99e8f7cd5e4ec3d7027199123412f8e6ac52a287ccdeae4395a5ad196b6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just as a monolingual language model can be fine-tuned to work better on an individual \ntask, a multilingual language model can be fine-tuned to work better in an individual \nlanguage. Imagine for instance a developer who wants to use a multilingual language \nmodel to detect Indonesian election disinformation on social media. One way they \ncould do it is by using an out-of-the-box multilingual model, such as BLOOM, and \nfine-tuning it by showing examples of false narratives circulated in Indonesian related \nto the local election. This likely would not work very well though, since BLOOM has \nonly been exposed to a limited amount of data on Indonesian text \u2014\u00a0only 1.2% of its \ntraining data is in Indonesian (BigScience W orkshop et al., 2023). Another better way \nto do it, if the developer has access to more Indonesian language data, would be first to \nfine-tune the model on additional Indonesian text (essentially, continuing to learn the \nfill-in-the-missing-word task, but this time just in Indonesian) and then further fine-\ntuning it on the task election disinfo detection using that dataset.\nModel developers though do not always have enough text data to sufficiently fine-\ntune a multilingual model to work in a specific language. T o make up for this, they \noften use imperfect machine-translated text. The two main methods of incorporating \ntranslated text are called translate-train or translate-test methods. With translate-train, \na multilingual language model is fine-tuned on data that has been translated from \n(usually) English into a desired lower resource language (Conneau & Lample, 2019). \nWith translate-test, a (usually) English monolingual language model is fine-tuned \non data translated from the desired language into English, and all testing data gets \ntranslated into English as well (Artetxe, Labaka, et al., 2020).\nI. Background 21\nThe\ttree\tis\tgreen. The\ttree\tis\tgreen.\nEl\t\u00e1rbol\tes\tverde.\nMonolingual model\nIt\tis\tcold\ttoday.\nMultlingual model\nI\tput\ton\ta\tshawl.\nI\tlike\tto\tsing.\t I\tlike\tto\tsing.\t\nJ\u2019aime\tchanter.\t\nAaj\tbohut\tthaand\thai. Main\tek\tshaal\tpahanta\thoon.\nI\tput\ton\ta\tshawl.It\tis\tcold\ttoday.", "mimetype": "text/plain", "start_char_idx": 3603, "end_char_idx": 5738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2620c189-b8b4-4344-a53c-6c9e0328f3b2": {"__data__": {"id_": "2620c189-b8b4-4344-a53c-6c9e0328f3b2", "embedding": null, "metadata": {"page_label": "22", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "090eca36-29ef-45d9-bf03-ebd78d053609", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "44cb19da5bd13c801a3dea2113ae4a642203e59b4ebcee795436e09303482ebf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n22\nImagine, for example, a developer building a language model to detect terrorist content \nin the Basque language with a handful of examples of terrorist content in Basque \nbut not enough Basque text data to properly fine-tune a language model. With the \ntranslate-train approach, a developer would take a large volume of English text data, \nmachine translate it into Basque, use that data to fine-tune a pretrained multilingual \nlanguage model, and then further fine-tune it to the task of terrorist content detection \nusing the native Basque data. With translate-test, a developer would fine-tune a \npretrained English language model on data translated from Basque to English, and \nthen further fine-tune it by translating the terrorist content data they have into English. \nSubsequently, to analyze Basque text, it would first have to be translated into English \nbefore being evaluated by the model. Reliance on translated data raises many concerns, \nas discussed in Part II.C.1.\nHowever, translated texts can help multilingual language models learn connections \nbetween languages. By feeding a model parallel texts \u2014 for instance, explicitly \ninforming it that \u201cbaahar bohut thand hai\u201d and \u201cIt\u2019s so cold outside\u201d have the same \nmeaning \u2014 it can better extrapolate other language parallels as well (e.g. NLLB T eam et \nal., 2022; Reid & Artetxe, 2022). Multilingual language models can learn connections \nbetween languages without explicit labeling, instead inferring relationships between \nlanguages on its own through borrowed words, numbers, and URLs (Pires et al., 2019). \nIn general, NLP researchers understand little about why it is that multilingual language \nmodels can be effectively fine-tuned to work in languages that they have relatively little \ndata for (Conneau, Khandelwal, et al., 2020; Pires et al., 2019; Wu & Dredze, 2019). \nSome argue that it is because multilingual language models have inferred language-\nagnostic concepts and universal rules that can be applied to any language (Artetxe, Ruder, \net al., 2020; Chi et al., 2020; Conneau, Wu, et al., 2020; T svetkov et al., 2016). Others \nsay that multilingual language models are just effective imitators (Bender et al., 2021; \nLauscher et al., 2020). The debate is impossible to fully resolve because of the overall \ncomplexity and opacity of large language models, but so far evidence suggests that at \nbest, the linguistic universals they learn are limited to narrow semantic and syntactic \ndomains (Libovick\u00fd et al., 2019; Wu & Dredze, 2019), such as learning plural/singular \nverb agreement across multiple languages (de V arda & Marelli, 2023). But even if a model \ncan infer syntactic or semantic commonalities between languages, such inferences will \nnot necessarily help it manage more complex, context-dependent tasks (Choi et al., 2021). \nFor instance, in some languages, multilingual language models do no better than random \nguessing at detecting hate speech (Lin et al., 2022). As will be discussed in the next section, \nthese are hardly the only limits of multilingual language models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8453c056-4770-4d6d-a882-862d42c07e47": {"__data__": {"id_": "8453c056-4770-4d6d-a882-862d42c07e47", "embedding": null, "metadata": {"page_label": "23", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eaf6042-758a-4e20-a01c-b126bb27f90b", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "937464263447eb2a5f89ebc94bca27c59cc86c59d1fd887dec517fb3cea11607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "23\nLost in Translation\nII. Limitations of \nLanguage Models \nin English and Non-\nEnglish Contexts\nT\nhe press, technology companies, and social media are abuzz \nabout the potential of large language models. In this section, \nhowever, we discuss the shortcomings of these models, \nparticularly as they operate in non-English language contexts. \nIn the first section, we discuss general concerns with building and \ndeploying large language models. These concerns apply both to the \nEnglish and non-English contexts. In the second section, we look at the \nproblems more specifically raised by multilingual language models.\nA. Concerns with Building and \nDeploying Large Language Models\n1. LARGE LANGUAGE MODELS ARE BOUND BY \nLANGUAGE THEY HAVE SEEN BEFORE AND STRUGGLE \nTO PERFORM IN NEW CONTEXTS.\nA large language model does not understand language; instead, it makes \nprobabilistic inferences about text based on the distribution of language \nwithin the data it is trained on. Bender and Koller argue that this means \nlanguage models are limited to contexts they have encountered before \nand struggle greatly in those they have not (2020). NLP researchers have \nalready proven this is the case in generative AI by demonstrating several \nunintuitive outcomes: for instance, language models are better able to \nperform mathematical operations with numbers that appear frequently \nin written language (e.g., multiplying numbers by 24), than numbers \nthat appear infrequently (e.g. multiplying numbers by 23) (Razeghi \net al., 2022). Large language models may exhibit similar limitations in \ncontent analysis as well. For instance, if a large language model were \nused to analyze a candidate\u2019s resume, it may struggle to account for \nlesser-known companies or newer skill sets without up-to-date, domain-\nspecific data to fine-tune on. These tasks are reliant on in-context \nknowledge and without domain-specific training, i.e. training an off-\nthe-shelf large language model with text relevant to the task at hand, \nthese models are likely to perform poorly and their purported domain-\nagnostic abilities should garner skepticism (Duarte et al., 2017).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d058450-2a2a-4f44-8132-c2bbe5358952": {"__data__": {"id_": "8d058450-2a2a-4f44-8132-c2bbe5358952", "embedding": null, "metadata": {"page_label": "24", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b0caec0-65cd-47cb-b812-7deec3927471", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "bacde24400862412ff2b48d9b81c0b7580efa9c9eed37dd6fe2e4b3ecfebebca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n24\n2. LARGE LANGUAGE MODELS REPRODUCE THE BIASES, VALUES, \nAND HARMS OF THE DATA THEY TRAIN ON.\nLarge language models are built using vast quantities of text scraped from the internet \nand exhibit all the biases and limitations of their data source (Okerlund et al., 2022). \nSome commonly used datasets, such as Common Crawl, include large volumes of \nhate speech and sexually explicit content (Luccioni & Viviano, 2021). Other problems \nare more nefarious. For example, researchers found that when GPT-3 generated \ncompletions for the prompt \u201cT wo Muslims walked into a___,\u201d 66% of completions \nincluded violent language, three times more than for other religious groups (Abid et \nal., 2021). Others have found similar entrenched biases against people with disabilities, \nfor example inferring negative sentiment from sentences that include disability-related \nterms (Hutchinson et al., 2020).\nThough technologists often try to pull out explicitly harmful data from training \nsets, models can still reify harms, such as referring to \u201cwomen doctors\u201d or calling \nundocumented immigrants \u201cillegals\u201d (Bender et al., 2021). Removing these instances \nof harmful data from training datasets, which are disproportionately outsourced \nto underpaid staff around the world, also imposes labor and psychological burdens \n(Williams et al., 2022). \nEven if datasets are rid of specific examples of harmful text, they will nonetheless \ncontain values and assumptions that are encoded into the language we speak and the \ndominant perspectives that exist in many pieces of written text, particularly government \ndocuments or state-run media pieces that may make up the bulk of text available for \nlow resource languages (Bender et al., 2021). Many machine learning researchers fail to \nconsider these problems in their work \u2014 one study found that 98% of machine learning \npapers mention no negative potential of the technologies they are describing (Birhane \net al., 2022). Y et the risks are very real: as Birhane & Prabhu put it, \u201cFeeding AI systems \non the world\u2019s beauty, ugliness, and cruelty, but expecting it to reflect only the beauty \nis a fantasy\u201d (2021). When these problems exist in any particularly popular foundation \nmodel, they proliferate across many different applications built on top of that model.\n3. THE DATA LARGE LANGUAGE MODELS TRAIN ON RAISE \nCOPYRIGHT AND PRIVACY CONCERNS.\nLegal experts also raise concerns about copyright and ownership of text that make up \nthe vast quantities of data that train and distinguish large models (Ebers et al., 2022; \nOkerlund et al., 2022). Getty Images has sued the creators of Stable Diffusion, an AI \ntool that creates images based on written prompts, claiming that the toolscraped Getty\u2019s \ndatabases of proprietary images and photos without permission (Vincent, 2023a). Legal \nquestions about ownership of text and whether scraping proprietary text is lawful (e.g., \nbecause it constitutes fair use) or not remain unanswered (Kublik, n.d.).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a993290a-7898-40f6-b477-0bf30940c1d2": {"__data__": {"id_": "a993290a-7898-40f6-b477-0bf30940c1d2", "embedding": null, "metadata": {"page_label": "25", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eedb5d77-eda4-476e-9b3e-e34a862ca41a", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7b4e5923853354d86b86d191425a86fdc8d434988c5ed449d1109290e2c7b78d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n25II. Limitations of Language Models in English and Non-English Contexts\nSome datasets that large language models train on are likely to capture examples of \nlanguage from sites such as social media, raising personal data privacy concerns. There \nis a high possibility that in gathering exchanges from social media networks, training \ndatasets inadvertently contain private and even sensitive information, which increases \nthe risk of models leaking details like names, phone numbers, or addresses from the data \non which they\u2019re trained (Carlini et al., 2021, 2023).\n4. TRAINING LARGE LANGUAGE MODELS COULD HAVE A \nSIGNIFICANT ENVIRONMENTAL IMPACT.\nFinally, there are increasing concerns about the environmental cost of producing large \nlanguage models. Scholars and advocates have raised concerns about the environmental \nimpact of training these models, particularly the largest ones with billions of \nparameters, due to their intense computation requirements (Crawford, 2021; Okerlund \net al., 2022). There is preliminary research attempting to quantify the energy impacts \nof computation at this scale (Kaack et al., 2022), but some early estimates suggest that \ntraining a single BER T model, one that serves as the foundation for some multilingual \nlanguage models, requires as much energy as a trans-American flight (Strubell et al., \n2019). Large language models, like GPT-3, require thousands of times more (Heikkil\u00e4, \n2022). Png writes that these costs may be concentrated in poorer countries, where \nserver farms and raw materials required to build necessary infrastructure are often \nlocated (2022).\nB. Limitations of Multilingual Language Models\n1. MULTILINGUAL LANGUAGE MODELS OFTEN RELY ON MACHINE-\nTRANSLATED TEXT THAT CAN CONTAIN ERRORS OR TERMS NATIVE \nLANGUAGE SPEAKERS DON\u2019T ACTUALLY USE.\nIncorporating machine-translated data into the training and fine-tuning of multilingual \nlanguage models creates various opportunities for the model to malfunction. \nMultilingual language models that depend on translation may struggle to build \naccurate representations of words or concepts which have different connotations in \ndifferent languages. For instance, in English, \u201cdove\u201d is a term associated with peace, but \nits equivalent in Basque, \u201cuso,\u201d is an emasculating insult. A translation-based cross-\nlingual model that does not train on the word \u201cuso\u201d used in its native context could \npotentially fail to see it used in a call for violence since the English mapping is so closely \nassociated with \u201cpeace.\u201d", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9f80178a-f532-4fb2-8bc2-00e1e7cec020": {"__data__": {"id_": "9f80178a-f532-4fb2-8bc2-00e1e7cec020", "embedding": null, "metadata": {"page_label": "26", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1487321-7801-47a3-8b62-ae34823edc14", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "f4724a3bdcb2d3f81b86fa7cf5a89ff0479a3db1fc68477f9b8938f8913b5774", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n26\nAnother issue is what NLP practitioners call the \u201ctranslationese\u201d problem (Yu et al., \n2022) \u2014 that is, machine-translated language materially differs from how human \nnative speakers naturally use language (Bizzoni et al., 2020; T eich, 2003). In generative \nAI, translationese can result in mono- or multilingual language models simplifying \nor overcomplicating sentences, producing repeated words, using too common or too \nuncommon words, borrowing too much or too little from the original language, and \nother patterns of speech native speakers would not use (V olansky et al., 2015). These \nmistakes are not consistent between languages or systems, so it would be difficult for \nmodels to be able to systematically root them out, though some argue that it is possible \n(Yu et al., 2022).\nThe problems of machine translation spread beyond models that intentionally train on \nit. The web is filled with machine-translated text, and models that train on web-scraped \ndata will inadvertently encounter a lot of it, particularly in low resource languages \n(Kreutzer et al., 2022). For instance, a lot of the Catalan data that exists on the web, \nparticularly on websites using the .cat top-level domain, is translated using Google \nT ranslate, even on official government websites (Pym et al., 2022). Even benchmarks to \ntest how well multilingual language models work in high and low resource languages are \noften translated from another language, leaving researchers with less of a sense of how \nwell these models work on language as spoken by native speakers. For instance, OpenAI \ntested GPT-4\u2019s capabilities in 26 languages, but using only benchmarks translated from \nEnglish (OpenAI, 2023).\n2. MULTILINGUAL LANGUAGE MODELS FAIL TO ACCOUNT FOR THE \nCONTEXTS OF LOCAL LANGUAGE SPEAKERS.\nAs discussed earlier, large language models only work well in contexts similar to \ncontexts of the data they are trained on. A language model trained on legal texts, \nfor instance, will perform much better on law-related tasks than medical tasks \nor interpreting the Quran (Koehn & Knowles, 2017). This poses a problem for \nmultilingual language models, which, particularly in low resource languages, are trained \non text that is translated from other language contexts or comes from a few distinctive \ncontexts, such as Wikipedia and the Bible. Multilingual language models that are not \ntrained on large volumes of text from native speakers of a given language will more \noften fail at tasks that require knowledge of an individual speaker\u2019s local context, such \nas hate speech detection and resume scanning (Lin et al., 2022).\nImagine, for example, a multilingual language model fine-tuned to detect anti-\nMuslim content in Assamese, a low-resource language with fifteen million speakers, \npredominantly in northeast India (Ethnologue, 2023a). Assamese and Bengali are both \nmedium resource languages, so a multilingual model may draw connections between \nthe two. However, anti-Muslim hate speech is very closely tied to historical events and \nthe specific political conditions of Assam. For instance, the term \u201cBangladeshi Muslim,\u201d", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7727578f-6869-4fd6-a34a-9d90a2cfca7b": {"__data__": {"id_": "7727578f-6869-4fd6-a34a-9d90a2cfca7b", "embedding": null, "metadata": {"page_label": "27", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fef6a84b-7e1a-4a15-bb77-483ff59ffef5", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "2863d7649fcc7b813fb75a147725e04d38398e723d4b04061eb70c614f3b033b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n27II. Limitations of Language Models in English and Non-English Contexts\nneutral in many other languages and contexts, is a hate speech dog whistle in Assamese \nbecause it casts Assamese Muslims as foreigners (a concept that is itself closely tied to the \nIndian government\u2019s repatriation efforts) (Avaaz, 2019). A multilingual model neither \ntrained on extensive native Assamese text nor explicitly trained by a language expert would \nlikely not be able to capture this hyperlocal distinction.\nMultilingual language models work by transferring between language contexts, but that \ntransfer often means simply that the context of higher resource languages overwrites \nlower resource ones. Spanish, for instance, tends to use more adjectives and analogies \ndescribing extreme situations than English, so a sentiment detection algorithm that \ntransfers linguistic properties over from English may mischaracterize Spanish text as \nhaving a stronger emotional valence than it would to a native speaker (Stadthagen-\nGonzalez et al., 2017). This structure transfer can also bring the biases of a source \nlanguage into a target language (Savoldi et al., 2021). For instance, if a language without \ngender pronouns, such as Hungarian or Y oruba, is mapped onto a language with \ngendered third-person pronouns, such as English or French, the language model could \nforce gender associations and biases of the gendered language onto the non-gendered \none, as often occurs in translation (Prates et al., 2020) (see Figure 4).\nFigure 4. Google Translate from \nHungarian to English. A screenshot of \nGoogle T ranslate, circa 2020, showing how \nthe multilingual language models project \ngender onto genderless languages.\nSource: (Prates et al., 2020)\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "225cd80f-bd6f-4343-ba5e-c7332f70ab5e": {"__data__": {"id_": "225cd80f-bd6f-4343-ba5e-c7332f70ab5e", "embedding": null, "metadata": {"page_label": "28", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2bad9372-40d9-4b3b-b3f3-f7a8d3793fd2", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d4cfcfd3913ed865cf024b5224b01e314d85df51a1939c6f1678c5dcd438d679", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n28\n3. MULTILINGUAL LANGUAGE MODELS DO NOT AND CANNOT WORK \nEQUALLY WELL IN ALL LANGUAGES.\nMultilingual language models not only do not work equally well in all languages but \nthey cannot, since the more languages a multilingual model is trained on, the less it \ncan capture unique traits of any specific languages. This problem is called the curse \nof multilinguality (Lauscher et al., 2020). Large language model developers are thus \nforced to trade off performance between disparate languages; making a model work \nbetter in Hindi for example, may come at a cost to its performance in English. In \npractice, when technology companies must choose which languages to deprioritize \nwithin their multilingual language models, they may be incentivized to have them \nbe languages where speakers tend to be less wealthy, have less political power, or live \noutside of the company\u2019s priority markets, thus exacerbating the resourcedness gap they \nare designed to address.\nIn general, semantic and syntactic similarity to a high resource language protects \nfrom the curse of multilinguality (Eronen et al., 2023). For instance, Muller et al. \ntested mBER T on languages it had not explicitly trained on before and found that it \nworked better in Swiss German (related to German, a high resource language), than \nit did in Estonian (a Uralic language, like medium resource languages Hungarian and \nFinnish), than it does Uyghur (a Turkic language, distant from any high or medium \nresource language, with four alphabets) (2021). In general, multilingual language \nmodels struggle with languages written in non-Latin scripts (Pires et al., 2019; Ruder \net al., 2021), language isolates (languages etymologically distinct from all other \nlanguages, such as Basque), and families of languages less connected to those of high \nresource languages. This threatens to create a poor-get-poorer dynamic for languages \nthat are only similar to other low resource languages, as is the case with many widely \nspoken African languages including Swahili, Amharic, and Kabyle (Joshi et al., 2020). \nThis dynamic further strengthens the post-colonial structural inequality discussed \nthroughout this report.\nMultilingual language models are also forced to trade off between languages in the \nvocabulary they use. Large language models train on the problem of predicting the next \nword in a sentence. If a model is trying to guess the word to fill in \u201cT oday I feel ___,\u201d it \nwill have a harder time doing so if it has to choose between ten million possible words \nfrom any language instead of just a few hundred thousand English words. The total \nnumber of words a language model has to choose from is called its vocabulary size. The \nlarger a model\u2019s vocabulary size, the more different possible words it can generate and \nrecognize, but also the more computational resources it takes to train. Multilingual \nlanguage models use all kinds of shortcuts to get their vocabulary size down. For instance,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8578e875-b982-4b96-855a-bfd22cab2e01": {"__data__": {"id_": "8578e875-b982-4b96-855a-bfd22cab2e01", "embedding": null, "metadata": {"page_label": "29", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1057d7cc-e7e9-4637-992b-69f7f53d80d6", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "76f058fbf8e992357c1b3e26f11faf933d5371e7f7aabe004a8dab4a07545390", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n29\nthey will often transliterate languages into Latin scripts or train the model to guess the \nnext subword (e.g. breaking \u201ctasks\u201d into \u201cta\u201d and \u201c##sks\u201d) or letter instead of the full \nword, thus collapsing the barrier between languages (T ay et al., 2022; C. W ang et al., \n2020). These shortcuts cut down on costs, but they also reduce a model\u2019s ability to \ncapture semantic relationships between words, thus degrading its performance overall.\nV ocabulary is often decided by how frequently different words, subwords, and \nletters appear in a model\u2019s training text, and since multilingual language models are \ntrained mostly on English data, their vocabularies will skew towards English as well. \nA multilingual model may have a relatively obscure word like \u201criposte\u201d in its English \nvocabulary, but be may missing common words in other high resource languages (e.g., \n\u201cescritorio\u201d in Spanish), common subwords in medium resource languages, (e.g., \u201ctzv\u201d \nin Hebrew), and entire letters in low resource languages (e.g., a character that appears in \nTigrinya but not other Ge\u2019ez-based scripts). This inferior representation makes models \nperform worse in a variety of tasks, and makes content analysis systems far easier to trick \nby doing things like changing white space, using typos, or in the case of toxic content \ndetection, adding common, positive words like \u201clove\u201d (Gr\u00f6ndahl et al., 2018; Lees et al., \n2022).\n4. WHEN MULTILINGUAL LANGUAGE MODELS FAIL, THEIR \nPROBLEMS ARE HARD TO IDENTIFY, DIAGNOSE, AND FIX.\nNLP practitioners depend on benchmarks to determine both how well a language \nmodel performs at specific tasks and how close it is in general to achieving \u201cnatural \nlanguage understanding\u201d (Bender & Koller, 2020). This latter type of benchmarking \nis very difficult in all languages, since it is hard to generalize about a language model\u2019s \ncapabilities from only a handful of disparate tests (Raji et al., 2021). However, the \nchallenges of both types of benchmarks are exacerbated in the multilingual context. \nThe disparities in NLP research attention and labeled data between languages mean \nthat there are far more benchmarks and tasks that can be used to test models in English \nthan in other languages, particularly low resource ones. Models developed to operate in \nnon-English contexts are still usually tested with benchmarks translated from English \nwhich, as discussed earlier, is often markedly different from the target language.\nThe alternative to translation is hiring people local to the contexts a model is being \napplied to and paying them to create data sets and develop benchmarks. This works \nparticularly well for models built to do a specific task in a specific language (Nguyen, \n2020; T attle, n.d.), but is very expensive and resource intensive to scale up for models \nmeant to work in many languages and contexts. It also raises challenging questions \nfor detecting bias in language models (T alat et al., 2022) and performing inherently \nII. Limitations of Language Models in English and Non-English Contexts", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83daae95-38dc-4b8a-a0e9-d5bf1ee59992": {"__data__": {"id_": "83daae95-38dc-4b8a-a0e9-d5bf1ee59992", "embedding": null, "metadata": {"page_label": "30", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f7e90e0-2fa3-4da9-90bc-c07182ed2114", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "f48ff0829807164d38efffd8bd5f2a803eca72195fc3478951e8530bbe31481b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n30\npolitical tasks, such as content moderation. For instance, a social media company trying \nto create a dataset of inflammatory content posted in Bosnia and Herzegovina needs \npeople who are experts in multiple ethnic conflicts and languages (Bosnian, Serbian, \nMontenegrin, and Macedonian) but also unbiased in those conflicts, all in a country \nthat lacks media pluralism or a strong civil society sector (Article 19, 2022). Scaling this \nto every geopolitical problem discussed in all languages on a given online service is a \ndaunting, if not impossible, task.\nWhen problems with multilingual language models can be found, it is often difficult \nto determine why they are occurring. Large language models are already opaque, even \nto those who develop them \u2014 neural networks, the core technology underlying large \nlanguage models, are known for being particularly obtuse and for representing language \nin a way that doesn\u2019t map cleanly onto human-understandable concepts (Nicholas, \n2020). However, multilingual language models are particularly opaque because they \nmake unintuitive, hard-to-trace connections between languages. T ake for instance, \nthis case from an NLP paper: the Google researchers behind the Perspective API, a \nmodel for detecting \u201ctoxic\u201d content, found that their model flagged tweets that used \nthe Italian word \u201csfiga\u201d (which roughly translates to \u201cbad luck\u201d) as hate speech because \ntwo of the three examples included in the training dataset that contained the subword \n\u201csfiga\u201d were labeled as hate speech (\u201csfigati\u201d is an insult meaning \u201closer\u201d) (Lees et \nal., 2020). If this were a multilingual model that had mapped Italian learnings onto \nTurkish analysis, perhaps sentences with the equivalent Turkish word for \u201cunlucky\u201d \n(\u201c\u015fanss\u0131z\u201d) would also be flagged as hate speech. Even if researchers had access to all the \ndata used to train that multilingual model, it would be extremely difficult to locate and \nfix this bug without knowing Italian or understanding how the model had mapped \nthese relationships.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0b0d568-7e95-43ab-9d5b-8f5d0bc8fee5": {"__data__": {"id_": "e0b0d568-7e95-43ab-9d5b-8f5d0bc8fee5", "embedding": null, "metadata": {"page_label": "31", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2691b756-3e5a-4d3c-9200-ffabb02b378a", "node_type": "4", "metadata": {"page_label": "31", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "eb402bd84a1eb59012e499dda223ee49f331f426ca39baeae3793d0de5fd3cf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "31\nLost in Translation\nIII. Recommendations\nE\nfforts to improve language models\u2019 performance in various \nlanguages and contexts are exciting, as they may boost \nconnectivity and information exchange for billions of users \naround the world. However, language models are limited in their \ncapabilities, and employing them too widely, without safeguards, or for \nthe wrong kinds of tasks has the potential to raise civil liberties concerns \nand erect new barriers (Maundu, 2023). Unthinking deployment \nof large language models may impede peoples\u2019 ability to access \ninformation, employment, and public benefits, with disparate impacts \nfor individuals in the Global South where many of the low resource \nlanguages are spoken. W e should be cautious about the rapid adoption \nof these technologies, especially as building blocks for other types of \nautomation in high-stakes arenas like content moderation, employment \nsoftware, and resource allocation.\nIn this section, we offer recommendations for companies, researchers, and \ngovernments to take into consideration as they build, study, and regulate \nlarge language models, particularly in non-English language contexts.\nA. Companies\nTECHNOLOGY COMPANIES SHOULD DISCLOSE WHEN, \nHOW, AND IN WHAT LANGUAGES THEY USE LARGE \nLANGUAGE MODELS.\nT o better understand the problems and challenges with deploying large \nlanguage models in different languages, researchers and the public need \nto know where to look. Companies that incorporate language models \ninto their technical systems should always disclose how they are using \nthem, which languages they are using them in, and what languages they \nhave been trained on. Currently, the approach of many companies to AI \ntransparency consists of trumpeting the capabilities of their AI systems \nin blog posts and press releases, and, for a few larger firms, releasing \nresearch versions of their language models that still differ from the ones \nthey use in production. Despite publishing on AI and pushing the field \nforward, technology companies tend to hold information about their \nproduction AI systems, even basic information about what languages \nthey are used in, close to the chest.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc909015-b43a-4271-acdd-d3c0ca16999e": {"__data__": {"id_": "bc909015-b43a-4271-acdd-d3c0ca16999e", "embedding": null, "metadata": {"page_label": "32", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39ade76d-4273-444c-ba32-232ba7a8c3a9", "node_type": "4", "metadata": {"page_label": "32", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "6c5add3f1dba1a367c489955ea7eaa6ea407c64aa77cbefdebb95119980d460d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n32\nAcademics and civil society have written extensively about how technology companies, \nparticularly online service providers, could offer better transparency and accountability \nfor their AI systems, including language models. The Santa Clara Principles, a set of \nprinciples developed and revised by global civil society groups, provides examples of \nthe types of disclosures companies can make about their content moderation policies \nand processes (2021). Groups like BigScience also pave the way, exemplifying the type \nof documentation other model-developers can publish about their content analysis \nsystems, including model cards, transparency reports, and other avenues to disclose \nmore information about the linguistic makeup of a model\u2019s training data (e.g. what \nlanguages it has trained on, how much data from each language, where those datasets \ncome from). Better transparency creates opportunities for external actors to more \nimmediately identify potential risks and impacts on users and for technology companies \nto mitigate the potential dangers of deploying large language models in English and \nnon-English contexts.\nWHEN DEPLOYED, LARGE LANGUAGE MODELS SHOULD BE \nACCOMPANIED BY ADEQUATE REMEDIAL CHANNELS AND \nMECHANISMS THAT ENSURE INDIVIDUALS CAN APPEAL OUTCOMES \nAND DECISIONS MADE BY THESE SYSTEMS.\nBecause of the complexities of human speech and the error-prone nature of automated \ntools, decision-making systems built on top of large language models should be used \nwithin narrow remits and with adequate remedial channels for users encountering \nthem. Those remedial channels and processes should have human reviewers with \nthe same language proficiencies that their systems are deployed in. Language- and \ncontext-specific remedial channels are particularly important for allowing users to \nappeal decisions made by online services, especially when those decisions either restrict \ntheir expression or access to information or fundamentally determine their access \nto economic or social rights like the right to housing, education, and social security \n(United Nations Human Rights Office of the High Commissioner, n.d.).\nT echnology companies can also offer accountability at a system level, not just the \nlevel of individual decisions. One way to do this is to conduct and publish human \nrights impact assessments at the different phases of the language model\u2019s life cycle \n\u2014 development, testing, deployment, and evaluation (Prabhakaran et al., 2022). \nPublishing human rights impact assessments will also aid in other actors\u2019 decisions \nwhen procuring these systems to conduct tasks in different domains and contexts. In \nparticular, these human rights impact assessments should consider the disparate risks \nto different language speakers in advance of a model being deployed in those languages. \nOnline service providers can provide transparency by disclosing the systems and \nlanguages they use large language models in.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8547566c-ae08-4ca5-bae2-7ebc3eeb1787": {"__data__": {"id_": "8547566c-ae08-4ca5-bae2-7ebc3eeb1787", "embedding": null, "metadata": {"page_label": "33", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b00766c-1d53-4d5b-9a03-87fa07cbaa5d", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "3ca579069332cb280af86b87f515f5932034672e58128d7bd2326918855e2694", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n33III. Recommendations\nCOMPANIES SHOULD INVEST IN IMPROVING LANGUAGE MODEL \nPERFORMANCE IN INDIVIDUAL LANGUAGES BY BRINGING IN \nLANGUAGE AND CONTEXT EXPERTS.\nRecently, an arms race has begun between Google and Meta to see who can include \nmore languages in their multilingual language model. Meta\u2019s \u201cNo Language Left \nBehind\u201d initiative trained a model on over 200 languages (NLLB T eam et al., 2022); \nmonths later, Google one upped Meta with its \u201c1,000 Languages Initiative\u201d (Vincent, \n2022). This race puts a premium on the number of languages the model trains on, \nrather than how well it works in each language. In particular, it is unclear how these \nmodels will handle the \u201ccurse of multilinguality,\u201d where, as explained in II.B.3, the \nmore languages a model trains on, the less it can capture the idiosyncrasies of each \nlanguage. It is also unclear how these companies define a model \u201cworking\u201d in any of \nthese languages.\nCompanies building large language models should not just focus on the number of \nlanguages their model is trained on but the quality of its performance in each language. \nIn part, that means better benchmarks, but benchmarks can only go so far. T o evaluate \nthe full range of potential applications and pitfalls that could come with applying a \nlanguage model in a specific language context, it is necessary to involve language experts, \ncivil society, local experts, heritage and language preservation advocates, linguists, and \nhuman rights experts. These actors are crucial to ensuring that labeled training datasets \nadequately capture the nuances and variations of a given language. Many organizations \nare already doing this type of work. Uli is an example of this, where two India-based \nnonprofit organizations \u2014 T attle and Centre for Internet & Society \u2014 convened a \nrange of gender, gender-based violence, communal violence, and other language experts \nto annotate training datasets in Indian English, T amil, and Hindi to build a tool capable \nof parsing sentiment and toxicity on T witter. Other researchers have also pointed to \nusing annotators to label training datasets as a way to equip models with the ability to \nparse variations in the speech of a certain language (Bergman & Diab, 2022; Nkemelu \net al., 2022). \nB. Researchers and Funders\nRESEARCH FUNDERS SHOULD INVEST IN SPECIFIC NLP LANGUAGE \nCOMMUNITIES TO KICKSTART THE VIRTUOUS CYCLE OF \nDEVELOPMENT.\nDeveloping NLP capabilities in any language is a cyclical process, and for high resource \nlanguages \u2014 particularly English \u2014\u00a0that cycle is virtuous. When a language has lots \nof clean, human-annotated datasets, researchers and developers are better equipped \nto build models and benchmarks to test models in that language. More models and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ccfa5162-5f0e-4cb3-b860-41124cf6187b": {"__data__": {"id_": "ccfa5162-5f0e-4cb3-b860-41124cf6187b", "embedding": null, "metadata": {"page_label": "34", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7367521-4b50-4ed8-812e-af774415413f", "node_type": "4", "metadata": {"page_label": "34", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7ed9f760ae1ea63604a01c76e0b8e0c9f6ef0ab768cbc3d9adaf251250865cec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n34\nbenchmarks lead to more publications, conferences, and real-world use cases. And \nfinally, increased demand for research and software in a language drives demand for \nmore datasets. For low resource languages, however, the virtuous cycle is hard to \nkickstart. Without tools, annotators, and financial investment earmarked for different \nlanguage communities, NLP researchers cannot create the datasets needed to build \nmodels or benchmarks, and even if they could, they face difficulties publishing or \ngetting attention for their work in popular journals and conferences. The most \nprestigious NLP publications focus disproportionately on English; languages without \ntheir own self-sustaining NLP communities end up to a handful of specialized outlets.\nInvestments into non-English NLP should particularly focus on creating self-sustaining \nscholarly NLP communities, and doing this requires investing in all levels at once. The \ngroups that are best set up to properly allocate these investments are the language- and \ngeography-specific NLP research communities that have cropped up over the years, \nsuch as such as Masakhane, AmericasNLP, ARBML, and others who can convene \npractitioners around common goals to advance the field (Alyafeai & Al-Shaibani, 2020; \nAmericasNLP, 2022; Orife et al., 2020). These communities know what kind of data \nsets should be built, which community actors are needed to properly vet them, and \nwhat kind of competitions and conferences should be run to keep the virtuous cycles \ngoing. One model for how this can work is exemplified by EV ALITA, an event hosted \nby the Italian Association for Computational Linguistics. In it, researchers first submit \ndata sets for new language tasks, such as identifying misogyny or dating documents. \nThen, researchers compete to train models to perform those tasks the best. Finally, \nthose results get published, thus generating interest and attention toward Italian NLP \nand ensuring researchers continue to build tools for the language (Basile et al., 2020).\nPrivate companies can contribute not only by financially supporting these efforts \nbut by sharing more of the non-English datasets they use to train their large language \nmodels, both for transparency and to support research. Large tech companies have \nalready shared the code for training many of their multilingual language models \n\u2014 Meta\u2019s XLM-R and Google\u2019s mBER T are the subjects of most multilingual \nmodel research in publication \u2014 and disclosed the data they train them on \u2014 \nCommonCrawl, and\u00a0Wikipedia and BooksCorpus, respectively. However, the models \nthat Google, Meta, OpenAI, and other large companies use in their products train on \nother, proprietary, language data. Companies should share more of their training data, \nboth for public accountability and to bolster research.\nLarge language models have by and large been built by private companies, but private \nincentives may be at odds with developing these models in safe and equitable ways. \nGovernment investment into non-English large language model research could lead \nto improvements in areas private companies may be underinvesting in (Mazzucato, \n2014). DARPA \u2019s late 2010\u2019s LORELEI project, aimed at spurring research into low", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b083478b-45c6-41ae-aff4-fa4f4f98616f": {"__data__": {"id_": "b083478b-45c6-41ae-aff4-fa4f4f98616f", "embedding": null, "metadata": {"page_label": "35", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6088c380-cb3e-4237-8a4a-9a5ec66b22e0", "node_type": "4", "metadata": {"page_label": "35", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "23795c78489fffb989ea89ed7fb5075d923a7a641cdd3f99ef11a618abf26b2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n35III. Recommendations\nresource languages to improve translation for humanitarian efforts, is a good first step, \nbut further government incentives could help assure that NLP researchers invest in \na broad range of approaches and languages, rather than focus disproportionately on \nEnglish. BigScience\u2019s BLOOM is a good example of how large language models can \nbe developed in the open and with public support. The French government is one of \nmany funders which has allowed BLOOM to remain open to inquiry by other NLP \npractitioners. The multilingual language model was trained using ROOT s, a 1.6TB \nmultilingual dataset that is clearly documented and available for NLP practitioners to \nanalyze (Lauren\u00e7on et al., 2022).\nRESEARCHERS SHOULD FOCUS ON MEASURING AND ADDRESSING \nTHE IMPACTS OF LARGE LANGUAGE MODELS.\nT echnologists understand little about the internal logic of how large language models \noperate and therefore have a difficult time predicting when they make mistakes, \nwhat the effects of these mistakes will be, and how to fix them. Multilinguality only \nexacerbates this problem. Better tools are needed to interrogate large language models, \nparticularly multilingual language models, about why they make the decisions and \nmistakes they do, and how to fix them.\nIn particular, the increased use of multilingual language models has the potential to \nhelp and harm language communities. Enabling greater digital participation amongst \na language community raises something that researchers call the \u201cJanus-face nature \nof digital participation\u201d (NLLB T eam et al., 2022): it allows more to participate and \nbenefit from the digital economy, however, it may also expose more people to the harms \npresent online, often without their consultation and consent (Hao, 2022; T oyama, \n2015). More research on the effects and externalities of the increased use of language \nmodels and specifically multilingual language models must grapple with the impacts \nthese tools have on different linguistic communities, linguistic preservation and \ndiversity efforts, and access to opportunity for all. \nDifferent actors have different roles to play here. Civil society has a role in documenting \nthe impacts of these models and imagining what these \u201cbetter\u201d models should look like. \nThere are many open questions around the types of problems that need automated \nsolutions, what more representative datasets might look like, how to manage the tradeoffs \nbetween languages, how large language models affect linguistic preservation efforts, and \nwhat the rights implications are of using large language models, among other things. \nAcademics and corporate researchers have a role in better defining the contexts and tasks \nthese models hope to address, and developing quantitative and qualitative methods to \nevaluate these desired normative values. And companies that deploy language models \ncan provide researchers more transparency into how their models work, what data they \nare trained on, and in what situations they use them so researchers can better tailor their \nresearch to reflect what is happening in real-world systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3857fe9-6a8f-4890-a920-3930b1e7eb3a": {"__data__": {"id_": "b3857fe9-6a8f-4890-a920-3930b1e7eb3a", "embedding": null, "metadata": {"page_label": "36", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e20b769f-4c85-4fe8-98d3-95fedf992891", "node_type": "4", "metadata": {"page_label": "36", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "defc9332537458e45b4dcdb404aed042ec92b7be48fc7b7ef5657fdd84ee78e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n36\nC. Governments\nGOVERNMENTS SHOULD CAUTION AGAINST USING AUTOMATED \nDECISION-MAKING SYSTEMS THAT RELY ON LARGE LANGUAGE \nMODELS TO MAKE HIGH-STAKES DECISIONS.\nMany governments have deployed or are considering deploying systems that use natural \nlanguage processing technology as part of AI systems to make high-impact decisions, \nsuch as determining immigration status or selecting judicial cases to try (Patel et al., \n2020; Rionda & Mejia, 2021). V endors who build these systems may soon follow the \nlarger industry trend of incorporating large language models since they are relatively \ncheap to build and easy to adapt as requirements change. However, as discussed \nthroughout this paper, large language models are a relatively novel technology that has \ntechnical limitations. These tools pose serious civil liberty concerns that are magnified \nin non-English contexts and when used to make decisions that may affect a person\u2019s \nlivelihood. For instance, if a large language model is used as the basis of an algorithm \nto evaluate affordable housing applications and the text that large language model was \ntrained on exhibits anti-Muslim bias, the resulting affordable housing algorithm may \ndisproportionately deny Muslims\u2019 applications. Relying on large language models to \nmake high-stakes decisions can have outsized, negative impacts on individuals\u2019 lives, \nimpeding safety and access to economic opportunities.\nGovernments should therefore never rely solely on automated systems that incorporate \nlarge language models to make high-risk decision-making areas, such as pretrial risk \nassessment, allocation of social services, and immigration status. Policymakers should \nconsider the impact on rights and access to services when procuring new tools and \nvendors to build these systems and conduct and disclose any assessments conducted \non these systems. They should also be cautious when adopting these systems for \ninformation sharing services, such as chatbots about social services or that provide \nhealthcare information, and test them extensively in every language in which they are \ndeployed, and never use them to entirely replace human intermediaries.\nGOVERNMENTS SHOULD NOT MANDATE OR INADVERTENTLY \nREQUIRE BY LAW THE USE OF AUTOMATED CONTENT ANALYSIS \nSYSTEMS TO DETECT OR REMOVE CONTENT IN ANY LANGUAGE.\nGovernments around the world are increasingly pressuring online service providers to \nlimit content they find to be inaccurate or harmful, such as misinformation related to \nhealth care, or preemptively monitor online speech which may incite violence. Given \nthe scale of content available on social media and other services, this has driven an \ninterest amongst governments to mandate that online service providers use automated \ncontent analysis systems to detect or remove content they deem as \u201cillegal\u201d or harmful \nto their constituents.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e45c91e-2517-4388-aee7-8263ef5f5463": {"__data__": {"id_": "6e45c91e-2517-4388-aee7-8263ef5f5463", "embedding": null, "metadata": {"page_label": "37", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee08117-200b-4e1e-9ca5-f5a333460148", "node_type": "4", "metadata": {"page_label": "37", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "b5348c367976cb5724d754c1b112362f1735e76d74c836d3ea62f1eed06a173e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n37III. Recommendations\nThis is ill-advised. Mandating the use of automated content moderation technologies \nor requiring companies to take down content in a limited time period (effectively \nrequiring the use of automated technologies) opens the door for the overbroad removal \nof speech. Large language models, especially in non-English language contexts, are not \na magical technology that can perfectly distinguish between \u201cgood\u201d and \u201cbad\u201d speech. \nAt best, they are an imprecise technology that fails to understand the context of speech \n\u2014 for instance, when an individual uses a slur versus when a journalist documents \nthe use of a slur by that individual. At worst, they are tools that can be appropriated \nby governments to squash dissent and freedom of expression. Efforts to persuade tech \ncompanies to improve their automated systems, clarify their policies, introduce more \naccountability, and promote parity between languages are all welcome, but requiring \ncompanies to adopt certain technologies is not an effective way to achieve those ends.\nINTERNATIONAL AND MULTILATERAL STANDARDS BODIES, \nREGULATORY AGENCIES, AND OTHERS SHOULD CONVENE \nMULTI-STAKEHOLDER DISCUSSIONS ABOUT STANDARDS AND \nGUARDRAILS FOR THE DEVELOPMENT AND USE OF LARGE \nLANGUAGE MODELS.\nThe norms around when and how multilingual language models should be deployed \nare very much in flux. Those norms so far have mostly been established implicitly by \ntechnology companies in the ways they build and deploy these models, but trends in \nthese norms may be at odds with the public interest. For instance, OpenAI revealed \nsome information about the training data they used for GPT-3 but almost nothing \nabout GPT-4; Open AI co-founder Ilya Sutskever described having shared information \nabout GPT-3\u2019s training data as \u201cjust not wise\u201d and something the company would \nunlikely do again (Vincent, 2023b).\nCompanies should not have a monopoly on the norms around language models. \nGovernmental and nongovernmental\u00a0convening bodies need to organize and push back \nto establish counter-norms that better serve the public\u2019s interests. This field is early on \nenough that these bodies should discuss what positive outcomes even look like. Users \naffected by the deployment of large language models need to be at the table for those \nconversations. Government agencies and multilateral organizations (e.g. the Internet \nEngineering T ask Force, United Nations) can play a coordinating role to get together \nthe relevant stakeholders to come up with such standards.\nThere are also larger questions to reckon with when it comes to the use of large \nlanguage models in non-English contexts. At once, companies are increasingly \ndeploying multilingual language models to bridge the gap between the functionality in \nEnglish and other languages across a myriad of tasks, such as harmful content detection, \nsentiment analysis, and content scanning. However, as we show in this paper, these \nmultilingual systems are relatively new and perform inconsistently across languages.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b3b655a4-f266-49c6-b548-e9e4ac6d88a8": {"__data__": {"id_": "b3b655a4-f266-49c6-b548-e9e4ac6d88a8", "embedding": null, "metadata": {"page_label": "38", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3241ca7d-cf4f-498c-b6ff-fc8ee4a5fc4f", "node_type": "4", "metadata": {"page_label": "38", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e9554e4c3ded8e2a027bbeda925fba3703e5ad72fd47d28283fddc0fc5fb8f16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lost in Translation\nCDT Research\n38\nIf deployed prematurely and without guardrails, these models pose real risks to \nindividuals around the world and in particular their ability to express themselves freely. \nThese risks have the potential to compound existing challenges in the information \nenvironment for individuals in W estern democracies where there are real vacuums of \navailable information in languages other than English and in countries in the Global \nSouth where there are already real threats to the free expression and exchange of \ninformation posed by majoritarian and institutional powers (Golebiewski & boyd, \n2018). Alternatively, companies may decide to only roll out systems that have been \nfine-tuned for English and wait until there is enough data and tooling available for non-\nEnglish language tools \u2014 something that will take an enormous amount of financial \ninvestment, time, effort, and rare consensus \u2014 further entrenching the digital divide \nand Anglocentrism present online. Both scenarios are lose-lose for all speakers on the \nweb. This is a wicked problem and the current incentives are at play to build bigger \nmodels, and with more languages. Multi-stakeholder bodies are much better positioned \nthan companies to determine when the risks associated with building larger, more \nmultilingual language models are worth taking.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c47b5631-fd71-4912-b7ff-2b7f86c6c8b6": {"__data__": {"id_": "c47b5631-fd71-4912-b7ff-2b7f86c6c8b6", "embedding": null, "metadata": {"page_label": "39", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c885dd46-1885-4680-b45f-729dc8cbd4c8", "node_type": "4", "metadata": {"page_label": "39", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a83bdc0636baf69948c05c8d9481abf3701ba767d1c7b38fa8abc3794a0ff9b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Non-English Content Analysis\n39\nWorks Cited\nAbid, A., Farooqi, M., & Zou, J. (2021). Large language models associate Muslims with violence. Nature Machine \nIntelligence, 3(6), Article 6. [perma.cc/HK4B-3AAQ]\nACL. (2021, August 3). ACL 2022 Theme Track: \u201cLanguage Diversity: from Low-Resource to Endangered \nLanguages.\u201d ACL. [perma.cc/F2YW-QZBP]\nACL Rolling Review Dashboard. (2022). Papers Mentioning >0 Languages. [perma.cc/EQU9-5CWQ]\nAgerri, R., Vicente, I. S., Campos, J. A., Barrena, A., Saralegi, X., Soroa, A., & Agirre, E. (2020). Give your T ext \nRepresentation Models some Love: The Case for Basque. Proceedings of the 12th Conference on Language \nResources and Evaluation, 4781\u20134788. [perma.cc/R2DA-GGQZ]\nAlyafeai, Z., & Al-Shaibani, M. (2020). ARBML: Democratizing Arabic Natural Language Processing T ools. \nProceedings of Second W orkshop for NLP Open Source Software (NLP-OSS), 8\u201313. [perma.cc/4TFY-E9EJ]\nAmer, M. (2022, July 13). Large Language Models and Where to Use Them: Part 2. Cohere. [perma.cc/CR T5-\nHDX8]\nAmericasNLP. (2022, December 7). Second W orkshop on NLP for Indigenous Languages of the Americas \n(AmericasNLP). [perma.cc/SC88-9WGF]\nAmrute, S., Singh, R., & Guzm\u00e1n, R. L. (2022). A Primer on AI in/from the Majority W orld. Data & Society. \n[perma.cc/SR8B-J2L9]\nAntoun, W ., Baly, F., & Hajj, H. (2020). AraBER T: T ransformer-based Model for Arabic Language \nUnderstanding. Proceedings of the 4th W orkshop on Open-Source Arabic Corpora and Processing Tools, with a \nShared Task on Offensive Language Detection, 9\u201315. [perma.cc/X5VJ-JKXQ]\nArtetxe, M., Labaka, G., & Agirre, E. (2020). T ranslation Artifacts in Cross-lingual T ransfer Learning. Proceedings \nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 7674\u20137684. \n[perma.cc/MZY5-DL83]\nArtetxe, M., Ruder, S., & Y ogatama, D. (2020). On the Cross-lingual T ransferability of Monolingual \nRepresentations. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, \n4623\u20134637. [perma.cc/7WMN-5QPR]\nArtetxe, M., & Schwenk, H. (2019). Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual \nT ransfer and Beyond. Transactions of the Association for Computational Linguistics, 7, 597\u2013610. [perma.cc/\nLB6R-GH9K]\nArticle 19. (2022). Bridging the Gap: Local voices in content moderation. Bosnia and Herzegovina. [perma.cc/ ASU5-\nST4N]\nAvaaz. (2019). Megaphone for Hate: Disinformation and Hate Speech on Facebook During Assam\u2019s Citizenship \nCount. Avaaz. [perma.cc/5MXS-7P7N]\nLost in Translation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "acc9b0ba-c380-48fd-bd61-e3e633059e6b": {"__data__": {"id_": "acc9b0ba-c380-48fd-bd61-e3e633059e6b", "embedding": null, "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c82b2a01-42ac-4a2d-bae2-416181fe7796", "node_type": "4", "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "78352c80974667371de05e53376d8c4a9d021b725674813f5a5ee9fa8078bcf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cfb6efe-aa32-41c5-b43b-eaa9077e59d7", "node_type": "1", "metadata": {}, "hash": "728081f0022f8f60fa1358ee21c81476a88e295ec26c250f90307aea4d6442a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "40\nLost in Translation\nCDT Research\nBasile, V ., Maro, M. D., Croce, D., & Passaro, L. (2020, December 17). EV ALITA 2020: Overview of the 7th \nEvaluation Campaign of Natural Language Processing and Speech Tools for Italian. Seventh Evaluation \nCampaign of Natural Language Processing and Speech T ools for Italian, Online. [perma.cc/76EK-EJQ8]\nBelloni, M. (2021, December 8). Multilingual message content moderation at scale. Bumble Tech. [perma.cc/\nRL2A-L2BD]\nBender, E. (2019, September 15). The #BenderRule: On Naming the Languages W e Study and Why It Matters. \nThe Gradient. [perma.cc/J3ZM-A5UP]\nBender, E., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can \nLanguage Models Be T oo Big? \ud83e\udd9c . Proceedings of the 2021 ACM Conference on Fairness, Accountability, and \nTransparency, 610\u2013623. [perma.cc/3KLC-TBUY]\nBender, E., & Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of \nData. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185\u20135198. \n[perma.cc/TN3W-5NTC]\nBergman, A., & Diab, M. (2022). T owards Responsible Natural Language Annotation for the V arieties of Arabic. \nFindings of the Association for Computational Linguistics: ACL 2022, 364\u2013371. [perma.cc/Q37M-8F2Y]\nBigScience W orkshop, Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. \nS., Yvon, F., Gall\u00e9, M., T ow, J., Rush, A. M., Biderman, S., W ebson, A., Ammanamanchi, P. S., W ang, T., \nSagot, B., Muennighoff, N., \u2026 W olf, T. (2023). BLOOM: A 176B-Parameter Open-Access Multilingual \nLanguage Model (arXiv:2211.05100). arXiv. [perma.cc/2K4Z-F5U7]\nBirhane, A., Kalluri, P., Card, D., Agnew, W ., Dotan, R., & Bao, M. (2022). The V alues Encoded in Machine \nLearning Research. 2022 ACM Conference on Fairness, Accountability, and Transparency, 173\u2013184. \n[perma.cc/9GNB-JHQ5]\nBirhane, A., & Prabhu, V . U. (2021). Large image datasets: A pyrrhic win for computer vision? 2021 IEEE Winter \nConference on Applications of Computer Vision, 1536\u20131546. [perma.cc/Q8LP-THYK]\nBizzoni, Y., Juzek, T. S., Espa\u00f1a-Bonet, C., Dutta Chowdhury, K., van Genabith, J., & T eich, E. (2020). How \nHuman is Machine T ranslationese? Comparing Human and Machine T ranslations of T ext and Speech. \nProceedings of the 17th International Conference on Spoken Language Translation, 280\u2013290. [perma.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1cfb6efe-aa32-41c5-b43b-eaa9077e59d7": {"__data__": {"id_": "1cfb6efe-aa32-41c5-b43b-eaa9077e59d7", "embedding": null, "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c82b2a01-42ac-4a2d-bae2-416181fe7796", "node_type": "4", "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "78352c80974667371de05e53376d8c4a9d021b725674813f5a5ee9fa8078bcf0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acc9b0ba-c380-48fd-bd61-e3e633059e6b", "node_type": "1", "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "32dee09a4cd11b8567c2c6f086d0a71a90c0fdf7a9de3eee4d38372190052ed6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[perma.cc/Q8LP-THYK]\nBizzoni, Y., Juzek, T. S., Espa\u00f1a-Bonet, C., Dutta Chowdhury, K., van Genabith, J., & T eich, E. (2020). How \nHuman is Machine T ranslationese? Comparing Human and Machine T ranslations of T ext and Speech. \nProceedings of the 17th International Conference on Spoken Language Translation, 280\u2013290. [perma.\ncc/4DTZ-DVKC]\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, \nA., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., \nDavis, J. Q., Demszky, D., \u2026 Liang, P. (2021). On the Opportunities and Risks of Foundation Models. \nStanford Center for Research on Foundation Models. [perma.cc/3TKJ-UM2F]\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., \nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., \nWu, J., Winter, C., \u2026 Amodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural \nInformation Processing Systems, 33, 1877\u20131901. [perma.cc/7EES-WDAB]\nCahyawijaya, S., Lovenia, H., Aji, A. F., Winata, G. I., Wilie, B., Mahendra, R., Wibisono, C., Romadhony, A., \nVincentio, K., Koto, F., Santoso, J., Moeljadi, D., Wirawan, C., Hudi, F., Parmonangan, I. H., Alfina, \nI., Wicaksono, M. S., Putra, I. F., Rahmadani, S., \u2026 Purwarianti, A. (2022). NusaCrowd: Open Source \nInitiative for Indonesian NLP Resources (arXiv:2212.09648). arXiv. [perma.cc/UQ3Y-4LKW]", "mimetype": "text/plain", "start_char_idx": 2093, "end_char_idx": 3627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90266fdc-f90f-44eb-9ff8-d1babe7026c9": {"__data__": {"id_": "90266fdc-f90f-44eb-9ff8-d1babe7026c9", "embedding": null, "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888883f-ca58-44a8-836f-232acbb45464", "node_type": "4", "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d122af76292748729f65bac570e42ed390fc6518fffaf26f4526c80912ca42c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01dcc353-0a12-4db6-abc7-0cae833ebdf8", "node_type": "1", "metadata": {}, "hash": "41aca32e15ce3554fd29408e262bedce8b4197363a05b9fb234fadf04fba8887", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "41\nLarge Language Models in Non-English Content Analysis\nWorks Cited\nCallahan, E. S., & Herring, S. C. (2011). Cultural bias in Wikipedia content on famous persons. Journal of the \nAmerican Society for Information Science and Technology, 62(10), 1899\u20131915. [perma.cc/2S8K-YEJK]\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., T ramer, F., & Zhang, C. (2023, February 1). Quantifying \nMemorization Across Neural Language Models. The Eleventh International Conference on Learning \nRepresentations. [perma.cc/678U-9PAQ]\nCarlini, N., T ramer, F., W allace, E., Jagielski, M., Herbert-V oss, A., Lee, K., Roberts, A., Brown, T., Song, D., \nErlingsson, U., Oprea, A., & Raffel, C. (2021). Extracting Training Data from Large Language Models \n(arXiv:2012.07805). arXiv. [perma.cc/58MA-VWRZ]\nCaswell, I., Breiner, T., van Esch, D., & Bapna, A. (2020). Language ID in the Wild: Unexpected Challenges on \nthe Path to a Thousand-Language W eb T ext Corpus. Proceedings of the 28th International Conference on \nComputational Linguistics, 6588\u20136608. [perma.cc/8RFD-DTUK]\nChi, E. A., Hewitt, J., & Manning, C. D. (2020). Finding Universal Grammatical Relations in Multilingual BER T. \nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5564\u20135577. \n[perma.cc/8LNR-VNY9]\nChoi, H., Kim, J., Joe, S., Min, S., & Gwon, Y. (2021). Analyzing Zero-shot Cross-lingual Transfer in Supervised \nNLP Tasks (arXiv:2101.10649). arXiv. [perma.cc/NEB9-8THZ]\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W ., Sutton, C., \nGehrmann, S., Schuh, P., Shi, K., T svyashchenko, S., Maynez, J., Rao, A., Barnes, P., T ay, Y., Shazeer, N., \nPrabhakaran, V ., \u2026 Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways. Google Research. \n[perma.cc/NZ7N-6GPB]\nChristian, J. (2018, July 20). Why Is Google T ranslate Spitting Out Sinister Religious Prophecies? Vice. [perma.\ncc/8YQU-NUFM]\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V ., W enzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., \nZettlemoyer, L., & Stoyanov, V . (2020). Unsupervised Cross-lingual Representation Learning at Scale. \nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8440\u20138451. \n[perma.cc/2MP6-9W3J]\nConneau, A., & Lample, G. (2019). Cross-lingual Language Model Pretraining. Advances in Neural Information \nProcessing Systems, 32. [perma.cc/N7EE-JM83]\nConneau, A., Wu, S., Li, H., Zettlemoyer, L., & Stoyanov, V . (2020). Emerging Cross-lingual Structure in \nPretrained Language Models. Proceedings of the 58th Annual Meeting of the Association for Computational \nLinguistics, 6022\u20136034. [perma.cc/3NHR-G7Y4]\nCorradi, A. (2017, April 25).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01dcc353-0a12-4db6-abc7-0cae833ebdf8": {"__data__": {"id_": "01dcc353-0a12-4db6-abc7-0cae833ebdf8", "embedding": null, "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0888883f-ca58-44a8-836f-232acbb45464", "node_type": "4", "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d122af76292748729f65bac570e42ed390fc6518fffaf26f4526c80912ca42c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90266fdc-f90f-44eb-9ff8-d1babe7026c9", "node_type": "1", "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e7a8731f0f564b0f303d4a3612bc11872261f4c9087763dca6e821e90a613515", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advances in Neural Information \nProcessing Systems, 32. [perma.cc/N7EE-JM83]\nConneau, A., Wu, S., Li, H., Zettlemoyer, L., & Stoyanov, V . (2020). Emerging Cross-lingual Structure in \nPretrained Language Models. Proceedings of the 58th Annual Meeting of the Association for Computational \nLinguistics, 6022\u20136034. [perma.cc/3NHR-G7Y4]\nCorradi, A. (2017, April 25). The Linguistic Colonialism of English. Brown Political Review. [perma.cc/5M3M-\n9EMN]\nCorvey, W . (2014). Low Resource Languages for Emergent Incidents. Defense Advanced Research Projects Agency. \n[perma.cc/4FDR-M3YC]\nCrawford, K. (2021). Atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Y ale University \nPress.", "mimetype": "text/plain", "start_char_idx": 2350, "end_char_idx": 3058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3fccce86-a7ff-4ee2-b29d-6748f0fa3cf9": {"__data__": {"id_": "3fccce86-a7ff-4ee2-b29d-6748f0fa3cf9", "embedding": null, "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94b24187-79c0-4578-b7b9-ed0172f76e2b", "node_type": "4", "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d8ac0bbb4ed31c67cd661b863b0bd757be9a249968093de054f3de5cea56705c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "906b1dc3-1307-4e75-b83c-b283d3055292", "node_type": "1", "metadata": {}, "hash": "5d4344af85926f27fd54ce455514179019f4ed3704a0c7c65affbfb863be6cea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "42\nLost in Translation\nCDT Research\nde V arda, A. G., & Marelli, M. (2023). Data-driven Cross-lingual Syntax: An Agreement Study with Massively \nMultilingual Models. Computational Linguistics, 1\u201339. [perma.cc/7LQP-EEBQ]\nde Vries, W ., van Cranenburgh, A., Bisazza, A., Caselli, T., van Noord, G., & Nissim, M. (2019). BERTje: A Dutch \nBERT Model (arXiv:1912.09582). arXiv. [perma.cc/MGU3-WPXR]\nDevlin, J., Chang, M.-W ., Lee, K., & T outanova, K. (2019). BER T: Pre-training of Deep Bidirectional \nT ransformers for Language Understanding. Proceedings of the 2019 Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language Technologies, V olume 1 (Long \nand Short Papers), 4171\u20134186. [perma.cc/E46R-UYDE]\nDodge, J., Sap, M., Marasovi\u0107, A., Agnew, W ., Ilharco, G., Groeneveld, D., Mitchell, M., & Gardner, M. (2021). \nDocumenting Large W ebtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Proceedings \nof the 2021 Conference on Empirical Methods in Natural Language Processing, 1286\u20131305. [perma.\ncc/3GC6-UEWJ]\nDuarte, N., Llans\u00f3, E., & Loup, A. C. (2017). Mixed Messages? The Limits of Automated Social Media Content \nAnalysis. Center for Democracy & T echnology. [perma.cc/9BRH-5ZZN]\nDulhanty, C., Deglint, J. L., Daya, I. B., & W ong, A. (2019, November 26). Taking a Stance on Fake News: Towards \nAutomatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance \nDetection. NeurIPS 2019, V ancouver. [perma.cc/P5JD-5AD9]\nEbers, M., Poncib\u00f2, C., & Zou, M. (Eds.). (2022). Contracting and Contract Law in the Age of Artificial \nIntelligence. Hart Publishing. [perma.cc/G4XR-VYNL]\nEronen, J., Ptaszynski, M., & Masui, F. (2023). Zero-shot cross-lingual transfer language selection using linguistic \nsimilarity. Information Processing & Management, 60(3), 103250. [perma.cc/S78N-C9MR]\nEthnologue. (2023a). Assamese. Ethnologue, Languages of the W orld. [perma.cc/BE78-H3PN]\nEthnologue. (2023b). Statistics. Ethnologue, Languages of the W orld. [perma.cc/H27U-44TK]\nGanguli, D., Hernandez, D., Lovitt, L., DasSarma, N., Henighan, T., Jones, A., Joseph, N., Kernion, J., Mann, \nB., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Elhage, N., Showk, S. E., Fort, S., Hatfield-Dodds, \nZ., Johnston, S., \u2026 Clark, J. (2022). Predictability and Surprise in Large Generative Models. 2022 ACM \nConference on Fairness, Accountability, and Transparency, 1747\u20131764. [perma.cc/C8YH-6LMA]\nGolebiewski, M., & boyd, danah. (2018). Data V oids: Where Missing Data Can Easily Be Exploited. Data & \nSociety. [perma.cc/HE5A-7QTJ]\nG\u00f3ngora, S., Giossa, N., & Chiruzzo, L. (2021). Experiments on a Guarani Corpus of News and Social Media.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "906b1dc3-1307-4e75-b83c-b283d3055292": {"__data__": {"id_": "906b1dc3-1307-4e75-b83c-b283d3055292", "embedding": null, "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94b24187-79c0-4578-b7b9-ed0172f76e2b", "node_type": "4", "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d8ac0bbb4ed31c67cd661b863b0bd757be9a249968093de054f3de5cea56705c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fccce86-a7ff-4ee2-b29d-6748f0fa3cf9", "node_type": "1", "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a509eb8c0786e41d8ac44fbb851111f94622b05e90113fff631e876cd5af814f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[perma.cc/C8YH-6LMA]\nGolebiewski, M., & boyd, danah. (2018). Data V oids: Where Missing Data Can Easily Be Exploited. Data & \nSociety. [perma.cc/HE5A-7QTJ]\nG\u00f3ngora, S., Giossa, N., & Chiruzzo, L. (2021). Experiments on a Guarani Corpus of News and Social Media. \nProceedings of the First W orkshop on Natural Language Processing for Indigenous Languages of the \nAmericas, 153\u2013158. [perma.cc/N6S5-4PGN]\nGrant-Chapman, H., Laird, E., & V enzke, C. (2021). Student Activity Monitoring Software Research Insights and \nRecommendations. Center for Democracy & T echnology. [perma.cc/FY8G-WC2P]\nGr\u00f6ndahl, T., Pajola, L., Juuti, M., Conti, M., & Asokan, N. (2018). All Y ou Need is \u201cLove\u201d: Evading Hate Speech \nDetection. Proceedings of the 11th ACM W orkshop on Artificial Intelligence and Security, 2\u201312. [perma.cc/\nT6P5-FRX5]", "mimetype": "text/plain", "start_char_idx": 2458, "end_char_idx": 3278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37aa9c0c-540b-4534-aa85-327a648b5650": {"__data__": {"id_": "37aa9c0c-540b-4534-aa85-327a648b5650", "embedding": null, "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8263fa6-df96-418b-81f7-c988498c8f38", "node_type": "4", "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "5708a4949f868b51afd441226f7a7ae7d46c73da254340dbfc1716e590af8c2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06156c45-8aae-4d99-ba85-1e4530cdb14e", "node_type": "1", "metadata": {}, "hash": "c4637ac30bee73774ba1ae8383d4966e64bf768ce619a71f1021a44cbb76b869", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "43\nLarge Language Models in Non-English Content Analysis\nWorks Cited\nHao, K. (2022, April 22). A new vision of artificial intelligence for the people. MIT T echnology Review. [perma.\ncc/54U3-KU5C]\nHeikkil\u00e4, M. (2022, November 14). W e\u2019re getting a better idea of AI\u2019s true carbon footprint. MIT T echnology \nReview. [perma.cc/8PWZ-ESJK]\nHutchinson, B., Prabhakaran, V ., Denton, E., W ebster, K., Zhong, Y., & Denuyl, S. (2020). Social Biases in NLP \nModels as Barriers for Persons with Disabilities. Proceedings of the 58th Annual Meeting of the Association \nfor Computational Linguistics, 5491\u20135501. [perma.cc/8FGR-P3FA]\nIzsak, P., Berchansky, M., & Levy, O. (2021). How to T rain BER T with an Academic Budget. Proceedings of the \n2021 Conference on Empirical Methods in Natural Language Processing, 10644\u201310652. [perma.cc/8MPG-\nW2QE]\nJoshi, P., Santy, S., Budhiraja, A., Bali, K., & Choudhury, M. (2020). The State and Fate of Linguistic Diversity and \nInclusion in the NLP W orld. Proceedings of the 58th Annual Meeting of the Association for Computational \nLinguistics, 6282\u20136293. [perma.cc/82HQ-EH65]\nKaack, L. H., Donti, P. L., Strubell, E., Kamiya, G., Creutzig, F., & Rolnick, D. (2022). Aligning artificial \nintelligence with climate change mitigation. Nature Climate Change, 12(6), Article 6. [perma.cc/7C4S-\nX2LH]\nKhan, M., & Hanna, A. (2023). The Subjects and Stages of AI Dataset Development: A Framework for Dataset \nAccountability. Ohio State Technology Law Journal, 19. [perma.cc/XLG3-AP2J]\nKinchin, N., & Mougouei, D. (2022). What Can Artificial Intelligence Do for Refugee Status Determination? A \nProposal for Removing Subjective Fear. International Journal of Refugee Law. [perma.cc/3KER-DZ5R]\nKoehn, P., & Knowles, R. (2017). Six Challenges for Neural Machine T ranslation. Proceedings of the First \nW orkshop on Neural Machine Translation, 28\u201339. [perma.cc/9WSQ-HQJY]\nKornai, A. (2013). Digital Language Death. PLOS ONE, 8(10), e77056. [perma.cc/MMZ8-C9VH]\nKreutzer, J., Caswell, I., W ang, L., W ahab, A., van Esch, D., Ulzii-Orshikh, N., T apo, A., Subramani, N., Sokolov, \nA., Sikasote, C., Setyawan, M., Sarin, S., Samb, S., Sagot, B., Rivera, C., Rios, A., Papadimitriou, I., Osei, \nS., Suarez, P. O., \u2026 Adeyemi, M. (2022). Quality at a Glance: An Audit of W eb-Crawled Multilingual \nDatasets. Transactions of the Association for Computational Linguistics, 10, 50\u201372. [perma.cc/YZ7B-\nQ7PN]\nKublik, V . (n.d.). EU/US Copyright Law and Implications on ML Training Data. V alohai. [perma.cc/LD3Z-\nR VW7]\nKupfer, M., & Muyumba, J. (2022). Language & Coloniality: Non-Dominant Languages in the Digital Landscape. \nPollicy.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06156c45-8aae-4d99-ba85-1e4530cdb14e": {"__data__": {"id_": "06156c45-8aae-4d99-ba85-1e4530cdb14e", "embedding": null, "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8263fa6-df96-418b-81f7-c988498c8f38", "node_type": "4", "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "5708a4949f868b51afd441226f7a7ae7d46c73da254340dbfc1716e590af8c2d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37aa9c0c-540b-4534-aa85-327a648b5650", "node_type": "1", "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "d805d6d53dbfdc14dabf191c0b2f92cc55eb72ef77dc22db49173cbf8493d39b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Transactions of the Association for Computational Linguistics, 10, 50\u201372. [perma.cc/YZ7B-\nQ7PN]\nKublik, V . (n.d.). EU/US Copyright Law and Implications on ML Training Data. V alohai. [perma.cc/LD3Z-\nR VW7]\nKupfer, M., & Muyumba, J. (2022). Language & Coloniality: Non-Dominant Languages in the Digital Landscape. \nPollicy. [perma.cc/PM8N-Y9YW]\nLauren\u00e7on, H., Saulnier, L., W ang, T., Akiki, C., Moral, A. V . del, Scao, T. L., W erra, L. V ., Mou, C., Ponferrada, \nE. G., Nguyen, H., Frohberg, J., \u0160a\u0161ko, M., Lhoest, Q., McMillan-Major, A., Dupont, G., Biderman, \nS., Rogers, A., Allal, L. B., T oni, F. D., \u2026 Jernite, Y. (2022, October 31). The BigScience ROOTS Corpus: \nA 1.6TB Composite Multilingual Dataset. Thirty-sixth Conference on Neural Information Processing \nSystems Datasets and Benchmarks T rack. [perma.cc/QS7B-YNYU]", "mimetype": "text/plain", "start_char_idx": 2323, "end_char_idx": 3154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d264ac55-67cf-4a98-a719-e9c0bab30979": {"__data__": {"id_": "d264ac55-67cf-4a98-a719-e9c0bab30979", "embedding": null, "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebd0a0b9-efa4-4618-aba2-24c8ca40f252", "node_type": "4", "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "f6a7efd85301611fdfed5b5dea77767dfba9b050d56c3688a20e2ad14d6dc2e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e3ad696-6851-4ea8-9361-c73936ebf239", "node_type": "1", "metadata": {}, "hash": "8200bb41813c628e80bdce7ecdc4a7d918dca04f8d7bf61315426d622c882422", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "44\nLost in Translation\nCDT Research\nLauscher, A., Ravishankar, V ., Vuli\u0107, I., & Glava\u0161, G. (2020). From Zero to Hero: On the Limitations of Zero-\nShot Language T ransfer with Multilingual T ransformers. Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP), 4483\u20134499. [perma.cc/ZJ3R-95JM]\nLees, A., Sorensen, J., & Kivlichan, I. (2020). Jigsaw @ AMI and HaSpeeDe2: Fine-Tuning a Pre-T rained \nComment-Domain BER T Model. In V . Basile, D. Croce, M. Maro, & L. C. Passaro (Eds.), EV ALITA \nEvaluation of NLP and Speech Tools for Italian\u2014December 17th, 2020 (pp. 40\u201347). Accademia University \nPress. [perma.cc/9D4M-RSCL]\nLees, A., T ran, V . Q., T ay, Y., Sorensen, J., Gupta, J., Metzler, D., & V asserman, L. (2022). A New Generation \nof Perspective API: Efficient Multilingual Character-level T ransformers. Proceedings of the 28th ACM \nSIGKDD Conference on Knowledge Discovery and Data Mining, 3197\u20133207. [perma.cc/5K82-WG8J]\nLibovick\u00fd, J., Rosa, R., & Fraser, A. (2019). How Language-Neutral is Multilingual BERT? (arXiv:1911.03310). \narXiv. [perma.cc/96R W-WXBL]\nLin, X. V ., Mihaylov, T., Artetxe, M., W ang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., \nPasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V ., O\u2019Horo, B., W ang, J., Zettlemoyer, L., Kozareva, Z., \nDiab, M., \u2026 Li, X. (2022). Few-shot Learning with Multilingual Generative Language Models. Proceedings \nof the 2022 Conference on Empirical Methods in Natural Language Processing, 9019\u20139052. [perma.\ncc/5QY9-97G5]\nLokhov, I. (2021, January 28). Why are there so many Wikipedia articles in Swedish and Cebuano? Datawrapper \nBlog. [perma.cc/WDL2-TF53]\nLuccioni, A., & Viviano, J. (2021). What\u2019s in the Box? An Analysis of Undesirable Content in the Common Crawl \nCorpus. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \n11th International Joint Conference on Natural Language Processing (V olume 2: Short Papers), 182\u2013189. \n[perma.cc/2QQU-NRPB]\nLunden, I. (2023, March 14). Nabla, a digital health startup, launches Copilot, using GPT-3 to turn patient \nconversations into action. TechCrunch. [perma.cc/MK55-SV54]\nMartin, G., Mswahili, M. E., Jeong, Y.-S., & W oo, J. (2022). SwahBER T: Language Model of Swahili. Proceedings \nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, 303\u2013313. [perma.cc/3ZP6-V6AJ]\nMartin, L., Muller, B., Su\u00e1rez, P. J. O., Dupont, Y., Romary, L., de la Clergerie, \u00c9. V ., Seddah, D., & Sagot, B. \n(2020). CamemBER T: A T asty French Language Model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e3ad696-6851-4ea8-9361-c73936ebf239": {"__data__": {"id_": "0e3ad696-6851-4ea8-9361-c73936ebf239", "embedding": null, "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebd0a0b9-efa4-4618-aba2-24c8ca40f252", "node_type": "4", "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "f6a7efd85301611fdfed5b5dea77767dfba9b050d56c3688a20e2ad14d6dc2e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d264ac55-67cf-4a98-a719-e9c0bab30979", "node_type": "1", "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "a00814671e7c10faa2e3732b3c293f1caf631d18cfcb11d280c29032c2c413d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proceedings \nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, 303\u2013313. [perma.cc/3ZP6-V6AJ]\nMartin, L., Muller, B., Su\u00e1rez, P. J. O., Dupont, Y., Romary, L., de la Clergerie, \u00c9. V ., Seddah, D., & Sagot, B. \n(2020). CamemBER T: A T asty French Language Model. Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics, 7203\u20137219. [perma.cc/76EU-4L TM]\nMasakhane. (n.d.). Masakhane. Retrieved December 21, 2022. [perma.cc/ A7SA-ALPM]\nMaundu, C. (2023, February 21). How language denies people access to public information. Nation. [perma.\ncc/8C4B-JS3Y]\nMazzucato, M. (2014). The entrepreneurial state: Debunking public vs. private sector myths (Revised edition). \nAnthem Press.\nMeta AI. (2019, November 7). XLM-R: State-of-the-art cross-lingual understanding through self-supervision. \nMeta AI. [perma.cc/J55N-4MV5]", "mimetype": "text/plain", "start_char_idx": 2297, "end_char_idx": 3226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "718dd368-afdc-4c7b-9def-9871b0d2300f": {"__data__": {"id_": "718dd368-afdc-4c7b-9def-9871b0d2300f", "embedding": null, "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c692eb65-ba63-430c-9f10-7bcd4c88b062", "node_type": "4", "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "89a2c539a2ec0a2fdee1d914b6b84102096cbc7b84e8d207f47a5aea49dc44e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4457c319-ccfc-43fd-a730-5489ce3c62bb", "node_type": "1", "metadata": {}, "hash": "3e0ca552182aa327033214e731ad3b0561b42750e9c4af6bbae33b659dc993bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "45\nLarge Language Models in Non-English Content Analysis\nWorks Cited\nMicallef, K., Gatt, A., T anti, M., van der Plas, L., & Borg, C. (2022). Pre-training Data Quality and Quantity for a \nLow-Resource Language: New Corpus and BER T Models for Maltese. Proceedings of the Third W orkshop \non Deep Learning for Low-Resource Natural Language Processing, 90\u2013101. [perma.cc/QY8V-9Q6H]\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, September 6). Efficient Estimation of W ord Representations \nin V ector Space. International Conference on Learning Representations. [perma.cc/T869-PDX4]\nMuller, B., Anastasopoulos, A., Sagot, B., & Seddah, D. (2021). When Being Unseen from mBER T is just the \nBeginning: Handling New Languages With Multilingual Language Models. Proceedings of the 2021 \nConference of the North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies, 448\u2013462. [perma.cc/J5MH-QDW3]\nNadkarni, P. M., Ohno-Machado, L., & Chapman, W . W . (2011). Natural language processing: An introduction. \nJournal of the American Medical Informatics Association : JAMIA, 18(5), 544\u2013551. [perma.cc/72PK-\nUGK9]\nNekoto, W ., Marivate, V ., Matsila, T., Fasubaa, T., Fagbohungbe, T., Akinola, S. O., Muhammad, S., Kabongo \nKabenamualu, S., Osei, S., Sackey, F., Niyongabo, R. A., Macharm, R., Ogayo, P., Ahia, O., Berhe, M. \nM., Adeyemi, M., Mokgesi-Selinga, M., Okegbemi, L., Martinus, L., \u2026 Bashir, A. (2020). Participatory \nResearch for Low-resourced Machine T ranslation: A Case Study in African Languages. Findings of the \nAssociation for Computational Linguistics: EMNLP 2020, 2144\u20132160. [perma.cc/5BVM-LUMM]\nNguer, E. M., Lo, A., Dione, C. M. B., Ba, S. O., & Lo, M. (2020). SENCORPUS: A French-W olof Parallel \nCorpus. Proceedings of the Twelfth Language Resources and Evaluation Conference, 2803\u20132811. [perma.cc/\nNBE7-QCZW]\nNguyen, T. (2020, November 27). Why fake news is so hard to combat in Asian American communities. V ox. \n[perma.cc/45GF-UUEC]\nNicholas, G. (2020). Explaining Algorithmic Decisions. Georgetown Law Technology Review, 4(711), 20. [perma.\ncc/UD7D-HF6F]\nNicholas, G. (2022). Shedding Light on Shadowbanning. Center for Democracy & Technology. [perma.cc/D2TS-\nY92D]\nNkemelu, D., Shah, H., Essa, I., & Best, M. L. (2023). Tackling Hate Speech in Low-resource Languages with \nContext Experts. International Conference on Information & Communication T echnologies and \nDevelopment, W ashington, USA. [perma.cc/5QK7-GTMR]\nNLLB T eam, Costa-juss\u00e0, M. R., Cross, J., \u00c7elebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, \nJ., Licht, D., Maillard, J., Sun, A., W ang, S., W enzek, G., Y oungblood, A., Akula, B., Barrault, L., Gonzalez, \nG. M., Hansanti, P., \u2026 W ang, J. (2022).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4457c319-ccfc-43fd-a730-5489ce3c62bb": {"__data__": {"id_": "4457c319-ccfc-43fd-a730-5489ce3c62bb", "embedding": null, "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c692eb65-ba63-430c-9f10-7bcd4c88b062", "node_type": "4", "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "89a2c539a2ec0a2fdee1d914b6b84102096cbc7b84e8d207f47a5aea49dc44e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "718dd368-afdc-4c7b-9def-9871b0d2300f", "node_type": "1", "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "901c6e1f59ee881e108605e22f44923c27bfcbf15889f0030cafe1ccd9eda463", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2022). No Language Left Behind: Scaling Human-Centered Machine \nTranslation (arXiv:2207.04672). arXiv. [perma.cc/LZH5-DMUA]\nOkerlund, J., Klasky, E., Middha, A., Kim, S., Rosenfeld, H., Kleinman, M., & Parthasarathy, S. (2022). What\u2019s \nin the Chatterbox? Large Language Models, Why They Matter, and What W e Should Do About Them. \nUniversity of Michigan. [perma.cc/8SXE-RSYE]\nOpenAI. (2023). GPT-4 Technical Report (arXiv:2303.08774). arXiv. [perma.cc/6ACB-LZYC]", "mimetype": "text/plain", "start_char_idx": 2749, "end_char_idx": 3212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c907e78c-8170-46e6-8b31-6d48a664c96b": {"__data__": {"id_": "c907e78c-8170-46e6-8b31-6d48a664c96b", "embedding": null, "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d500a24-754d-419a-96c6-39f8e8ff868d", "node_type": "4", "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "cc6674c2b5c6b8f30ddd9de92258f5e170cddcb34b2b5ac3077b5f7f4b2b0538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e92e0471-6548-4fb9-ab08-f274275ab898", "node_type": "1", "metadata": {}, "hash": "bb6c6c2a95b9f2b8cc4497301fa7653d261742fb4bdfa0a7fcced4841a8cd21c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "46\nLost in Translation\nCDT Research\nOrife, I., Kreutzer, J., Sibanda, B., Whitenack, D., Siminyu, K., Martinus, L., Ali, J. T., Abbott, J., Marivate, V ., \nKabongo, S., Meressa, M., Murhabazi, E., Ahia, O., van Biljon, E., Ramkilowan, A., Akinfaderin, A., \n\u00d6ktem, A., Akin, W ., Kioko, G., \u2026 Bashir, A. (2020). Masakhane\u2014Machine Translation For Africa \n(arXiv:2003.11529). arXiv. [perma.cc/84Z4-S7AZ]\nPatel, F., Levinson-W aldman, R., Koreh, R., & DenUyl, S. (2020). Social Media Monitoring. Brennan Center for \nJustice. [perma.cc/N5LF-ZKP2]\nPhillipson, R. (1992). Linguistic Imperialism. Oxford University Press.\nPires, T., Schlinger, E., & Garrette, D. (2019). How Multilingual is Multilingual BER T? Proceedings of the 57th \nAnnual Meeting of the Association for Computational Linguistics, 4996\u20135001. [perma.cc/4DPF-LWWX]\nPng, M.-T. (2022). At the T ensions of South and North: Critical Roles of Global South Stakeholders in AI \nGovernance. 2022 ACM Conference on Fairness, Accountability, and Transparency, 1434\u20131445. [perma.cc/\nZ7HD-3T4A]\nPolignano, M., Basile, P., Degemmis, M., Semeraro, G., & Basile, V . (2019). AlBERTo: Italian BERT Language \nUnderstanding Model for NLP Challenging Tasks Based on Tweets. Sixth Italian Conference on \nComputational Linguistics, Bari, Italy. [perma.cc/RBY9-4JHJ]\nPrabhakaran, V ., Mitchell, M., Gebru, T., & Gabriel, I. (2022). A Human Rights-Based Approach to Responsible AI \n(arXiv:2210.02667). arXiv. [perma.cc/R97H-WQSK]\nPrates, M., Avelar, P., & Lamb, L. (2020). Assessing gender bias in machine translation: A case study with Google \nT ranslate. Neural Computing and Applications, 32. [perma.cc/CGK2-NMU2]\nPym, A., Ayvazyan, N., & Prioleau, J. M. (2022). Should raw machine translation be used for public-health \ninformation? Suggestions for a multilingual communication policy in Catalonia. Just. Journal of Language \nRights & Minorities, Revista de Drets Ling\u00fc\u00edstics i Minories, 1(1\u20132), 71\u201399. [perma.cc/HSA8-TB3F]\nRaji, D., Denton, E., Bender, E. M., Hanna, A., & Paullada, A. (2021). AI and the Everything in the Whole \nWide W orld Benchmark. Proceedings of the Neural Information Processing Systems Track on Datasets and \nBenchmarks, 1. [perma.cc/EX84-X9BQ]\nRazeghi, Y., Logan IV , R. L., Gardner, M., & Singh, S. (2022). Impact of Pretraining T erm Frequencies on Few-\nShot Numerical Reasoning. Findings of the Association for Computational Linguistics: EMNLP 2022, \n840\u2013854. [perma.cc/SMG9-BSKV]\nReid, M., & Artetxe, M. (2022). On the Role of Parallel Data in Cross-lingual Transfer Learning \n(arXiv:2212.10173). arXiv. [perma.cc/83GW-CVXX]\nRichter, F. (n.d.). English Is the Internet\u2019s Universal Language. Statista Infographics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e92e0471-6548-4fb9-ab08-f274275ab898": {"__data__": {"id_": "e92e0471-6548-4fb9-ab08-f274275ab898", "embedding": null, "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d500a24-754d-419a-96c6-39f8e8ff868d", "node_type": "4", "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "cc6674c2b5c6b8f30ddd9de92258f5e170cddcb34b2b5ac3077b5f7f4b2b0538", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c907e78c-8170-46e6-8b31-6d48a664c96b", "node_type": "1", "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "7233ae956998c9d73f65c5a02abd3f7b9947e23e7d48ac13a215f43b8336a2c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Findings of the Association for Computational Linguistics: EMNLP 2022, \n840\u2013854. [perma.cc/SMG9-BSKV]\nReid, M., & Artetxe, M. (2022). On the Role of Parallel Data in Cross-lingual Transfer Learning \n(arXiv:2212.10173). arXiv. [perma.cc/83GW-CVXX]\nRichter, F. (n.d.). English Is the Internet\u2019s Universal Language. Statista Infographics. Retrieved December 14, \n2022, from [perma.cc/WW7B-7X37]\nRionda, V . P. S., & Mejia, J. C. U. (2021). PretorIA y la automatizaci\u00f3n del procesamiento de causas de derechos \nhumanos. Derechos Digitales and Dejusticia. [perma.cc/65MQ-X484]\nRowe, J. (2022, March 2). Marginalised languages and the content moderation challenge. Global Partners Digital. \n[perma.cc/GU4K-5HBE]", "mimetype": "text/plain", "start_char_idx": 2350, "end_char_idx": 3055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ad393b2-3ad1-4625-b08c-b8117ba82f78": {"__data__": {"id_": "7ad393b2-3ad1-4625-b08c-b8117ba82f78", "embedding": null, "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1b086c1-22e1-4117-8ec1-0dff3d646703", "node_type": "4", "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e69fecb320399e228965373dd288b10b5a369483dec086d8045c1879ff30aaaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4718c3d7-139e-43ad-8f1e-8b2fdf45f7d3", "node_type": "1", "metadata": {}, "hash": "dd7dced7895356e0eb41a20d723bbc3f2adc284e3876a72893b038a7ea8f3e02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "47\nLarge Language Models in Non-English Content Analysis\nWorks Cited\nRuder, S., Constant, N., Botha, J., Siddhant, A., Firat, O., Fu, J., Liu, P., Hu, J., Garrette, D., Neubig, G., & \nJohnson, M. (2021). XTREME-R: T owards More Challenging and Nuanced Multilingual Evaluation. \nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 10215\u201310245. \n[perma.cc/W4TJ-SGTB]\nSanta Clara Principles. (2021). Santa Clara Principles on Transparency and Accountability in Content Moderation. \nSanta Clara Principles. [perma.cc/T623-A VW6]\nSanty, S., Kummerfeld, J., & Rubio, H. (2023). Languages mentioned in Paper Abstracts. ACL Rolling Review. \n[perma.cc/EQU9-5CWQ]\nSavoldi, B., Gaido, M., Bentivogli, L., Negri, M., & Turchi, M. (2021). Gender Bias in Machine T ranslation. \nTransactions of the Association for Computational Linguistics, 9, 845\u2013874. [perma.cc/9K3F-5VBZ]\nSchwenk, H. (2019, January 22). LASER natural language processing toolkit\u2014Engineering at Meta. Meta AI. \n[perma.cc/46JG-AZ4T]\nSengupta, P. B., Claudia Pozo, Anasuya. (2022, March 31). Does the internet speak your language? Launching the \nfirst-ever State of the Internet\u2019s Languages report. Whose Knowledge? [https://perma.cc/9KCX-M863]\nSharir, O., Peleg, B., & Shoham, Y. (2020). The Cost of Training NLP Models: A Concise Overview \n(arXiv:2004.08900). arXiv. [perma.cc/8KVV-C6P2]\nShenkman, C., Thakur, D., & Llans\u00f3, E. (2021). Do You See What I See? Capabilities and Limits of Automated \nMultimedia Content Analysis. Center for Democracy & T echnology. [perma.cc/W85T-HQQF]\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., \nGarriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., W arstadt, A., Kocurek, A. \nW ., Safaya, A., T azarv, A., \u2026 Wu, Z. (2022). Beyond the Imitation Game: Quantifying and extrapolating the \ncapabilities of language models (arXiv:2206.04615). arXiv. [perma.cc/278S-ZJV9]\nStadthagen-Gonzalez, H., Imbault, C., P\u00e9rez S\u00e1nchez, M. A., & Brysbaert, M. (2017). Norms of valence and \narousal for 14,031 Spanish words. Behavior Research Methods, 49(1), 111\u2013123. [perma.cc/7FWX-Z3JD]\nStrubell, E., Ganesh, A., & McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. \nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3645\u20133650. \n[perma.cc/9P4Y-J4HT]\nT alat, Z., N\u00e9v\u00e9ol, A., Biderman, S., Clinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., \nRadev, D., Sharma, S., Subramonian, A., T ae, J., T an, S., Tunuguntla, D., & W al, O. van der. (2022).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4718c3d7-139e-43ad-8f1e-8b2fdf45f7d3": {"__data__": {"id_": "4718c3d7-139e-43ad-8f1e-8b2fdf45f7d3", "embedding": null, "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1b086c1-22e1-4117-8ec1-0dff3d646703", "node_type": "4", "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e69fecb320399e228965373dd288b10b5a369483dec086d8045c1879ff30aaaa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ad393b2-3ad1-4625-b08c-b8117ba82f78", "node_type": "1", "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e6be317b487b3f3e182c570c96ddf2674fac1926ac455915b7316674bd4c5fa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[perma.cc/9P4Y-J4HT]\nT alat, Z., N\u00e9v\u00e9ol, A., Biderman, S., Clinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., \nRadev, D., Sharma, S., Subramonian, A., T ae, J., T an, S., Tunuguntla, D., & W al, O. van der. (2022). Y ou \nreap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings. Proceedings of \nBigScience Episode #5, 26\u201341. [perma.cc/3ECR-4E7U]\nT attle. (n.d.). Uli. [perma.cc/4AB2-D4GX]\nT ay, Y., T ran, V . Q., Ruder, S., Gupta, J., Chung, H. W ., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. \n(2022, February 23). Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. \nInternational Conference on Learning Representations 2022. [perma.cc/YRL4-E7DT]\nT eich, E. (2003). Cross-Linguistic V ariation in System and T ext: A Methodology for the Investigation of \nT ranslations and Comparable T exts. In Cross-Linguistic V ariation in System and Text. De Gruyter Mouton. \n[perma.cc/L8A8-RH8B]", "mimetype": "text/plain", "start_char_idx": 2398, "end_char_idx": 3379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04eec716-5102-43ed-b3b3-ce391a16df5d": {"__data__": {"id_": "04eec716-5102-43ed-b3b3-ce391a16df5d", "embedding": null, "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76f442f7-285d-4bce-9e61-ffb24148240c", "node_type": "4", "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0e0afed6a02af7ede2e388459184cd5048bf0d20e463db365e568492a7086615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83b501f3-92f0-4f39-b559-daebd309dd7d", "node_type": "1", "metadata": {}, "hash": "89391ffcbcc3880b7ac837e9c722c9a2c4e8465c26cf8bb8fe834d640dec509c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "48\nLost in Translation\nCDT Research\nT orbati, Y. (2019, September 26). Google Says Google Translate Can\u2019t Replace Human Translators. Immigration \nOfficials Have Used It to V et Refugees. ProPublica. [perma.cc/ZUN6-LHA5]\nT oyama, K. (2015). Geek heresy: Rescuing social change from the cult of technology. PublicAffairs.\nT svetkov, Y., Sitaram, S., Faruqui, M., Lample, G., Littell, P., Mortensen, D., Black, A. W ., Levin, L., & Dyer, \nC. (2016). Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation \nLearning. Proceedings of the 2016 Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies, 1357\u20131366. [perma.cc/4RES-KFNM]\nUnited Nations Human Rights Office of the High Commissioner. (n.d.).   Economic, social and cultural rights. \nOHCHR. [perma.cc/Y6MK-SZZ4]\nV allee, H. Q. la, & Duarte, N. (2019). Algorithmic Systems in Education: Incorporating Equity and Fairness When \nUsing Student Data. Center for Democracy and Technology. [perma.cc/CC89-ZVNV]\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017, \nDecember 5). Attention Is All You Need. Advances in Neural Information Processing Systems. [perma.\ncc/2ZDX-Z796]\nVincent, J. (2022, November 2). Google plans giant AI language model supporting world\u2019s 1,000 most spoken \nlanguages. The V erge. [perma.cc/3Y48-X7WV]\nVincent, J. (2023a, January 17). Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its \ncontent. The V erge. [perma.cc/4CXS-3WNN]\nVincent, J. (2023b, March 15). OpenAI co-founder on company\u2019s past approach to openly sharing research: \u201cW e were \nwrong.\u201d The V erge. [perma.cc/DPL6-4PD2]\nVitulli, M. A. (2018). Writing W omen in Mathematics Into Wikipedia. Notices of the American Mathematical \nSociety, 65(03), 330\u2013334. [perma.cc/X73F-AZPM]\nV olansky, V ., Ordan, N., & Wintner, S. (2015). On the features of translationese. Digital Scholarship in the \nHumanities, 30(1), 98\u2013118. [perma.cc/7F8S-3YXK]\nW ang, C., Cho, K., & Gu, J. (2020). Neural Machine T ranslation with Byte-Level Subwords. Proceedings of the \nAAAI Conference on Artificial Intelligence, 34(05), Article 05. [perma.cc/5DL7-XSSP]\nW ang, Z., K, K., Mayhew, S., & Roth, D. (2020). Extending Multilingual BER T to Low-Resource Languages. \nFindings of the Association for Computational Linguistics: EMNLP 2020, 2649\u20132656. [perma.cc/ZNC8-\nC9E7]\nWilliams, A., Miceli, M., & Gebru, T. (2022). The Exploited Labor Behind Artificial Intelligence. No\u0113ma. [perma.\ncc/GE8H-7SUN]\nWu, S., & Dredze, M. (2019). Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BER T. Proceedings \nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), 833\u2013844.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83b501f3-92f0-4f39-b559-daebd309dd7d": {"__data__": {"id_": "83b501f3-92f0-4f39-b559-daebd309dd7d", "embedding": null, "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76f442f7-285d-4bce-9e61-ffb24148240c", "node_type": "4", "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "0e0afed6a02af7ede2e388459184cd5048bf0d20e463db365e568492a7086615", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04eec716-5102-43ed-b3b3-ce391a16df5d", "node_type": "1", "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "9a8face570f5a9579966f98efc10a50ec7911fde2469f32692f6ab14e72a9a2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2022). The Exploited Labor Behind Artificial Intelligence. No\u0113ma. [perma.\ncc/GE8H-7SUN]\nWu, S., & Dredze, M. (2019). Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BER T. Proceedings \nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), 833\u2013844. [perma.cc/EJ3G-MFYN]\nWu, S., & Dredze, M. (2020). Are All Languages Created Equal in Multilingual BER T? Proceedings of the 5th \nW orkshop on Representation Learning for NLP, 120\u2013130. [perma.cc/5E6X-NNAA]", "mimetype": "text/plain", "start_char_idx": 2506, "end_char_idx": 3089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83c71cca-4da4-4a91-b591-ed7b0892485a": {"__data__": {"id_": "83c71cca-4da4-4a91-b591-ed7b0892485a", "embedding": null, "metadata": {"page_label": "49", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf8f8c83-ed31-499f-99a1-98049710487b", "node_type": "4", "metadata": {"page_label": "49", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "49c4942e10eb85991733a95d8c563c40bf60bb510bfcc572e9286edb7f9aba55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "49\nLarge Language Models in Non-English Content Analysis\nWorks Cited\nYu, S., Sun, Q., Zhang, H., & Jiang, J. (2022). T ranslate-T rain Embracing T ranslationese Artifacts. Proceedings of \nthe 60th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short Papers), 362\u2013\n370. [perma.cc/7F8C-EYM6]\nZhang, S., Frey, B., & Bansal, M. (2022, April 25). How can NLP Help Revitalize Endangered Languages? A Case \nStudy and Roadmap for the Cherokee Language. Proceedings of the 60th Annual Meeting of the Association \nfor Computational Linguistics (V olume 1: Long Papers). ACL 2022, Dublin, Ireland. [perma.cc/2XF2-\n2GDC]\nZhang, Y., W arstadt, A., Li, X., & Bowman, S. R. (2021). When Do Y ou Need Billions of W ords of Pretraining \nData? Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \n11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), 1112\u20131125. \n[perma.cc/43ZK-2ZXC]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ff387b3-6e42-4fd4-8e48-3a4d3039e28f": {"__data__": {"id_": "7ff387b3-6e42-4fd4-8e48-3a4d3039e28f", "embedding": null, "metadata": {"page_label": "50", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b1fb81b-4525-461c-81fe-8b6300df297f", "node_type": "4", "metadata": {"page_label": "50", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "e1f75b0714ac7632e6015ba952520e395bd27fd097f6e345b441d35b2c55e932", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "cdt.org\ncdt.org/contact\n202-637-9800\n@CenDem T ech\nCenter for Democracy & Technology \n1401 K Street NW , Suite 200 \nW ashington, D.C. 20005", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fde49924-31c9-430b-8aad-34286417b0ec": {"__data__": {"id_": "fde49924-31c9-430b-8aad-34286417b0ec", "embedding": null, "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eebf101-fe44-4e65-9b26-d16a6f3fea72", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1b484b19ce46d721ea8a1b4f49a36e86efa6afedb75057f73db0ee2e45fc51b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AN ANALYSIS ON LARGE LANGUAGE MODELS IN HEALTHCARE :\nA CASE STUDY OF BIOBERT\nA PREPRINT\nShyni Sharaf\nSchool of Computer Science and Engineering\nKerala University of Digital Sciences-\nInnovation and Technology\nThiruvananthapuram, India\nshyni.cs22@duk.ac.in\nV . S. Anoop\nApplied NLP Research Lab\nSchool of Digital Sciences\nKerala University of Digital Sciences-\nInnovation and Technology\nThiruvananthapuram, India\nanoop.vs@duk.ac.in\nABSTRACT\nThis paper conducts a comprehensive investigation into applying large language models, particularly\non BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing\n(NLP) approaches in healthcare, shedding light on the limitations and challenges these methods\nface. Following that, this research explores the path that led to the incorporation of BioBERT\ninto healthcare applications, highlighting its suitability for addressing the specific requirements\nof tasks related to biomedical text mining. The analysis outlines a systematic methodology for\nfine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes\nvarious components, including the gathering of data from a wide range of healthcare sources, data\nannotation for tasks like identifying medical entities and categorizing them, and the application\nof specialized preprocessing techniques tailored to handle the complexities found in biomedical\ntexts. Additionally, the paper covers aspects related to model evaluation, with a focus on healthcare\nbenchmarks and functions like processing of natural language in biomedical, question-answering,\nclinical document classification, and medical entity recognition. It explores techniques to improve\nthe model\u2019s interpretability and validates its performance compared to existing healthcare-focused\nlanguage models. The paper thoroughly examines ethical considerations, particularly patient privacy\nand data security. It highlights the benefits of incorporating BioBERT into healthcare contexts,\nincluding enhanced clinical decision support and more efficient information retrieval. Nevertheless,\nit acknowledges the impediments and complexities of this integration, encompassing concerns\nregarding data privacy, integrity, bias mitigation, transparency, resource-intensive requirements, and\nthe necessity for model customization to align with diverse healthcare domains.\nKeywords Large language models \u00b7 Healthcare \u00b7 BioBERT \u00b7 Health informatics \u00b7 Natural Language Processing\n1 Introduction\nNLP processing has evolved to become LLM (Large Language Model). In the 1940s, after World War II, people\nrealized the importance of translation between languages and wanted to create a machine that could perform automatic\ntranslation. Early NLP systems were rule-based systems that humans manually programmed with rules for processing\nlanguage. These systems often had many limitations in handling complex language and could be easily deceived by\nunexpected inputs. In the early 2000s, statistical NLP models began to emerge. These models were trained on large\ntext datasets and learned to predict the next word in a sequence based on preceding words. Statistical NLP models\nexhibited greater robustness compared to rule-based systemsWang et al. [2023a] and could handle a wider range of\nlanguage tasks. By the mid-2010s, deep learning models started to revolutionize NLP. These models, based on artificial\nneural networks, had the capability to learn intricate patterns from data. Deep learning models Lavanya and Sasikala\n[2021]quickly outperformed statistical NLP models such as machine translation, summarization of texts, and question\nanswering. In 2017, the Transformer La Quatra and Cagliero [2022]a deep learning model designed for processing\narXiv:2310.07282v2  [cs.AI]  12 Oct 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc164f5c-b6ca-4cc9-8560-002be969953b": {"__data__": {"id_": "dc164f5c-b6ca-4cc9-8560-002be969953b", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4b9e9f5-ffb5-4d9a-8c8f-411ed4f0ee78", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a08d6b5c8d20467d53250c235952703c52778a90bff612b891052d4a556a5713", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nsequential data, such as text, marked a significant milestone. It achieved state-of-the-art results across numerous NLP\ntasks and soon became the standard architecture for training LLMs. This shift led to the emergence of natural language\nprocessing (NLP) as we know it today. NLP has radically transformed into the era of large language models (LLMs).\nLLMs are trained on massive text datasets, sometimes containing hundreds of billions or even trillions of words.\nThis extensive training enables LLMs to understand the intricate patterns and relationships inherent in language. As\na result, LLMs have fundamentally altered how we interact with and harness the power of language. Popular LLMs\nlike GPT-3.5, GPT-4, PaLM, Cohere, LaMDA, Llama, and others have revolutionized our interaction with data by\nredefining the boundaries of language understanding and generation. Natural Language Processing (NLP) is a branch\nof artificial intelligence (AI) and computational linguistics that facilitates interaction between computers and humans\nthrough natural language. LLMsReddy [2023]process vast amounts of textual data, learn the underlying patterns, and\ngenerate contextually relevant human-like text. This technology has not only catalyzed but has become a driving force\nin transforming healthcare and biomedical applications. In this article, we conduct a comparative analysis of the\ndiverse applications of LLMs in the healthcare and biomedical domains. We explore how LLMs are reshaping the\nlandscape by offering innovative solutions to long-standing challenges. Current healthcare and biomedical systems often\noperate inefficiently, have limited access to relevant information, and involve cumbersome documentation processes.\nLLMs can address these challenges by providing rapid, context-aware responses to medical queries, extracting valuable\ninsights from unstructured data, and automating clinical documentation. The major contributions of this article are as\nfollows:\n\u2022 Conducts a detailed evaluation of the existing prominent state-of-the-art large language models introduced in\nthe healthcare domain.\n\u2022 Taking BioBERT as a reference pre-trained language model, we check the applications of the same in\nhealthcare.\n\u2022 Discusses the prominence of BioBERT in downstream clinical natural language processing tasks and discusses\nthem in detail.\n\u2022 Outlines the challenges with LLM in healthcare and presents the future research directions.\n1.1 An overview of LLM components\n\u2022 Input Text: Initially, the LLM receives raw text as input. This text can be in sentences, paragraphs, or\ndocuments.\n\u2022 Tokenization: The text we give is divided into individual tokens. Tokens can be words, subwords, or characters,\ndepending on LLMs tokenization scheme. This step breaks down the text into manageable units for processing.\n\u2022 Word Embeddings: Each token is transformed into a high-dimensional vector through word embeddings.\nThese vectors capture the token\u2019s meaning and context. Word embeddings are learned during the model\u2019s\ntraining using a vast amount of text data.\n\u2022 Transformer Layers: The embedded vectors are passed through multiple transformer layers. Each transformer\nlayer consists of two main components:\n\u2013 Multi-Head Self-Attention: This component weighs the importance of each token in relation to others,\ncapturing dependencies and context.\n\u2013 Feedforward Neural Networks: Complex transformations are applied to the vectors, enhancing the\nmodel\u2019s understanding of patterns and relationships.\n\u2022 Output Layer: After processing through the transformer layers, the output is fed into a linear layer. This\nlinear layer generates a probability distribution that spans the model\u2019s vocabulary. It estimates the probability\nof different words or word sequences following the input text.\n\u2022 Probability Estimation: The probability distribution generated in the previous step is used for various tasks\nsuch as language generation, text completion, and question answering.\n\u2022 Training and Fine-Tuning: LLMs are initially trained on a large text corpus to learn embeddings and model\nparameters. Fine-tuning can follow, where the model is further trained on task-specific data to adapt its\nlanguage understanding to the specific task or domain.\n2 NLP for Healthcare Applications\nLLMs have emerged as a transformative technology in healthcare, enabling an extensive range of applications, from\nclinical decision support to medical data analysis. LLMs allow healthcare professionals to harness the power of language\ndata for improved patient care, research, and administrative tasks.\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3503ad07-253c-43fa-afe8-df305bf5ae27": {"__data__": {"id_": "3503ad07-253c-43fa-afe8-df305bf5ae27", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658501cc-22a7-431e-a0ba-f0c1d40a13cc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7a95a04cecd74aaeb975827038661b043b91477734bafcfd65b28fc251ee627", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa7fd295-404c-40a4-a5e3-142ce4b8d30e", "node_type": "1", "metadata": {}, "hash": "b61c09940c23f84ad760b5c7d695794f95dc5da7076f44a0cf4abd86a68e65cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Medical Question Answering: LLMs can answer medical questions, providing quick and accurate responses.\nThis application aids healthcare professionals in accessing medical knowledge and information rapidly.\n\u2022 Electronic Health Record (EHR) Analysis: LLMs can analyze unstructured text in electronic health records,\nextracting valuable insights about patient histories, diagnoses, treatments, and clinical notes. This supports\nclinical decision-making and research.\n\u2022 Clinical Documentation: LLMs can assist healthcare providers in generating clinical notes, reports, and\ndocumentation. This streamlines the documentation process, allowing clinicians to focus more on patient care.\n\u2022 Medical Imaging: LLMs can assist in medical image interpretation by generating natural language descriptions\nof images. This can improve communication between radiologists and referring physiciansWang et al.\n[2023b]Rao et al. [2023].\n\u2022 Clinical Decision Support: LLMs can provide context-aware information to support clinical decisions. They\ncan recommend treatment options, predict patient outcomes, and identify potential risks Rao et al. [2023].\n\u2022 Healthcare Communication: LLMs can improve doctor-patient communication by offering language transla-\ntion services, ensuring effective communication in multilingual healthcare settings Yunxiang et al. [2023]\n\u2022 Patient Engagement: LLMs can be used in chatbots and virtual assistants Ray [2023] to engage with patients,\nanswer their healthcare queries, and provide health-related information and guidance. Healthcare professionals\ncan use NLP to extract relevant information from patient records, such as medical history, medication allergies,\nand previous diagnoses, enabling the creation of personalized treatment plans and early identification of\nhigh-risk patients for disease prevention.\n\u2022 Enhancing Medical Research: NLP can also analyze large amounts of medical data to identify patterns and\ntrendsHao et al. [2018]. It helps researchers to develop new treatments and therapies.\n\u2022 Improving Clinical Trials: NLP algorithms can sift through much data and extract information relevant to the\nclinical trial. NLP helps clinical trials Chen et al. [2020]by finding the right participants faster and cheaper\nthrough patient data analysis and improves efficiency. It reduces the time and cost.\n\u2022 Improving Digital Health Records: NLP can make digital patient records more correct and complete. These\nrecords hold information about a person\u2019s health history and treatments. NLP helps doctors to get the right\ndetails from these health records Costea [2020]so they can make better decisions for patient care.\n\u2022 Supports Medical Practitioners: NLP makes many everyday tasks of health professionals easierDemner-\nFushman et al. [2009]. For instance, it finds possible issues with medicines, helps doctors adjust treatment\nplans, and helps doctors write notes faster by saving time and reducing errors, so they can spend more time\ncaring for patients. Also, NLP aids in extracting Information from medical literature, helping healthcare\nprofessionals to learn Henwood and Lister [2007]stay current with the latest research and best practices.\n3 Language Models and Healthcare\nLarge Language Models (LLMs) are one of the most exciting areas in Artificial Intelligence (AI) research that have\nthe ability to process and generate human-like text for various healthcare applications. The more data we train, the\nmore predictions will be more accurate. Mainly used LLM are GPT-3, BERT, and RoBERTa Liu et al. [2019], which\nare trained on billions of words and patterns. so these models can understand the structure easily and generate text.\nOnce the model is generated it can be fine-tuned for a specific Task. The applications of LLM Hao et al. [2018]\nhealthcare have many different aspects and have the power to bring about significant positive changes in various\nfields. These technologies offer real-time assistance to healthcare professionals by helping them diagnose diseases and\ngive the right treatments without Errors. Predictive Analytics in healthcare can use data to predict disease outbreaks\nand enhance healthcare delivery efficiency. The significance of studying LLM applications in healthcare is that it is\nversatile. LLMs should be used in healthcare in a collaborative and verified way to ensure responsible and effective\nuse, ultimately improving patient care. This means the use of LLMs should be carefully monitored and evaluated,\nand thereby identifying potential problems or risks. LLMs are a powerful tool that has the potential to revolutionize\nhealthcare.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa7fd295-404c-40a4-a5e3-142ce4b8d30e": {"__data__": {"id_": "aa7fd295-404c-40a4-a5e3-142ce4b8d30e", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "658501cc-22a7-431e-a0ba-f0c1d40a13cc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7a95a04cecd74aaeb975827038661b043b91477734bafcfd65b28fc251ee627", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3503ad07-253c-43fa-afe8-df305bf5ae27", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4ba240beb77dfa93e9545791c0a0e9b24abfc2a42e3a9eec8f207a26d99f9fe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Predictive Analytics in healthcare can use data to predict disease outbreaks\nand enhance healthcare delivery efficiency. The significance of studying LLM applications in healthcare is that it is\nversatile. LLMs should be used in healthcare in a collaborative and verified way to ensure responsible and effective\nuse, ultimately improving patient care. This means the use of LLMs should be carefully monitored and evaluated,\nand thereby identifying potential problems or risks. LLMs are a powerful tool that has the potential to revolutionize\nhealthcare.\n3.1 Benefits of LLMs in Healthcare:\n\u2022 Improved Support for Clinical Decisions: LLMs assist healthcare providers in decision-making by providing\naccess to a vast amount of medical knowledge and up-to-date research. They can suggest potential diagnoses,\ntreatment options, or relevant research articles quickly. LLMs can make the diagnosis and data more accurate\nthan humans; thereby, the quality of outcomes and care of patients can be improved.\n3", "mimetype": "text/plain", "start_char_idx": 4140, "end_char_idx": 5140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a894447-6661-4d46-b084-c482f50bd1d4": {"__data__": {"id_": "2a894447-6661-4d46-b084-c482f50bd1d4", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c4c6d05-91cf-46ef-be46-b6e213e426e3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c6265914e11897d33f6edc7f2e259de3cf708c011816813ea5c7927bf37bb4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48ce95a7-fd2c-4ee0-9873-f52975254451", "node_type": "1", "metadata": {}, "hash": "1f619f194410287b2cf95d1d86a091c0987d8372991bd39c9bbd74bb321ca598", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Efficient Information Retrieval: LLMs can be used to clarification medical tests by analyzing the results\nproviding valuable information and helping to find out the abnormalities. By this, we can reduce the time and\ncost of interpreting the results and improve accuracy and reliability.\n\u2022 Clarification of Medical Tests: LLMs can identify clinical trials by analyzing the current conditions of the\npatient, medical history, and treatment plans. This will improve the efficiency and effectiveness and provide\npotential lifesaving treatments.\n\u2022 Searching for Potential Clinical Trials: LLMs can identify clinical trials by analyzing the current conditions\nof the patient, medical history, and treatment plans. This will improve the efficiency and effectiveness and\nprovide potential lifesaving treatments\n3.2 Limitations and Challenges of Using LLMs in Healthcare\n\u2022 Data Privacy and Security: Integrating LLMs into healthcare must proceed cautiously to safeguard highly\nsensitive healthcare data. Ensuring data privacy and security, along with compliance with regulations such as\nHIPAA, is paramount to prevent the potentially severe consequences of data breaches.\n\u2022 Bias and Fairness: LLMs trained on biased data may produce biased or unfair results in healthcare applications.\nThis can lead to disparities in care, misdiagnoses, or unfair allocation of resources.\n\u2022 Lack of Transparency: LLMs often operate as \"black boxes,\" making it challenging to understand their\ndecision-making processes. This lack of transparency can hinder trust among healthcare professionals and\npatients.\n\u2022 Quality Control: Ensuring the quality and accuracy of information generated or retrieved by LLMs is crucial.\nErroneous information or recommendations could harm patients or mislead healthcare providers.\n\u2022 Concern of Ethical issues : Using LLMs in healthcare raises ethical concerns, such as the potential for\ntechnology to replace human interaction in patient care, leading to depersonalized medicine.\n\u2022 Resource Intensiveness: Developing, fine-tuning, and maintaining LLMs for healthcare can be resource-\nintensive regarding computational power, data annotation, and expert oversight.\n\u2022 Generalization Challenges: LLMs may struggle with generalizing to specific healthcare domains, specialties,\nor rare conditions if not adequately fine-tuned. Customization may be necessary.\n4 Related Studies\nIn this section, we review relevant articles that have explored the integration of LLMs in healthcare applications.\nThe articles showcase the significant impact of large language models in various healthcare-related tasks, such as\nbiomedical text mining, medical image interpretation, medical question answering, and processing electronic health\nrecords. They also highlight the need for careful evaluation and consideration of limitations when applying these models\nin clinical settings. The strengths include improved task performance and potential benefits for healthcare However, the\nresource-intensive nature of such models and potential challenges in fine-tuning for specific healthcare applications\nshould be considered. N. Kang et al. Kang et al. [2013]primarily focus on evaluating the performance of MetaMap and\nPeregrine tools used for biomedical concept normalization. The study investigates the usefulness of rule-based NLP\nmodules that are used to enhance the performance of MetaMap and Peregrine, an adjunct to dictionary-based concept\nnormalization in the biomedical field, to evaluate the Corpus for Arizona Disease.\nS. A. Hasan et al. Hasan and Farri [2019] discuss the application of deep learning(DL) techniques in clinical natural\nlanguage processing (CNLP). The model emphasizes the use of DL models for various clinical applications. Deep\nlearning-driven clinical NLP applications include diagnostic inferencing, biomedical article retrieval, clinical paraphrase\ngeneration, adverse drug event detection, and medical image caption generation. J. Lee et al. lee2020biobertLee et al.\n[2020]introduced BioBERT, a pre-trained biomedical language representation model tailored for biomedical text mining.\nBioBERT\u2019s training involved a substantial biomedical text corpus. This model excelled in various tasks such as named\nentity recognition, relation extraction, and question answering, achieving state-of-the-art performance across the board.\nShin et al.Shin et al. [2020]contributed to the field with BioMegatron, a larger pre-trained biomedical language\nmodel aimed at biomedical text mining analogous to BioBERT. Differing in scale, BioMegatron was trained on an even\nmore extensive corpus of biomedical text and exhibited state-of-the-art performance in tasks such as entity recognition,\nrelation extraction, and question-answering. Additionally, X. Yang et al.Yang et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48ce95a7-fd2c-4ee0-9873-f52975254451": {"__data__": {"id_": "48ce95a7-fd2c-4ee0-9873-f52975254451", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c4c6d05-91cf-46ef-be46-b6e213e426e3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c6265914e11897d33f6edc7f2e259de3cf708c011816813ea5c7927bf37bb4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a894447-6661-4d46-b084-c482f50bd1d4", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "10d7b554bd7527a33c7ea797c2c8f8222c9c4975f97fb1d192943cb0975e191e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Shin et al.Shin et al. [2020]contributed to the field with BioMegatron, a larger pre-trained biomedical language\nmodel aimed at biomedical text mining analogous to BioBERT. Differing in scale, BioMegatron was trained on an even\nmore extensive corpus of biomedical text and exhibited state-of-the-art performance in tasks such as entity recognition,\nrelation extraction, and question-answering. Additionally, X. Yang et al.Yang et al. [2022a] presented GatorTron, a\nsubstantial clinical language model created for processing and interpreting electronic health records (EHRs). With\nextensive scaling in model parameters and training data, GatorTron significantly improved performance across clinical\nNLP tasks, offering potential enhancements in healthcare-related natural language processing by evaluating it on 5\nclinical NLP tasks like clinical concept extraction, medical relation extraction, semantic textual similarity, natural\n4", "mimetype": "text/plain", "start_char_idx": 4430, "end_char_idx": 5363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c378954c-b525-4deb-8693-091245ecb204": {"__data__": {"id_": "c378954c-b525-4deb-8693-091245ecb204", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd498739-8e5a-4c7f-aa1a-3a3abc3eec10", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "28ffda9509ea5b6ec8477807fcb748d297d55cb62d30f7b4286d68e005954c8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd922aa5-bc7f-40b1-9c7f-be083170925d", "node_type": "1", "metadata": {}, "hash": "178bd7f92cca4cdd489c2180ae1d25d60a04a783a4bfc86c0e95707c3295f267", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nTable 1: Some state-of-the-art approaches in using LLM and related techniques in the healthcare NLP\nAuthor Model Methodology\nN. Kang et al. Kang et al. [2013] Rule-based NLP A rule-based NLP module used to enhance the perfor-\nmance of MetaMap and Peregrine.\nS. A. Hasan et al. Hasan and Farri [2019] Deep Learning Addresses the challenges posed by clinical documents,\nincluding acronyms, nonstandard clinical jargon, incon-\nsistent document structure, and privacy concerns.\nJ. Lee et al. Lee et al. [2020] BioBERT Pre-training on large-scale biomedical corpora out-\nperforms BERT and other models in biomedical text-\nmining tasks.\nHC Shin et al. Shin et al. [2020] BioMegatron Empirical study on factors affecting domain-specific\nlanguage models, pre-training on larger domain corpus.\nX. Yang et al. Yang et al. [2022b] GatorTron Developing a large clinical language model, scaling up\nthe number of parameters and training data.\nK. Singhal et al.Singhal et al. [2022] MultiMedQA, PaLM,\nFlan-PaLM, Med-PaLM MultiMedQA benchmark, human evaluation of model\nanswers, instruction prompt tuning.\nWang et al.Wang et al. [2023b] [5] ChatGPT, CAD networks Integrating LLMs with CAD networks, enhancing out-\nput with natural language text.\nS. Reddy et al.Reddy [2023] \u2013 Discusses the potential use of Large Language Models\n(LLMs) in healthcare. Highlights concerns related to\nmisinformation and data falsification. Proposes a frame-\nwork for evaluation, including human assessments.\nH. Zhang et al. Zhang et al. [2023] HuatuoGPT Leveraging distilled data from ChatGPT and real-world\ndata from doctors for medical consultation, reward\nmodel training.\nK. Singhal et al.Singhal et al. [2023] Med-PaLM - 2 Improving upon Med-PaLM with base LLM improve-\nments, medical domain fine-tuning, and prompting\nstrategies.\nlanguage inference (NLI), and medical question answering (MQA).\nJ.Singhal et al.Singhal et al. [2022]explored the encoding of clinical knowledge using Large Language Models.\nThey demonstrated that training LLMs on extensive clinical text enabled them to accurately answer questions related\nto clinical concepts, showcasing their potential in encoding clinical knowledge. They proposed a robust framework\nfor human evaluation of model responses, incorporating factors such as factuality, precision, potential harm, and bias\ninto the assessment process. PaLM, and its instruction-tuned variant, Flan-PaLM, were evaluated using MultiMedQA.\nWang et al.Wang et al. [2023b]presented Chatcad, a large language model designed for interactive computer-aided\ndiagnosis (CAD) in medical image analysis. Trained on a dataset featuring medical images and their accompanying text\ndescriptions, Chatcad demonstrated the ability to accurately diagnose diseases from images, aiding radiologists in their\ndiagnoses.\nS. Reddy et al.Kang et al. [2013]reddy2023evaluatingintroduced a framework for evaluating the translational\nvalue of Large Language Models (LLMs) in healthcare. This framework was a comprehensive tool for assessing\nLLMs\u2019 performance in healthcare applications. It was subsequently employed to assess the NLP performance of\nLLM\u2019s on the grounds of not assessing the models\u2019 functional, utility, and ethical aspects as they apply to healthcare,\nand recommended governance aspects of LLMs in healthcare are required. Zhang et al.Zhang et al. [2023]unveiled\nHuatuoGPT, a specialized LLM tailored for medical consultation. By leveraging data from ChatGPT and real-\nworld doctors, HuatuoGPT was fine-tuned to provide clinical advice and support to patients. This unique approach\nimproved its performance, achieving state-of-the-art results in medical consultation tasks. K. Singhal et al.Singhal et al.\n[2023]introduced Med-PaLM 2, an LLM designed for expert-level medical question answering.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd922aa5-bc7f-40b1-9c7f-be083170925d": {"__data__": {"id_": "bd922aa5-bc7f-40b1-9c7f-be083170925d", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd498739-8e5a-4c7f-aa1a-3a3abc3eec10", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "28ffda9509ea5b6ec8477807fcb748d297d55cb62d30f7b4286d68e005954c8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c378954c-b525-4deb-8693-091245ecb204", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "879498b4944f2a3dadbe6201ef0f98bb1463f23bbbc76504c9871c7a541e1c80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhang et al.Zhang et al. [2023]unveiled\nHuatuoGPT, a specialized LLM tailored for medical consultation. By leveraging data from ChatGPT and real-\nworld doctors, HuatuoGPT was fine-tuned to provide clinical advice and support to patients. This unique approach\nimproved its performance, achieving state-of-the-art results in medical consultation tasks. K. Singhal et al.Singhal et al.\n[2023]introduced Med-PaLM 2, an LLM designed for expert-level medical question answering. This model achieved\nremarkable scores in medical question-answering tasks with a score of 67.2% on the MedQA dataset, highlighting its\npotential for delivering high-precision performance to medical question answering.\n5 An Analysis of LLMs in Healthcare - A Case Study of BioBERT\nIn this section, we delve into the methodologies and outcomes of the aforementioned articles. We assess how LLMs\nare employed to address healthcare challenges and explore their impact on various aspects of the healthcare industry.\n5", "mimetype": "text/plain", "start_char_idx": 3399, "end_char_idx": 4384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8492990-3cc3-4e6b-92ac-db7bac4e1d4b": {"__data__": {"id_": "c8492990-3cc3-4e6b-92ac-db7bac4e1d4b", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96d96ca3-b8cc-4f9d-83ae-990e7310483c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "08b0609f98c457ed375f51e87648abad7e183c717cf77e4d9127aa970cde6cfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nThe paper \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining\" by J Lee\net al. Lee et al. [2020] investigates how the recently introduced pre-trained language model BERT can be adapted\nfor biomedical corpora. In this article, we explore the possibility of Fine-tuning BioBERT for the healthcare domain,\nwhich can be a valuable endeavor given its success in biomedical text-mining tasks. To adapt BioBERT for healthcare\napplications, methodology outlines the steps and considerations for fine-tuning BioBERT for healthcare-specific tasks.\nIt emphasizes the importance of domain expertise, data quality, and ethical considerations in developing robust and\nreliable healthcare language models. To adapt BioBERT for healthcare applications, the following methodology can be\nconsidered:\n\u2022 Data Collection: Gather a comprehensive and diverse dataset from healthcare and biomedical sources. This\ndataset should include electronic health records (EHRs) Yang et al. [2022a]medical literature, clinical notes,\nmedical imaging reports, and other relevant sources. And annotate the data for various healthcare-related tasks,\nsuch as medical entity recognition (e.g., disease names, medications, procedures), medical text classification\n(e.g., diagnosis prediction, disease classification), and medical question-answeringSinghal et al. [2023].\n\u2022 Pre-processing: Prepare the data by cleaning and formatting it for training. This may involve standardizing\nmedical terms, removing duplicates, removing any errors or inconsistencies in the data, and handling missing\nvalues. Then Customize tokenization to accommodate the unique vocabulary and structure of biomedical\nand clinical texts. Clinical text data often contains specialized vocabulary and structure, so it is important\nto use a customized tokenizer for this type of data. Hence specialized tokenizers may be needed to handle\nmedical terminology, abbreviations, and symbols. Some common tokenizers for biomedical and clinical text\ndata include:\n\u2013 The BioBERT tokenizer. This tokenizer is based on the BERT tokenizer but has been customized to\nhandle medical terminology and abbreviations.\n\u2013 The MedTokenizer. This tokenizer is specifically designed for biomedical text data.\n\u2013 The SciBERT tokenizer. This tokenizer is designed for scientific text data, which includes biomedical\ntext.\nFigure 1: Overall architecture for BioBERT Pre-training\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a0020db-1ff4-420b-94eb-977141939801": {"__data__": {"id_": "5a0020db-1ff4-420b-94eb-977141939801", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08ce4d48-c7d4-484a-bca0-6d99c85818ab", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ae3fde92cb199afc0e3c02908590433b4a7572dc25e05f48bb08b682c4478f56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nFigure 2: Overall architecture for BioBERT Finetuning\nBioBERT is a pre-trained language model trained on a massive dataset of biomedical text. The pre-trained\nweights represent the model\u2019s understanding of the general structure and semantics of biomedical text. Design a set of\ndownstream tasks specific to healthcare. Many different downstream tasks can be performed using BioBERT. Some\ncommon tasks include:\n\u2022 Medical Entity Recognition: This task involves identifying and extracting medical entities from text. These\nentities can include diseases, medications, and medical procedures.\n\u2022 Medical Text Classification: In this task, the text is categorized into different healthcare-related categories.\nExamples of categories include diagnosis, prognosis, and treatment.\n\u2022 Disease Prediction: This task involves predicting the likelihood of a patient having a particular disease.\n\u2022 Medical Question-Answering: This task involves answering questions about medical topics based on text.\nFine-tune BioBERT on these tasks using the annotated healthcare dataset. Fine-tuning is adjusting the model\u2019s weights\nto improve performance on a specific task. This is done by feeding the model the annotated healthcare dataset and\nletting it learn from it. Apply appropriate loss functions. A loss function measures the model\u2019s performance on the task.\nThe loss function is used to update the model\u2019s weights during fine-tuning. Incorporate transfer learning techniques:\nTransfer learning involves using a model trained on one task to enhance the performance of a model on a distinct task.\nThis can be achieved by initializing the new model with the pre-trained model\u2019s weights. Conduct experiments on\nhyperparameters. Hyperparameters represent the configurations of the machine learning algorithm and significantly\ninfluence the model\u2019s performance. Common hyperparameters to explore encompass:\n\u2022 Adjust the learning rate, which dictates the magnitude of weight updates in each training iteration.\n\u2022 Vary the batch size, determining the quantity of samples utilized for weight updates in each training iteration.\n\u2022 Modify the number of epochs, specifying how often the model undergoes training on the data.\n5.1 Evaluation Metrics:\nAssess the fine-tuned BioBERT model across a range of healthcare-related benchmarks and tasks. These include\nbiomedical NLP tasks, medical question-answering, clinical document classification, medical entity recognition,\ngenerating discharge summaries, interpreting medical records, and providing medical advice.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfa1d953-404e-40c8-bac5-2e3e65ee64a6": {"__data__": {"id_": "dfa1d953-404e-40c8-bac5-2e3e65ee64a6", "embedding": null, "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fab280d9-b044-4f1f-8cc8-3af99641d9e6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9d8b7cc3a9a564f71de36b0252733f61c3b5d9a2f55bdba6995b8c4e59859d51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 F1 score: It is calculated by taking the harmonic mean of precision and recall. it is the measure of the accuracy\nand completeness of the model\u2019s predictions. The F1 score is a good metric for tasks such as medical entity\nrecognition and text classification.\n\u2022 Accuracy: Accuracy is the percentage of predictions that the model gets correct.\n\u2022 Precision: Precision is the percentage of positive predictions that are actually positive.\n\u2022 Recall: Recall is the percentage of actual positives that are predicted as positive.\n\u2022 AUC: AUC is the area under the receiver operating characteristic curve. It measures the model\u2019s ability\nto distinguish between positive and negative examples. AUC is a good metric for tasks such as medical\nquestion-answering and disease prediction.\n\u2022 C-index: The C-index measures the model\u2019s ability to predict the survival of patients.\n5.2 Model Interpretability:\nTo enhance the interpretability of a fine-tuned BioBERT model, employ the following techniques:\n\u2022 Analyze the model\u2019s predictions : Examine the model\u2019s predictions and comprehend their rationale. This\ninvolves inspecting the model\u2019s features for making predictions and scrutinizing the attention weights assigned\nto various parts of the text.\n\u2022 Utilize visualization techniques : Make the model\u2019s predictions more comprehensible through graphical\nrepresentations. Employ heat maps to visualize attention weights or other visualization methods to elucidate\nhow the model generates predictions.\n\u2022 Leverage explainability tools : Utilize various explainability tools designed to elucidate how a machine\nlearning model arrives at its predictions. These tools reveal the features employed by the model for prediction\nand provide insight into the significance of each feature.\n5.3 Validation and Testing\nTo validate the performance of a fine-tuned BioBERT model for healthcare tasks, consider the following actions.\n\u2022 Compare model\u2019s performance with that of other existing biomedical models like BioMegatronShin et al.\n[2020] GatorTron Yang et al. [2022b]and clinical language modelsSinghal et al. [2022]. Use the same\nevaluation metrics and datasets to determine the best-performing model based on these metrics.\n\u2022 Experiment with hyperparameters, recognizing that these settings can significantly influence the model\u2019s\nperformance. Conduct experiments with different hyperparameters to identify the optimal configuration for\nthe specific task.\n\u2022 Validate the model on external healthcare datasets or benchmarks to assess its generalizability and robustness.\nThe model should demonstrate strong performance on previously unseen datasets.\nWhen validating the performance of a fine-tuned BioBERT model for healthcare tasks, also consider the following\nfactors:\n\u2022 The size and quality of the training dataset.\n\u2022 The specific task for which the model is being evaluated.\n\u2022 The choice of evaluation metrics.\n\u2022 The clinical requirements that the model aims to address.\n5.4 Deployment and Integration:\nTo deploy and integrate a fine-tuned BioBERT Lee et al. [2020]model into healthcare applications and systems, take the\nfollowing actions:\n\u2022 Apply regularization techniques to prevent overfitting, a potential issue when training the model on a limited\ndataset. Overfitting occurs when the model captures noise in the data rather than the underlying patterns.\nRegularization discourages the model from learning non-generalizable patterns.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc8ded55-c754-47fb-939d-6bcd74a47820": {"__data__": {"id_": "bc8ded55-c754-47fb-939d-6bcd74a47820", "embedding": null, "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "635cd94a-f448-45d2-871c-552bf4d5d189", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95cb7e0d42f1d7efc711cd4b1d6ca715481f09b42d68259b09d645fc20a1f384", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Augment dataset by artificially increasing its size. Employ techniques such as image translation, text generation,\nand synthetic data creation to enhance the dataset. Data augmentation bolsters the model\u2019s performance by\nincreasing its resilience to data noise and variations.\n\u2022 Integrate the model into the application or system, making it accessible for making predictions or recommen-\ndations. Embed the model within the application or system or provide an API for seamless access.\n\u2022 Ensure compliance with relevant healthcare regulations and privacy standards during the model\u2019s deployment.\nThis is crucial for safeguarding patient privacy and promoting responsible model usage. Be aware that\nhealthcare regulations and privacy standards can vary between regions.\nWhile deploying and integrating a fine-tuned BioBERT model into healthcare applications, consider the following:\n\u2022 Evaluate the model\u2019s performance on a held-out dataset to ensure its effectiveness with new data.\n\u2022 Continuously monitor the model\u2019s performance to confirm it meets expectations.\n\u2022 Regularly update the model to account for changes in the data.\n5.5 Continuous Improvement:\nContinuously update and fine-tune the model in response to new healthcare data availability or evolving clinical\nrequirements.\n\u2022 Seek feedback from healthcare professionals, leveraging their expertise in the field for model improvement.\nUse their insights to identify areas where the model underperforms or to uncover new potential applications.\n\u2022 Fine-tune the model using newly acquired healthcare data, applying the same training process employed in the\nmodel\u2019s initial training phase.\n\u2022 Experiment with various hyperparameters to optimize the model\u2019s performance for the specific task.\n\u2022 Apply regularization techniques to prevent overfitting, a concern that may arise when training the model on a\nlimited dataset.\n\u2022 Enhance the model\u2019s robustness by employing data augmentation techniques, making it more resilient to noise\nand data variations.\n\u2022 Continually monitor the model\u2019s performance to ensure it meets expectations. If performance deteriorates,\nconsider fine-tuning or updating it with fresh data.\n5.6 Documentation and Accessibility:\nComprehensively document the fine-tuned BioBERT model, including pre-trained weights and code, and make it\naccessible to the healthcare and research community. Provide comprehensive documentation, code, and model\ncheckpoints in various formats like a technical paper, a blog post, and a GitHub repository. This approach will expand\naccessibility to a broader audience.\n5.7 Ethical Considerations:\nTo ensure that the fine-tuned model addresses ethical concerns related to patient privacy and data security and that it\navoids inadvertently revealing sensitive patient information in compliance with healthcare regulations like HIPAA, the\nfollowing specific ethical considerations should be incorporated when using a fine-tuned BioBERT model:\n\u2022 Respecting Patient Privacy:Users must refrain from utilizing the model to access or disclose sensitive patient\ninformation, including patient names, medical records, and insurance details.\n\u2022 Enhancing Data Security: The model should be safeguarded against unauthorized access and use. This\nentails implementing measures like encryption and access control.\n\u2022 Mitigating Bias: Efforts should be made to prevent bias against any particular group of people. This can be\nachieved by employing a balanced dataset and avoiding discriminatory features.\n\u2022 Ensuring Transparency: The model must be transparent and interpretable. Users should have the capacity to\ncomprehend how the model operates and how it generates its predictions.\n\u2022 Establishing Accountability: Developers and users of the model bear responsibility for its actions. They are\nobligated to ensure the model\u2019s safe and responsible use.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf076420-90f7-428d-b2f9-37e1e98cea38": {"__data__": {"id_": "bf076420-90f7-428d-b2f9-37e1e98cea38", "embedding": null, "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f8e5fe2-bfbc-4721-81e1-50f7b5e42d1d", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5d337adf38851ad3c2fccbd87056cff9ebb14dec8404353a3c4ff63a8711799a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n6 Discussion\nBased on analyzing the selected works, we realized that LLMs have the potential to revolutionize healthcare. They\ncan introduce novel approaches to enhance clinical decision-making, facilitate information retrieval, and enable more\nnatural language interaction. We have explored the potential benefits and limitations of integrating these language\nmodels into healthcare applications. BioBERT\u2019s primary strength resides in its capacity to comprehend and process\nintricate biomedical and clinical texts. Its pre-training on an extensive corpus of biomedical literature provides it\nwith a robust foundation to accurately interpret medical terminologies, abbreviations, and concepts. Such capability\nproves indispensable in the healthcare context, where specialized language prevails. Moreover, BioBERT can undergo\nfine-tuning for specific applications, encompassing medical entity recognition, text classification, disease prediction,\nand question-answering. This adaptability empowers healthcare professionals to harness the model\u2019s capabilities across\na broad spectrum of clinical and administrative functions.\n6.1 Advantages of using BioBERT for healthcare applications\nBioBERT offers improved clinical decision support, representing one of its most promising applications. Healthcare\nproviders can utilize the model to swiftly access current medical knowledge, research articles, and patient records. This\nempowers them to render more informed decisions regarding diagnosis, treatment, and patient care, enhancing patient\noutcomes. BioBERT significantly enhances information retrieval efficiency from electronic health records (EHRs)\nand other clinical documents. Its ability to process and analyze extensive text data aids healthcare professionals in\npromptly accessing patient-specific information, thereby reducing the risk of overlooking critical data. The model\u2019s\nnatural language processing capabilities make it accessible to healthcare professionals, even those without technical\nbackgrounds. This promotes more effective communication between healthcare providers and technology, enhancing\nuser experience and adoption. However, we must recognize and tackle the challenges linked to deploying BioBERT in\nhealthcare:\n1. Data Privacy and Security: Healthcare data is highly sensitive and falls under stringent privacy regulations.\nTo ensure BioBERT\u2019s compliance with these regulations, such as HIPAA in the United States, it is crucial to\nprevent data breaches and safeguard patient information.\n2. Bias and Fairness: BioBERT, like other language models, can inherit biases present in the training data. This\nbias can lead to disparities in healthcare if not carefully mitigated. Developing techniques to identify and\nrectify bias in healthcare-specific contexts is essential.\n3. Lack of Transparency: Interpreting BioBERT\u2019s decisions can be challenging due to its complex architecture.\nEfforts to make the model more transparent and explainable are necessary to build trust among healthcare\nprofessionals.\n4. Quality Control: Ensuring the quality and accuracy of information generated or retrieved by BioBERT is\nparamount. Erroneous information or recommendations could have serious consequences in clinical settings.\n5. Resource Intensiveness: Developing, fine-tuning, and maintaining BioBERT for healthcare can be resource-\nintensive, requiring substantial computational power, data annotation, and expert oversight.\n6. Generalization Challenges: BioBERT may struggle with generalizing to specific healthcare domains, spe-\ncialties, or rare conditions if not adequately fine-tuned. Customization may be necessary to achieve optimal\nperformance.\nBioBERT holds immense potential for revolutionizing healthcare applications by improving information retrieval,\nclinical decision support, and natural language interaction. However, its deployment in the healthcare sector must\nbe accompanied by stringent measures to address privacy concerns, mitigate bias, ensure transparency, maintain data\nquality, allocate resources effectively, and fine-tune for specific healthcare contexts. With careful consideration and\nresponsible implementation, BioBERT can become a valuable tool for healthcare professionals, enhancing patient care\nand medical research.\n7 Conclusion and Future Work\nIn conclusion, this study offers valuable insights into how the Large Language Models can impact the healthcare sector.\nIt highlights the potential of enhancing various aspects of healthcare, applications such as improving patient care and\nstreamlining healthcare processes. However, challenges such as model performance and ethical considerations remain.\nFuture research shall focus on addressing the existing challenges and further harnessing the capabilities of LLMs by\nextending to the following dimensions.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c64596a9-9f3e-4437-8f92-72e1cf74c52a": {"__data__": {"id_": "c64596a9-9f3e-4437-8f92-72e1cf74c52a", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78a453aa-4ef7-49bd-918b-c021b1a277c3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "216b4e368ca2f3db5f00410cb0b4dd98d53b80c0326bdd6f66566fe5e080515a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b9cf5a8-5b0e-4b15-9fd7-9822cc708161", "node_type": "1", "metadata": {}, "hash": "768e3c79056fc65028d6010bd18cef16bda7bb2923e323b922d4f1cb8ca6d689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\n\u2022 Improving model performance\n\u2022 Extending NLP for downstream tasks.\n\u2022 Harnessing the capabilities of multimodal LLMs to provide a more comprehensive understanding of patient\nhealth.\n\u2022 Developing cost-effective methods for developing and deploying LLMs.\nReferences\nJiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen\nHu, et al. Prompt engineering for healthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670,\n2023a.\nPM Lavanya and E Sasikala. Deep learning techniques on text classification using natural language processing (nlp) in\nsocial healthcare network: A comprehensive survey. In 2021 3rd international conference on signal processing and\ncommunication (ICPSC), pages 603\u2013609. IEEE, 2021.\nMoreno La Quatra and Luca Cagliero. Transformer-based highlights extraction from scientific papers.Knowledge-Based\nSystems, 252:109382, 2022.\nSandeep Reddy. Evaluating large language models for use in healthcare: A framework for translational value assessment.\nInformatics in Medicine Unlocked, page 101304, 2023.\nSheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis\non medical image using large language models. arXiv preprint arXiv:2302.07257, 2023b.\nArya Rao, John Kim, Meghana Kamineni, Michael Pang, Winston Lie, and Marc D Succi. Evaluating chatgpt as an\nadjunct for radiologic decision-making. medRxiv, pages 2023\u201302, 2023.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on\nllama model using medical domain knowledge. arXiv preprint arXiv:2303.14070, 2023.\nPartha Pratim Ray. Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics,\nlimitations and future scope. Internet of Things and Cyber-Physical Systems, 2023.\nTianyong Hao, Xieling Chen, Guozheng Li, and Jun Yan. A bibliometric analysis of text mining in medical research.\nSoft Computing, 22:7875\u20137892, 2018.\nXieling Chen, Haoran Xie, Gary Cheng, Leonard KM Poon, Mingming Leng, and Fu Lee Wang. Trends and features of\nthe applications of natural language processing techniques for clinical trials text analysis. Applied Sciences, 10(6):\n2157, 2020.\nElena-Alexandra Costea. Machine learning-based natural language processing algorithms and electronic health records\ndata. Linguistic and Philosophical Investigations, (19):93\u201399, 2020.\nDina Demner-Fushman, Wendy W Chapman, and Clement J McDonald. What can natural language processing do for\nclinical decision support? Journal of biomedical informatics, 42(5):760\u2013772, 2009.\nSuzanne Henwood and Jim Lister. NLP and coaching for health care professionals: Developing expert practice. John\nWiley & Sons, 2007.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\nNing Kang, Bharat Singh, Zubair Afzal, Erik M van Mulligen, and Jan A Kors. Using rule-based natural language\nprocessing to improve disease normalization in biomedical text. Journal of the American Medical Informatics\nAssociation, 20(5):876\u2013881, 2013.\nSadid A Hasan and Oladimeji Farri. Clinical natural language processing with deep learning. Data Science for\nHealthcare: Methodologies and Applications, pages 147\u2013171, 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6b9cf5a8-5b0e-4b15-9fd7-9822cc708161": {"__data__": {"id_": "6b9cf5a8-5b0e-4b15-9fd7-9822cc708161", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78a453aa-4ef7-49bd-918b-c021b1a277c3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "216b4e368ca2f3db5f00410cb0b4dd98d53b80c0326bdd6f66566fe5e080515a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c64596a9-9f3e-4437-8f92-72e1cf74c52a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "eac156bfff7ec407e951dd6f6ed6e82b1daaabd51bfbd1416304f1765ee6463c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1907.11692,\n2019.\nNing Kang, Bharat Singh, Zubair Afzal, Erik M van Mulligen, and Jan A Kors. Using rule-based natural language\nprocessing to improve disease normalization in biomedical text. Journal of the American Medical Informatics\nAssociation, 20(5):876\u2013881, 2013.\nSadid A Hasan and Oladimeji Farri. Clinical natural language processing with deep learning. Data Science for\nHealthcare: Methodologies and Applications, pages 147\u2013171, 2019.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a\npre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240,\n2020.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav\nMani. Biomegatron: Larger biomedical domain language model. arXiv preprint arXiv:2010.06060, 2020.\n11", "mimetype": "text/plain", "start_char_idx": 3025, "end_char_idx": 3923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0a3f6a32-2797-43f2-8358-9a54d3d840a2": {"__data__": {"id_": "0a3f6a32-2797-43f2-8358-9a54d3d840a2", "embedding": null, "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "805f8da0-1849-4e62-86ae-138388034d4e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "56437a353fb32fa534bf4f16d5a17dc890bf625bc13887d7ce034d7de18e0b84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT A PREPRINT\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas,\nCheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records. NPJ\nDigital Medicine, 5(1):194, 2022a.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\npreprint arXiv:2212.13138, 2022.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu,\nZhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint\narXiv:2305.15075, 2023.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather\nCole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models.arXiv\npreprint arXiv:2305.09617, 2023.\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas,\nCheryl Martin, Mona G Flores, Ying Zhang, et al. Gatortron: A large clinical language model to unlock patient\ninformation from unstructured electronic health records. arXiv preprint arXiv:2203.03540, 2022b.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c3d612c-b103-4c1f-9fa5-b782e7074002": {"__data__": {"id_": "2c3d612c-b103-4c1f-9fa5-b782e7074002", "embedding": null, "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c430753-7c2d-414f-b707-e50f9c43f353", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5495482e69f49269890b3f2f2da0b5728b0aaa3007bfbeab01c65a4ef9d3c0ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 \n \nReview article / Perspective article \nLLMs-Healthcare : Current Applications and Challenges  of Large Language Models in \nvarious Medical Specialties \nUmmara Mumtaz, Awais Ahmed, Summaya Mumtaz \nAbstract \nWe aim  to present a comprehensive overview of the latest advancements in utilizing Large \nLanguage Models (LLMs) within the healthcare sector, emphasizing their transformative impact \nacross various medical domains.  LLMs have become pivotal in supporting healthcare , including \nphysicians, healthcare providers, and patients. Our review provides insight into the applications of \nLarge Language Models (LLMs) in healthcare, specifically focusing on diagnostic and treatment-\nrelated functionalities. We shed light on how LLMs are applied in cancer care, dermatology, dental \ncare, neurodegenerative disorders, and mental health, highlighting their innovative contributions \nto medical diagnostics and patient care. Throughout our analysis, we explore the challenges and \nopportunities associated with integrating  LLMs in healthcare, recognizing their potential across \nvarious medical specialties despite existing limitations. Additionally, we offer an overview of \nhandling diverse data types within the medical field. \n \nKeywords: Large language models ; Medical Special ties; Cancer; Mental Health;  Healthcare; \nDiagnosis and Treatments; Clinical Notes; Dermatology \n \n1. Introduction \nThe field of artificial intelligence (AI) has undergone a remarkable evolution in recent years, with \nsignificant advancements, particularly noticeable in natural language processing (NLP) and the \ndevelopment of Large Language Models (LLMs). These models represent a paradigm shift in AI's \ncapability to understand, generate, and interact using human language. At their foundation, LLMs \nare complex algorithms trained on vast, text -based documents and datasets [1]. Such extensive \ntraining allows them to recognize patterns adeptly , predict subsequent words in a sentence, and", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e0e1eba-71a0-4a00-b27a-0c2907614df0": {"__data__": {"id_": "1e0e1eba-71a0-4a00-b27a-0c2907614df0", "embedding": null, "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d097ee2-813f-447d-b568-38598951c098", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "dae9220b6c447f815cacbabc7270ed2853a6d054be8592690b3b2c67f9e9a8fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 \n \ngenerate coherent, contextually relevant text for the specified inputs, often called prompts within \nthe NLP community. This ability demonstrates the technical prowess of LLMs and signifies their \npotential to revolutionize how machines understand and process human language. One of the most \nprominent features of LLMs is their proficiency in processing and analyzing large volumes of text \nrapidly and accurately, a capability that far surpasses human potential in speed and efficiency [2]. \nThis quality makes them indispensable in areas requiring the analysis of extensive data sets. They \nare also known as \u201cfew -shot\u201d learners, meaning once trained on massive datasets, they can be \nretrained for new domains utilizing a small number of domain-specific examples[3].  \nLLMs have become increasingly prevalent in the medical domain, demonstrating their versatility  \nand expanding influence. Their applications in healthcare are multifaceted, ranging from \nprocessing vast quantities of medical data  and interpreting clinical notes  to generating \ncomprehensive, human -readable reports [4]. This broad spectrum of functionalities shows how \nLLMs are not just tools for data processing but are also instrumental in providing innovative \nsolutions across various aspects of healthcare. LLMs are increasingly being utilized to tackle \ncritical challenges in patient care. This includes providing customized educational content to \npatients, assisting healthcare professionals in making complex diagnostic decisions, and easing the \nadministrative burdens often associated with healthcare provision[4,5]. \nWhile large language models have been applied across a spectrum of activities in healthcare, \nincluding medical question answering, examination, pure research -oriented tasks, and \nadministrative duties in hospitals, this review will focus exclusively on their practical applications \nin healthcare, such as diagnostics and treatment purposes. We uncover their deployment in critical \nareas such as cancer care, dermatology, dental, and mental health. This exploration is crucial, as it \nshowcases LLMs\u2019 capacity to innovate medical diagnostics and patient care, streamline treatment \ntasks, and address the challenges and opportunities in harnessing their full potential  in complex \nmedical areas . We conduct an in -depth analysis of the applications of LLMs across different \nmedical fields, aiming to present a brief yet thorough summary. We focus on the advancements \nand challenges of integrating these sophisticated models into routine healthcare practices. We offer \ninsights into the current state of progress and identify  barriers to their widespread adoption in \nclinical settings. The paper is structured to cover each medical specialty and associated challenges,", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c86744b6-f16a-48ee-8ade-d146439499cd": {"__data__": {"id_": "c86744b6-f16a-48ee-8ade-d146439499cd", "embedding": null, "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8aaa4c8-dbb4-4d68-9958-a815ca1a9348", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5d7de0cc64d9f2c0d8c3ad4fa9aa4bdcdace734dd4ed084da4585ce2a5f70c14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 \n \nfollowed by examining various data types in the medical field. The conclusion summarizes the \nfindings and implications. \n \n \n2. Cancer Care (Oncology)  \nCancer is characterized by the uncontrolled growth of abnormal cells  in the body. It is examined \nwithin oncology\u2014studying cancer types and related factors. Adopting Large Language Models \n(LLMs) such as ChatGPT in oncology has become a focal point of recent research, especially in \nsupporting decision -making processes for cancer treatment. These advanced models are being \nexplored for their capability to enhance diagnos tic accuracy, personalize therapy options, and \nstreamline patient care in oncology. By analyzing vast amounts of data, LLMs can provide insights \nthat potentially improve treatment outcomes and patient management strategies. In the subsequent \ndiscussion, we explore the studies dedicated to integrating LLMs within oncological care , \nencapsulating the innovative efforts to harness LLMs' capabilities in enhancing the diagnostic, \ntreatment, and management processes associated with cancer care. \nIn a study conducted by Vera Sorin and Eyal Klang [6], the capabilities of ChatGPT, a large \nlanguage model (LLM), were explored as a decision -support tool for breast tumor boards. The \nresearch's primary objective was determining how ChatGPT's recommendations align with expert-\nFigure 1 : Visualizing LLM Applications in different medical specialties w.r.t input data type and medical use-case", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0654b99f-2090-43af-80b6-8f932d69533b": {"__data__": {"id_": "0654b99f-2090-43af-80b6-8f932d69533b", "embedding": null, "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dafd02e1-cc66-4f4d-9953-e2860149c41b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "aff4282fb1c353b9cafe690e96ea8df0eb5fa8cdf6f8313dea7ff39bd4a2b70f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 \n \ndriven decisions during breast tumor board meetings. For this purpose, clinical data from ten \npatients discussed in a breast tumor board at their institution was inputted into ChatGPT -3.5. \nSubsequently, the model's management recommendations were compared with the final decisions \nmade by the tumor board. Moreover, two senior radiologists independently evaluated ChatGPT's \nresponses, grading them on a scale from 1 (complete disagreement) to 5 (complete agreement) \nacross three categories: summarization of the case, the recommendation provided, and the \nexplanation for that recommendation. Most patients in the study, 80%, had invasive ductal \ncarcinoma, with one case each of ductal carcinoma in -situ and a phyllodes tumor with atypia. \nChatGPT's recommendations aligned with the tumor board's decisions in seven out of the ten cases, \nmarking a 70% concordance. Upon grading, the first reviewer gave mean scores of 3.7, 4.3, and \n4.6 for summarization, recommendation, and explanation , respectively, while the second \nreviewer's scores were 4.3, 4.0, and 4.3 in the same categories. As an initial exploration, the study \nsuggests that LLMs like ChatGPT could potentially be a valuable asset for breast tumor boards. \nHowever, as technology rapidly advances, medical professionals must know its advantages and \npotential limitations. \nIn a study by Stefan Lukac and Davut Dayan  in January 2023, the capabilities of ChatGPT to assist \nin the decision -making process for therapy planning in primary breast cancer cases were \ninvestigated[7]. Though the ChatGPT was able to identify specific risk factors for hereditary breast \ncancer and could discern elderly patients requiring chemotherapy assessment for cost/benefit \nevaluation, it generally offered non -specific recommendations concerning various treatment \nmodalities such as chemotherapy and radiation therapy. Notably, it made errors in patient-specific \ntherapy suggestions, misidentifying patients with Her2 1+ and 2+ (FISH negative) as candidates \nfor trastuzumab therapy and mislabeling endocrine therapy as \"hormonal treatment .\u201d The study \nconcluded that while ChatGPT demonstrates potential utility in clinical medicine, its current \nversion lacks the precision to offer specific therapy recommendations for primary breast cancer \npatients. It underscores the necessity for further refinement before it  can be a reliable adjunct in \nmultidisciplinary tumor board decisions. \nGeorges Gebrael assessed the utility of ChatGPT 4.0 to enhance triage efficiency and accuracy in \nemergency rooms for patients with metastatic prostate cancer [8]. Between May 2022 and April \n2023, clinical data of 147 patients presenting with metastatic prostate cancer were examined, of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f95f138a-ac5c-4368-9c1a-914766bf44aa": {"__data__": {"id_": "f95f138a-ac5c-4368-9c1a-914766bf44aa", "embedding": null, "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "496a0755-6c23-4b1c-9cbf-6f8480be3815", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "788bdcfd9b41c256ca17ea2ddf6da1e6b248a0748ac185f55b8176e52b263cc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 \n \nwhich 56 were selected based on inclusion criteria. ChatGPT demonstrated a high sensitivity of \n95.7% for determining patient admissions but had a low specificity of 18.2% for discharges. It \nagreed with physicians' primary diagnoses in 87.5% of cases. It outperformed physicians regarding \naccurate terminology usage (42.9% vs. 21.4%) and diagnosis comprehensiveness, having a median \ndiagnosis count of 3 compared to physicians' 2. ChatGPT was more concise in its responses but \nprovided more additional treatment r ecommendations than physicians. The data suggests that \nChatGPT could serve as a valuable tool for assisting medical professionals in emergency room \nsettings, potentially enhancing triage efficiency and the overall quality of patient care. \nA study led by Arya Rao et al.  investigated the potential of ChatGPT-3.5 and GPT-4 (OpenAI) in \naiding radiologic decision-making, specifically focusing on breast cancer screening and breast pain \nimaging services [9]. The researchers measured the models' responses against the ACR \nAppropriateness Criteria using two prompt formats: open -ended (OE) and select all that apply \n(SATA). For breast cancer screening, both versions scored an average of 1.830 (out of 2) in the \nOE format, but GPT-4 outperformed ChatGPT-3.5 in the SATA format, achieving 98.4% accuracy \ncompared to 88.9%. Regarding breast pain, GPT -4 again showed superiority, registering an \naverage OE score of 1.666 and 77.7% in SATA, while ChatGPT -3.5 scored 1.125 and 58.3%, \nrespectively. The data suggests the growing viability of large l anguage models like ChatGPT in \nenhancing radiologic decision -making processes, with potential benefits for clinical workflows \nand more efficient radiological services . However, further refinement and broader use cases are \nneeded for full validation. \nHana et al. conducted a retrospective study in February 2023 to evaluate the appropriateness of \nChatGPT's responses to common questions concerning breast cancer prevention and screening[10]. \nLeveraging methodologies from prior research that assessed ChatGPT's capacity to address \ncardiovascular disease-related inquiries, the team formulated 25 questions rooted in the BI-RADS \nAtlas and their clinical experiences within tertiary care breast imaging departments. Each question \nwas posed to ChatGPT three times, and three fellowship -trained breast radiologists critically \nassessed the responses . The radiologists categorized each response as \"appropriate,\" \n\"inappropriate,\" or \"unreliable\" based on the content's clinical relevance and consistency. Their \nevaluations considered two hypothetical scenarios: content for a hospital website and direct \nchatbot-patient interactions. The majority's opinion dictated the final determination of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b648c630-f976-400b-ae29-23a2dd6c5d5f": {"__data__": {"id_": "b648c630-f976-400b-ae29-23a2dd6c5d5f", "embedding": null, "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7a8c841-2634-4954-99ce-bf4241308431", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "43b3513c646f83338cd442203f8a0172896c719c21087908edabaa0e45e33377", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \n \nappropriateness. Results revealed that ChatGPT provided suitable answers for 88% (22 out of 25) \nof the questions in both contexts. However, one question  pertained to mammography scheduling \nin light of COVID-19 vaccination, which elicited an inappropriate response.  \nAdditionally, there were inconsistencies in answers related to breast cancer prevention and \nscreening location queries. While ChatGPT frequently referenced guidelines from the American \nCancer Society in its responses, it omitted those from the American College of Radiology and the \nUS Preventive Services Task Force. These findings aligned with earlier research by Sarraju et \nal.[11], where 84% of ChatGPT's cardiovascular disease prevention responses were deemed \nappropriate. Despite showing considerable potential as an automated tool for patient education on \nbreast cancer, ChatGPT exhibited certain limitations, emphasizing the essenti al role of physician \noversight and the ongoing need for further refinement and research into large language models in \nhealthcare education. \nBrian Schulte , in 2023, explored the ability of ChatGPT to identify suitable treatments for \nadvanced solid cancers[12]. Through a structured approach, the study assessed ChatGPT's capacity \nto list appropriate systemic therapies for newly diagnosed advanced solid malignancies and then \ncompared the treatments ChatGPT suggested with those recommended by the National \nComprehensive Cancer Network (NCCN) guidelines. This comparison resulted in the valid \ntherapy quotient (VTQ) measure . The research encompassed 51 diagnoses and found that \nChatGPT could identify 91 unique medications related to advanced solid tumors. On average, the \nVTQ was 0.77, suggesting a reasonably high agreement between ChatGPT's suggestions and the \nNCCN guidelines. Furthermore, ChatGPT always mentioned at least one systemic therapy aligned \nwith NCCN's suggestions. However, there was a minimal correlation between the frequency of \neach cancer type and the VTQ. In conclusion, while ChatGPT displays promise in aligning w ith \nestablished oncological guidelines, its current role in assisting medical professionals and patients \nin making treatment decisions still needs to be defined. As the model evolves, it is hoped that its \naccuracy in this area will be enhanced, but continued research is essential to fully understand and \nharness its potential. \nIn a study led by Julien Haemmerli  et al., the capability of ChatGPT  was explored in the context \nof CNS tumor decision-making, specifically for glioma management [13]. Using clinical, surgical, \nimaging, and immunopathological data from ten randomly chosen glioma patients discussed in a", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "158b692a-7ec1-4af5-bb42-4e2f6d3cc2ad": {"__data__": {"id_": "158b692a-7ec1-4af5-bb42-4e2f6d3cc2ad", "embedding": null, "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d350a4be-b8a0-4b03-b0d4-7c965fa18abd", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b13f2f9f5551025424c065532362300532fc20d284345535a85e53c39052e5b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 \n \nTumor Board, ChatGPT's recommendations were compared with those of seven CNS tumor \nexperts. While most patients had glioblastomas, findings revealed that ChatGPT's diagnostic \naccuracy was limited, with a notable discrepancy in glioma classifications. However, it \ndemonstrated competence in recommending adjuvant treatments, aligning closely with expert \nopinions. Despite its limitations, ChatGPT shows potential as a supplementary tool in oncological \ndecision-making, particularly in settings with constrained expert resources. \nIn Shan Chen et al.\u2019s research on the effectiveness of ChatGPT in offering cancer treatment advice, \nthe study scrutinized the model's alignment with the National Comprehensive Cancer Network \n(NCCN) guidelines for breast, prostate, and lung cancer treatments [14]. Through four diverse \nprompt templates, the study assessed if the mode of questioning influenced the model's responses. \nWhile ChatGPT's recommendations aligned with NCCN's guidelines in 98% of the prompts, 34.3% \nof these recommendations also presented inf ormation that needed to be more  in sync with the \nNCCN guidelines. The study concluded that, despite its potential, ChatGPT's performance in \nconsistently delivering reliable cancer treatment advice was unsatisfactory. Consequently, patients \nand medical professionals must  exercise caution when relying on ChatGPT and similar tools for \neducational purposes. \n2.1. Challenges associated with LLMs as a decision-support tool in Cancer Care: \nWhile integrating Large Language Models (LLMs) like ChatGPT into oncology shows promise, \nparticularly in decision support for cancer treatment, it also presents several critical challenges, as \ndiscussed in the previous section . These challenges must be addressed to ensure LLMs' safe and \neffective use in high-stakes medical environments. Firstly, the issue of accuracy and precision in \nLLMs is a significant concern. For instance, in Julien Haemmerli's [13] study on glioma therapy, \nChatGPT demonstrated limitations in accurately classifying glioma types. Similarly, the study by \nStefan Lukac and Davut Dayan [7] revealed errors in patient-specific therapy suggestions, such as \nmisidentifying patients for trastuzumab therapy. These inaccuracies highlight the risk of potential \nmisdiagnoses or inappropriate treatment recommendations, which could have profound \nimplications for patient care. \nAnother challenge is the capacity of LLMs to consider the comprehensive clinical picture, \nincluding patient functional status, which is often a nuanced judgment call made by experienced \nphysicians. ChatGPT's moderate performance in this area, as seen in Ha emmerli's study [13],", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fce1ba28-f54b-44b6-a24d-6a7e5ecd97e9": {"__data__": {"id_": "fce1ba28-f54b-44b6-a24d-6a7e5ecd97e9", "embedding": null, "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ef14852-83e2-4429-869e-3384de05af97", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "342aba6e97818a38e99e0d37bc57733531fcf08891b9213443bfa88cd8482a31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 \n \nindicates a gap between current LLM capabilities and the complex decision -making processes in \nmedical practice. Furthermore, the integration of LLMs into existing medical workflows raises \nconcerns. For example, Georges Gebrael's[8] study on triage in metastatic prostate cancer showed \nthat while ChatGPT had high sensitivity, its low specificity for discharges could lead to operational \ninefficiencies. Integrating LLMs within healthcare systems also poses challenges in data privacy, \ninteroperability, and the need for robust IT infrastructure. \nLastly, the role of LLMs in patient education and communication is not without limitations. Hana \nL Haver et al.[10] studies demonstrated inconsistencies in ChatGPT's responses to breast cancer \nprevention and screening questions . This inconsistency highlights the importance of human \noversight in verifying the information provided by LLMs, ensuring it aligns with established \nmedical guidelines and practices. In summary, while LLMs present exciting opportunities for \nenhancing cancer care, their current limitatio ns in accuracy, comprehensive clinical assessment, \nintegration into existing systems, and patient education necessitate a cautious and critical approach. \nThese models should be viewed as supplementary tools that augment, rath er than replace, the \nexpertise of medical professionals. Continuous evaluation, refinement, and ethical consideration \nare essential to harness the full potential of LLMs in oncology. \n3. Skin Care: Dermatology \nOur skin is a barrier against external threats such as viruses, bacteria, and other harmful organisms. \nDermatology is the branch of medicine dealing with skin diseases.  There has been a surge in cases \nrelated to skin diseases in the past years , affecting people of all ages [15].  Common skin-related \ndiseases include acne, alopecia, bacterial skin infections, decubitus ulcers, fungal skin diseases, \npruritus, and psoriasis[16]. Traditional dermatology diagnosis is based on a visual inspection of skin \nfeatures and subjective evaluation by a dermatologist[17]. The realm of dermatology diagnosis faces \nseveral significant challenges. Firstly, accurately interpreting skin disease imagery is complex due \nto the wide variety of skin conditions and their subtle visual differences. This task requires a high \nlevel of exp ertise, leading to the second challenge: a noticeable shortage of dermatologists, \nespecially in remote or underserved areas. Lastly, creating patient -friendly diagnostic reports is \nanother hurdle. These reports need to be detailed yet understandable to non -specialists, making \ntheir production time-consuming and labor-intensive for dermatologists.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "090ed244-b2d5-4ccb-a266-0d4fb5bcba9a": {"__data__": {"id_": "090ed244-b2d5-4ccb-a266-0d4fb5bcba9a", "embedding": null, "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "627c606b-520a-4fc2-883e-258ea18d1ba9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "46962a740c63428f21ee71cd71d916f8905852634d4bdad17018e7165e0d26cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 \n \nIn addressing the above challenges in dermatological diagnostics, Zhou et al. introduced SkinGPT-\n4, an innovative interactive dermatology diagnostic system underpinned by an advanced visual \nLarge Language Model [18]. This study was mainly focused on tackling the prevalent issues in \ndermatology, such as the shortage of specialized medical professionals in remote areas, the \nintricacies involved in interpreting skin disease images accurately, and the demanding nature of \ncreating patient-friendly diagnostic reports. SkinGPT-4, utilizing a refined version of MiniGPT-4, \ntrained on an extensive dataset that included 52,929 images of skin diseases, both from public \ndomains and proprietary sources, along with detailed clinical concepts and doctors' notes. This \ncomprehensive training on skin -related disease images enabled SkinGPT -4 to articulate medical \nfeatures in skin disease images using natural language  and make  precise diagnoses. The \nfunctionality of SkinGPT-4 allows users to upload images of their skin conditions, after which the \nsystem autonomously analyzes these images. It identifies the characteristics and categorizes the \nskin conditions, performs an in -depth analysis, and provides interactive treatment \nrecommendations. A notable aspect of SkinGPT-4 is its local deployment feature, combined with \na solid commitment to maintaining user privacy, making it a viable option for patients seeking \naccurate dermatological assessments. To ascertain the efficacy of SkinGPT-4, the study conducted \na series of quantitative evaluations on 150 real-life dermatological cases. Certified dermatologists \nindependently reviewed these cases to validate the diagnoses provided by SkinGPT-4. Among the \n150 cases, a commendable 78.76% of the diagnoses rendered by SkinGPT -4 were validated as \neither accurate or relevant by the dermatologists, breaking down into 73.13% that firmly aligned \nand another 5.63% that agreed. The outcomes of this evaluation underscored the accuracy of \nSkinGPT-4 in diagnosing skin diseases. While SkinG PT-4 is not positioned as a replacement for \nprofessional medical consultation, its contribution to enhancing patient comprehension of medical \nconditions, improving communication between patients and doctors, expediting dermatologists' \ndiagnostic processes,  and potentially fostering human -centered care and healthcare equity in \nunderdeveloped regions is significant. \n3.2. Challenges associated with utilizing LLMs in Dermatology: \nThe introduction of SkinGPT-4 by Zhou et al. marks a significant advancement in dermatological \ndiagnostics, addressing challenges like the dermatologist shortage and the complexities of skin \ndisease image interpretation and patient -friendly report generation [18]. Despite its innovative", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbdb4073-6df6-4699-9252-82e8356c8c34": {"__data__": {"id_": "fbdb4073-6df6-4699-9252-82e8356c8c34", "embedding": null, "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5fb1c77f-0fd2-4253-8ce5-987c9c6b9718", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "790a45c0dbfee8755e0c23544c350e2103ef7fce1c033a4172ef8706b241b84f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 \n \napproach and the training on an extensive dataset to articulate medical features in skin images, \nthere are inherent challenges. Some challenges associated with deploying SkinGPT-4 include \nensuring consistent diagnostic accuracy across various skin conditions, safeguarding patient \nprivacy while managing sensitive health data, and integrating the technology seamlessly into \nexisting healthcare systems. Additionally, despite SkinGPT -4's high diagnostic accuracy, \ncontinuous human oversight in medic al diagnosis a nd treatment planning remains critical to \ncomplement the AI's capabilities with professional medical judgment and ensure optimal patient \ncare outcomes. Additionally, advancements might focus on developing models that can adapt to \nnew, emerging skin conditions and leveraging telemedicine to extend dermatological care to \nremote areas, thus promoting healthcare equity. \n \n4. Neurodegenerative Disorders: Dementia & Alzheimer's  \nNeurodegenerative disorders involve the gradual deterioration of specific neuron groups, differing \nfrom the non -progressive neuron loss seen in metabolic or toxic conditions. These diseases are \ncategorized by their primary symptoms (such as dementia, parkinsonism, or motor neuron disease), \nthe location of neurodegeneration within the brain (including frontotemporal degenerations, \nextrapyramidal disorders, or spinocerebellar degenerations), or by the underlying molecular \nabnormalities[19]. Dementia is a broad category of brain diseases that cause a long -term and often \ngradual decrease in the ability to think and remember, affecting daily functioning. Alzheimer's \ndisease (AD) is the most common cause of dementia, characterized by memory loss, language \nproblems, and unpredictable behavior. \nLLMs such as Google Bard and ChatGPT have emerged as valuable tools for predicting \nneurodegenerative disorders. A study by Koga et al. evaluated these models' predictive accuracy \nusing cases from Mayo Clinic conferences [20]. Using the Mayo Clinic brain clinicopathological \nconferences as their sample pool, the researchers extracted 25 cases of neurodegenerative disorders. \nThese clinical summaries were then utilized for training and  testing the models. The diagnoses \noffered by each model were compared against the official diagnosis provided by medical \nprofessionals. Findings from the study highlighted that  ChatGPT-3.5 aligned with 32% of all the \nphysician-made diagnoses, Google Bard with 40%, and ChatGPT -4 with 52%. When assessing \nthe accuracy of these diagnostic predictions, ChatGPT -3.5 and Google Bard both achieved a", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2611, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "67a0324c-6fe7-40b1-be46-627a9ad2233e": {"__data__": {"id_": "67a0324c-6fe7-40b1-be46-627a9ad2233e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "727ae153-069f-4bad-9283-6c25b5eed07f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0eac3227e5c1e41f8db631838e581653da7e98abec7c6b5367815dea4d7c4ad2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 \n \ncommendable score of 76%, while ChatGPT -4 led the pack with an impressive accuracy rate of \n84%. The evident proficiency exhibited by LLMs, specifically ChatGPT and Google Bard, \nhighlights their considerable potential in revolutionizing diagnostic processes in \nneurodegenerative disorders. \nThis study conducted by Agbavor and Liagn (2022)  explored the use of GPT -3-generated text \nembeddings to predict dementia, utilizing data from the ADReSSo Challenge  (Alzheimer\u2019s \nDementia Recognition through Spontaneous Speech  only challenge[22]), which focuses on \nidentifying cognitive impairment through spontaneous speech [21]. The author proposed using the \nmodel to identify individuals with dementia against healthy individuals as controls. Using the 237 \nspeech recordings  derived from the ADReSSO  (Alzheimer\u2019s Dementia Recognition through \nSpontaneous Speech only challenge), the author used a 70/30 split and obtained 71 data s amples \nas the testing set and 166 as the training set. In the training set, 87 individuals had AD, and 79 \nwere healthy controls. GPT-3 was innovatively used for embedding the transcribed speech texts. \nThen, the model extracts the acoustic features such as temporal analysis (periodicity of speech, \npause rate, phonation rate, etc.) and speech production (vocal quality, articulation, prosody, etc.). \nThese features serve as the input for the classification model used in AD prediction . GPT-3 \nembeddings are then compared with BERT and traditional acoustic features. The findings reveal \nthat text embeddings outperform traditional acoustic methods and compare well with fine -tuned \nmodels such as BERT. This suggests that GPT-3's text embeddings offer a promising approach for \nearly dementia diagnosis. \nAnother study conducted by Mao and colleagues [23] outlines developing and applying  a deep \nlearning framework utilizing the BERT model for predicting the progression from Mild Cognitive \nImpairment (MCI) to Alzheimer's Disease (AD) using unstructured EHR notes.  The study  \ncataloged 3,657 MCI -diagnosed patients and their clinical notes from  Northwestern Medicine \nEnterprise Data Warehouse (NMEDW) between 2000 and 2020, using only their initial MCI \ndiagnosis notes for analysis. These notes underwent de-identification, cleaning, and segmentation \nbefore training an AD -specific BERT model (AD -BERT). AD-BERT transformed patient note \nsections into vector forms, which a fully connected network analyzed to predict MCI -to-AD \nprogression. For validation, a similar methodology was applied to 2,563 MCI patients from  Weill", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2c09db04-2675-4b0b-9bbc-de3ad5d1c664": {"__data__": {"id_": "2c09db04-2675-4b0b-9bbc-de3ad5d1c664", "embedding": null, "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17e04d86-1cca-4f25-85db-eaeafa02e575", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "72bc7609f560eda7304e94ae9bd8b88b8e62b19bec15717aadbd811cc1f33210", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 \n \nCornell Medicine (WCM) . AD -BERT outperformed seven baseline models, showing superior \naccuracy in both patient groups, evidenced by its AUC and F1 scores.  \nIn the diagnosis of complex conditions like Alzheimer's disease, medical professionals use a \nvariety of data such as images, patient demographics, genetic profiles, medication history, \ncognitive assessments, and speech data. Some of the recent studies have proposed multi -modal \nAD diagnosis or prediction methods lever aging the popular pre -trained large language model \n(LLM) to add text data sources, in addition to images and other data types[24,25-26].  \n4.1. Challenges associated with LLMs in Neurodegenerative disorders \nUtilizing LLMs in diagnosing and managing neurodegenerative disorders like dementia and \nAlzheimer's disease presents several challenges. Firstly, the complexity and variability of these \nconditions require highly accurate and deep understanding, which LLMs may not always provide \ndue to limitations in their training data. The ethical and privacy concerns about handling sensitive \npatient data pose significant hurdles. Furthermore, integrating these models into clinical workflows \ndemands substantial validation to ensure they  complement, rather than complicate, healthcare \nprofessionals' decision -making processes. Lastly, there's a need for continuous updates and \nimprovements in these models to keep pace with the latest medical research and clinical practices \n5. Dentistry  \nThe World Health Organization reports that oral diseases impact approximately 3.5 billion \nindividuals globally, with dental caries, periodontal diseases, and tooth loss being the most \nprevalent. These conditions, largely preventable and manageable with ear ly diagnosis, have seen \nthe application of AI methodologies in recent years, including the diagnosis of dental caries [27, 28]  \nand periodontitis[29]. Despite this, exploring Large Language Models (LLMs) in dentistry remains \nnotably scarce, with limited studies demonstrating their practical application.  \nHuang et al. stand out by proposing LLM -based deployment strategies within dentistry, marking \nthe emerging area of research with significant potential for advancement [29]. To showcase the \neffectiveness and potential of applying Large Language Models (LLMs) in dentistry, this work \nintroduced a framework for an automated diagnostic system utilizing Multi -Modal LLMs. This \ninnovative system incorporate d three distinct input modules: visual, auditory, and textual data, \nenabling comprehensive analysis. Visual inputs, such as dental X-rays and CT scans, are evaluated", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "12af3b18-6bc7-41e3-9fe5-494324982f8a": {"__data__": {"id_": "12af3b18-6bc7-41e3-9fe5-494324982f8a", "embedding": null, "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2e93cd9-3b0b-4f27-ac99-059e8a76b03e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ad682fdd4e78762c4ea6d06fdad20d2498e1b03064f33e0bd3b1a109ab8407c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 \n \nfor anomalies using vision -language models, facilitating precise diagnostics. Audio inputs serve \ndual purposes: detecting voice anomalies and understanding patient narratives, which are \nconverted to text for further analysis  by LLM. To illustrate the capabilities of the multi -modal \nLLM AI system in dental practice, Huang et al. proposed its application in diagnosing and planning \ntreatment for dental caries. The process begins with inputting a tooth's X-ray into the system, where \nvision-language modeling is employed to detect any decay on the tooth. Once identified, the \nsystem utilizes LLM to propose a comprehensive treatment plan, articulated through seven detailed \nsteps. These steps range from initial patient communication to scheduling follow-up appointments, \nhighlighting a thorough approach to patient care. Despite its advanced diagnostics, the system's \nlimitations, such as failing to detect potential bone loss, are acknowledged, suggestin g areas for \nfurther research and development to enhance its effectiveness in dental diagnostics. \n5.1.  Challenges associated with dental care:  \nThe accuracy of LLMs like ChatGPT depends on the availability of high -quality, relevant dental \ndata. A significant hurdle in designing and training LLMs for dental care is limited access to the \ndental records owned by private dental clinics and concerns over patient privacy, which restricts \naccess to comprehensive and current datasets. LLMs\u2019 development and effectiveness in dentistry \nmust navigate these challenges, ensuring access to extensive, up -to-date information while \naddressing privacy and ownership issues to avoid biases and maintain data integrity. \nThe potential of LLMs in dental healthcare seems promising and can revolutionize how dental \nprofessionals diagnose, treat, and manage patient care today. LLMs could significantly improve \ndiagnostic precision by leveraging the vast amounts of data available in patient records and \nimaging, allowing for early detection and intervention in dental conditions. Furthermore, the \nability of LLMs to generate personalized treatment plans and educational materials tailored to \nindividual patient needs could enhance the effectiveness of patient care. This personalization and \nthe model\u2019s ability to process and analyze data swiftly  could lead to more efficient and patient -\ncentered dental healthcare practices. As LLMs continue to evolve, their integration into dental \nhealthcare is expected to deepen, offering innovative solutions to longstanding challenges and \nimproving patient outcomes worldwide. \n6. Mental Health: Psychiatry and Psychology", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cb13c36b-d5d4-4a9f-8491-4ca9a13a3987": {"__data__": {"id_": "cb13c36b-d5d4-4a9f-8491-4ca9a13a3987", "embedding": null, "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88a2232d-8553-4d2d-8011-b24a5c3e7370", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5dec5bad92d3ed5c9057413d101a7bb4107e7f00d86859fdb8a6698fcb8424f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 \n \nMental health disorders, which affect millions globally, significantly reduce the life quality of \nindividuals and their families. In psychiatry, LLMs have the potential to refine diagnostic precision, \noptimize treatment outcomes, and enable more tailored patient care, moving beyond traditional, \nsubjective diagnostic approaches prone to inaccuracies. By leveraging AI to analyze extensive \npatient data, it's possible to uncover patterns not easily detectable by humans, thereby improving \ndiagnosis[28,29]. \nGalatzer-Levy and colleagues, in 2023, delved into exploring the potential role of large language \nmodels (LLM) in psychiatry [30]. Their primary investigation tool  was Med -PALM 2, an LLM \nequipped with comprehensive medical knowledge. The model was trained and tested using a blend \nof clinical narratives and patient interview transcripts. The dataset encompassed expert evaluations \nusing instruments like the 8 -item Patient Health Questionnaire and the PTSD Checklist -Civilian \nVersion (PCL -C). The study  intended to gauge the severity of PTSD using the PCL -C while \nemploying the PHQ -8 to assess depression and anxiety levels. The evaluation process involved \nextracting from Med -PALM 2 clinical scores, the rationale for such scores, and the model's \nconfidence in its derived results. The gold standard for this evaluation was the DSM 5 (Diagnostic \nand Statistical Manual of Mental Disorders, Fifth Edition). The researchers' rigorous testing \nprocess involved the analysis of 46 clinical case studies, 115 PTSD evaluations, and 145 \ndepression instances. These were probed using prompts to tease out diagnosti c information and \nclinical scores. The rigorous assessment also saw Med -PaLM 2 fine-tuned through many natural \nlanguage applications and a substantial textual database. Notably , research -quality clinical \ninterview transcripts were employed as inputs when assessing the model's efficacy. Med-PaLM 2 \ndemonstrated its prowess in evaluating psychiatric states across various psychiatric conditions. \nRemarkably, when tasked with predicting psychiatric risk from clinician and patient narratives, \nthe model showcased an impressive accuracy rate ranging between 80% and 84%. \nAnother study  evaluated the performance of various LLMs, including Alpaca and its variants, \nFLAN-T5, GPT-3.5, and GPT -4, across different mental health prediction tasks such as mental \nstate (depressed, stressed or risk actions like suicide) using online text [31]. Through extensive \nexperimentation, including zero-shot, few-shot, and instruction fine-tuning methods, it was found \nthat instruction fine -tuning notably enhances LLMs' effectiveness across all tasks. Notably, the", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6ce1d6a6-51d7-4efe-bae1-784868504592": {"__data__": {"id_": "6ce1d6a6-51d7-4efe-bae1-784868504592", "embedding": null, "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75f1f244-a235-4305-9c7f-2cb28e21c5f3", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a0d2dd47bac8d9acf0415a6b2e00380b03a9deadb7ed64f125058e3fb78410d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 \n \nfine-tuned models, Mental -Alpaca and Mental -FLAN-T5, demonstrated superior performance \nover larger models like GPT-3.5 and GPT-4 and matched the accuracy of task-specific models.  \nThe use of conversational agents based on LLMs for mental well-being support is growing, yet the \neffects of such applications still need to be  fully understood. A qualitative study by Ma et al. of \n120 Reddit posts and 2,917 comments from a subreddit dedicated to mental health support apps \nlike Replika reveals mixed outcomes [32]. While Replika offers accessible, unbiased support that \ncan enhance confidence and self -exploration, it struggles with content moderation, consistent \ninteractions, memory retention, and preventing user dependency, potentially increasing social \nisolation. \nFollowing the advancements with ChatGPT, research into automated therapy using AI's latest \ntechnologies is gaining momentum. This new direction aims to shift mental health assessments \nfrom traditional rating scales to a more natural, language -based communication. The emergence \nof large language models, like those powering ChatGPT and BERT, marks a significant shift in \nartificial intelligence, potentially revolutionizing standardized psychological assessments. This \nevidence points towards AI's capacity to transform mental health evaluations into interactions that \nmirror natural human communication, pending comprehensive validation in specific application \nscenarios[33]. \n6.1. Challenges associated with applications of LLMs for Mental Health \nIn mental health applications, LLMs face challenges like ensuring content sensitivity and safety to \navoid harmful advice, maintaining accuracy and reliability to prevent misdiagnoses, and offering \npersonalized, empathetic responses for adequate support. Data privacy and security are paramount \ndue to the personal nature of discussions. There's also a need to prevent user over -reliance on \nLLMs, potentially deterring professional help. Ethical considerations include the impact of \nreplacing human interactions w ith AI and avoiding biases. Additionally, navigating regulatory \ncompliance within mental health laws and guidelines is crucial for lawful operation. \n8 Other Medical Specialties: Nephrology, Gastroenterology, Allergy and immunology \nThe integration of Large Language Models into medical specialties like nephrology and \ngastroenterology remains in the early stages, with their full potential yet to be realized. Current \napplications in these areas are sparse, highlighting opportunities for future exploration and", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a2c9d2b5-1c33-434f-9976-f0dc07b82b15": {"__data__": {"id_": "a2c9d2b5-1c33-434f-9976-f0dc07b82b15", "embedding": null, "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1fe440f1-aa33-4c34-81c3-52cabb12e457", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "72e8efd29878ea65f7f15f81c0a2cb1e2c6c578d685b5bbfc685b614fc2ff9ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 \n \nimplementation. This brief overview aims to shed light on the existing implementations of LLMs \nwithin these specific fields, indicating the nascent but promising role of advanced AI technologies \nin enhancing diagnostic and treatment methodologies in nephrology and gastroenterology. \n8.1.Nephrology \nWithin the domain of nephrology, LLMs are being utilized to assist in diagnosing kidney diseases, \nproviding treatment guidance, and monitoring  renal function, as noted by Wu and colleagues[34]. \nThese LLMs facilitate the evaluation of crucial data such as laboratory results, clinical data, and a \npatient's medical history during the diagnostic phase. As such, the LLMs chosen for nephrological \napplications are often preferred to possess a sophisti cated medical knowledge capability, \nespecially in  multiple-choice medicine test -taking. Various LLMs, including Orca Mini 13B, \nStable Vicuna 13B, Falcon 7B, Koala 7B, Claude 2, and GPT-4, have found applications in treating \nand diagnosing kidney diseases. However, owing to their unique zero-shot reasoning capabilities, \nGPT-4 and Claude 2 are particularly suitable for this intricate medical specialty. Currently, these \nmodels are employed to respond to multiple -choice questions about nephrology. Wu et al.  \nincorporated questions from clinical backgrounds linked to 858 nephSAP multiple-choice queries \ncollated between 2016 and 2023. When evaluating the proficiency of Claude 2 and GPT -4, \nperformance was gauged based on the proportion of correctly answered neph rology-related \nnephSAP multiple-choice questions. GPT-4 demonstrated superior performance, garnering a score \nof 73.3%, in contrast to Claude 2, which achieved a score of 54.4%. When individual nephrology \ntopics were examined, GPT -4 consistently outperforme d its counterparts, including Claude 2, \nVuna, Kaola, Orca-mini, and Falcon. \n8.2. Gastroenterology \nLahat et al. explored the capabilities of large language models, specifically OpenAI's ChatGPT, in \nresponding to queries within the realm of gastrointestinal health [35]. Their evaluation employed \n110 real -world questions, benchmarking ChatGPT's responses against the expert consensus of \nseasoned gastroenterologists. These queries spanned a spectrum of topics, from diagnostic tests \nand prevalent symptoms to treatments for a range of gastrointestinal issues. The source of these \nquestions was public internet platforms. The researchers evaluated the outputs of ChatGPT on \nmetrics such as accuracy, clarity, up-to-dateness, and efficacy, rating them on a scale from 1 to 5. \nThese outputs were then categorized into symptoms, diagnostic tests, and treatments. ChatGPT", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7bb9d668-765b-4cab-a2d7-ede1e49fc77f": {"__data__": {"id_": "7bb9d668-765b-4cab-a2d7-ede1e49fc77f", "embedding": null, "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97eb801-302b-491e-a50d-3561ed42ddc7", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ab96e175e78b42866ba38791643d7185892b7a2392c0b2ed246d3b0286355607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17 \n \naveraged scores of 3.7 for clarity, 3.4 for accuracy, and 3.2 for efficacy in the symptom category . \nDiagnostic test-related queries resulted in scores of 3.7 for clarity, 3.7 for accuracy, and 3.5 for \nefficacy. As for treatment-related questions, the model achieved 3.9 for clarity, 3.9 for accuracy, \nand 3.3 for efficacy. The results indicated the subst antial potential of ChatGPT in providing \nvaluable insights within the gastrointestinal specialty. \n8.3. Allergy and immunology \nIn allergy and immunology, LLMs akin to their applications in dermatology, have shown \npromising potential. According to a study by Goktas et al., LLMs, specifically models like GPT-4 \nand Google Med -PaLM2, significantly enhance  the diagnostic process within allergy and \nimmunology disciplines[36]. These advanced models elevate the precision of diagnosis and  can \ntailor treatment plans to suit individual patient needs. Beyond the clinical realm, they also play a \npivotal role in fostering patient engagement, ensuring patients are actively involved and informed \nin their healthcare journey. As a result, the integration of LLMs in allergy and immunology \nrepresents a paradigm shift towards more accurate, personalized, and patient-centric medical care. \nSection 9: Handling different types of data in the medical industry \nThis section provides  an overview of how different data formats and types are handled in the \nmedical industry when used as training data or inputs for a large language model. \n9.1. Clinical Notes  \nClinical notes, an integral component of patient health records, have increasingly been utilized in \nmedicine as input to large language models (LLMs). These notes, typically generated by healthcare \nprofessionals, serve as rich patient information repositories, including  their medical history, \npresent symptoms, diagnoses, treatments, and more. Clinical notes are fed into LLMs to extract \nmeaningful patterns, predictions, and insights. Before using these notes, they are often \npreprocessed to ensure they are in  a format that's easily digestible for the models. This \npreprocessing can involve converting handwritten notes into digital formats, anonymizing patient \ndata to maintain privacy, and structuring the data in a consistent format. LLMs can directly process \nthese notes and produce a range of tools suited for activities like condensing medical data, assisting \nin clinical decisions, and creating medical reports [37]. To utilize clinical notes in LLMs, prompts \ncontaining questions, scenarios, or comments about the note are used, such as \"Assume the role of", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1e8ba4c3-917a-4fe2-bbdf-05493604fcb4": {"__data__": {"id_": "1e8ba4c3-917a-4fe2-bbdf-05493604fcb4", "embedding": null, "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "261b10ec-031a-4407-8267-231ce7fda391", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "de60a238cc299b5808d8c677e9aa58145c1025bbf07cf6345cff9a2351d519b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 \n \na neurologist at the Mayo Clinic brain bank clinicopathological conference.\" Based on this, the \nmodel provides an output that aids in evaluation or diagnosis across different medical fields[37]. \n9.2.X-rays/ Images  \n  X-rays are medical imaging that utilizes ionizing radiation to produce images of internal body \norgans. This data type may include CT scans (tomography), chest X -rays, and bone X-rays. In \nmedicine, X-ray images can be processed by a computer -aided detection (CAD) model, which is \npre-trained to derive the outputs in tensor form. These tensors are then translated into natural \nlanguage, where they can be used as LLM input to generate  summaries or descriptions of the X -\nray images. Wang et al. illustrated how the X-rays of exam images are handled for utilizing them \nwith the LLMs[38]. They established that the model is fed into pre -trained CAD models to derive \nthe output. Then, translate the tensor (output) into natural language. Lastly, the language models \nare used to make conclusions and summarize the results. They establish that X-ray images can be \nused as input in the LLM and fed into the model with prompts to generate the image summarization \nor descriptive caption. The LLM supports visual question answering, where the x -ray images of \nthe patients are fed into an image encoder (BLIP -2), where the natural language presentation is \ngenerated and embedded based on the image understanding. \nBazi and colleagues proposed a transformer encoder-decoder architecture to handle the visual data \nwhen using the LLM [39]. They extracted the image features using the vision transformer (ViT) \nmodel and then used the textual encoder transformer to embed the questions. It is then fed to the \nresulting textual and visual representations into a multi-modal decoder to generate the answers. To \ndemonstrate how LLM handles the visual data, they used VQA datasets for radiology images, \ntermed PathVQA and VQA-RAD. In decoding the radiology images, the proposed model achieved \n72.97% and 8.99% for the VQA-RAD and 62.37% or 83.86% for PathVQA. \n9.3.Radiological reports \nRadiological reports are documents from radiologists that present the findings or interpretation of \nmedical imaging studies such as MRIs, X -rays, and CT scans. These data are processed as texts \nwithin the report to be input for LLMs in medicine. After data augmentation, the radiological \nreports are used as inputs in the LLM model. Tan and colleagues collected 10,602 CT scan reports \nfrom patients with cancer at a single facility[40]. These reports were categorized into four response", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "575d903a-b2cf-4616-90b2-4d7b3bf52ca5": {"__data__": {"id_": "575d903a-b2cf-4616-90b2-4d7b3bf52ca5", "embedding": null, "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "274809b3-12fc-4a47-b044-a014509ebad1", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "213e64d73b0244e05ead4c53f07d699e1b295b0636859482be4623f87aa83926", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19 \n \ntypes: no evidence of disease, partial response, stable disease, or progressive disease. To analyze \nthese reports, we utilized various models , including transformer models, a bidirectional LSTM \nmodel, a CNN model, and traditional machine learning approaches. Techniques such as data \naugmentation through sentence shuffling with consistency loss and prompt-based fine-tuning were \napplied to enhance the performance of the most effective models. \n9.4.Speech data \nSpeech data, encompassing medical interviews, consultations, and patient audio interactions, \nserves as a valuable reservoir of information. Before its use in Large Language Models (LLMs), \nthis data is converted into a textual format through automatic speec h recognition (ASR) systems. \nNotably, converting audio data into text is accomplished using pre-trained models, with Wav2vec \n2.0 emerging as a leading contender in speech recognition technology. In their groundbreaking \nwork, Agbavor and Liang [21] employed the Wav2vec2 -base-960 base model, an advanced tool \nfine-tuned on an extensive 960 -hour dataset of 16 kHz speech audio. Their methodology \nincorporated Librosa for audio file loading and Wav2Vec2Tokenizer for the crucial task of \nwaveform audio token ization. These tokenized audio segments are inputted into the \nWav2Vec2ForCTC model depending on memory capacities . This model decodes the tokens, \nresulting in the generation of text transcripts. Furthermore, an alternative approach to leveraging \nspeech data in LLMs involves using open MILE, an open -source toolkit. Open MILE  offers \nfunctionalities like speech classification and facilitates extracting audio features from speech or \nmusical signals, proving its versatility in handling audio data for various applications.  \n9.5.Tabular Data \nIn the medical domain, tabular data typically encompasses clinical measurements, patient records, \nand lab outcomes, arranged methodically in a matrix of rows and columns. A transformation via \ntabular modeling is requisite for this structured data to be effectively utilized by Large Language \nModels (LLMs). The ubiquity of this tabular format in clinical and physician databases has often \nled to the use of tree-based models like bagging and boosting. However, these models come with \ntheir share of limitations. Highlighting an innovative approach to this challenge, Chen et al.  \npresented a study employing a data set of 1479 patients undergoing immune checkpoint blockade \n(ICB) treatments for various cancer types[41]. Segmenting the dataset, with 295 patients for testing \nand 1184 for training, they unveiled how LLMs process tabular data. Crucial to this process is", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "42c38932-b5f3-4ca1-9d7e-9b2a9f9a6964": {"__data__": {"id_": "42c38932-b5f3-4ca1-9d7e-9b2a9f9a6964", "embedding": null, "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb3bd1fc-a037-4475-9bcf-87df0437c95b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "050d26c1ecdac9e850d959dd32fdc14dc1dec115325d990456a10e32afa17162", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 \n \nserializing the feature columns into coherent sequences of natural language tokens that the LLM \ncan interpret. This serialization can be achieved through various methods, be it the prompting -\nbased regeneration approach, using {attribute} is {value} functions , or manual serialization \ntemplates.  \nFurthermore, Chen and his team introduced an advanced tabular model, ClinTaT, augmented from \nits original design. This refined model incorporates a continuous embedding layer harmonized \nwith multiple distinct layers that mirror the table's continuous feature count. Continuous variables \nare melded with embedded categorical data for the final processing step, which is then channeled \ninto the transformer for analysis. \n10. Conclusion  \nLarge Language Models (LLMs) applications have carved out a transformative niche in the \nhealthcare sector. From patient engagement and education to diagnostic assistance, administrative \nsupport, and medical research, the multifaceted applications of LLMs have demonstrated their \npotential to optimize  various facets of the medical landscape. Their expansive knowledge \nrepositories and adeptness at understanding context and generating human -like textual responses \nhave positioned LLMs as invaluable assets within the healthcare domain. Their integration with \nchatbots offers a more personalized and efficient patient experience, aiding in tasks ranging from \nmedication clarification to mental health support. On the diagnostic front, incorporating LLMs \nwith electronic health systems and medical imaging promises to enhance the accuracy and \nefficiency of diagnosis and treatment plans. LLMs' capability to assist in clinical documentation, \nmedical language translation, and medical education for patients highlights their adaptability and \nrelevance in varied healthcare scenarios. \nHowever, while the benefits of LLMs are numerous, their practical application in the healthcare \nsector also underscores the importance of precision, context awareness, and ethical considerations, \ngiven the critical nature of medical decision-making. While LLMs like ChatGPT and Med-PaLM \nhave shown significant potential, there's an imperative for ongoing refinement, especially when \nhandling complex or rare medic al cases. As LLMs become more integrated into patient care, \nresearch addressing the ethical implica tions, including data privacy, the balance between \nautomation and human intervention, and informed patient consent, will be paramount. \nCollaborative research exploring the fusion of LLMs with other emerging technologies, such as", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00dd7c54-2741-4adb-9aef-5cbbd5ac3c9f": {"__data__": {"id_": "00dd7c54-2741-4adb-9aef-5cbbd5ac3c9f", "embedding": null, "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50e0664c-c917-4d0e-a0d5-680efe8ede59", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4d1332087a3725e251065f178d2da5976450a56572de598982e8e823f73baecf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21 \n \naugmented reality or wearable health devices, can open new avenues for patient care and remote \nmonitoring. Enhancing the LLMs' contextual understanding is crucial. Future work should focus \non the model's ability to consider a patient's medical history and present conditions before offering \nrecommendations. In sum, the horizon of LLMs in healthcare is expansive and promising. As we \ncontinue to witness the convergence of technology and medicine, the collaboration of \nmultidisciplinary teams\u2014combining expertise from AI, medicine, ethics, and other domains\u2014will \nbe integral to harnessing the full potential of LLMs in healthcare. \nReferences \n1. Min B, Ross H, Sulem E, et al., 2023, Recent advances in natural language processing via \nlarge pre-trained language models: A survey. ACM Computing Surveys, 56: 1-40. \n2. Wei J, Tay Y, Bommasani R, et al., 2022, Emergent abilities of large language  models. \narXiv preprint arXiv:2206.07682. \n3. Brown T, Mann  B, Ryder  N, et al. , 2020,  Language models are few -shot \nlearners. Advances in Neural Information Processing Systems, 33: 1877-1901. \n4. Thirunavukarasu AJ, Ting  DSJ, Elangovan  K, et al. , 2023, Large language  models in \nmedicine. Nature medicine, 29: 1930-1940. \n5. Cascella M, Montomoli  J, Bellini  V, Bignami E., 2023, Evaluating the feasibility  of \nChatGPT in healthcare: an analysis of multiple clinical and research scenarios . Journal of \nMedical Systems, 47: 33. \n6. Sorin V, Klang E, Sklair-Levy, et al.,  2023,  Large language model (ChatGPT) as a support \ntool for breast tumor board. NPJ Breast Cancer, 9:44. https://doi.org/10.1038/s41523-023-\n00557-8. \n7. Lukac S, Dayan D, Fink V, et al. , 2023, Evaluating ChatGPT as an adjunct for the  \nmultidisciplinary tumor board decision -making in primary breast cancer cases. Arch \nGynecol Obstet, 308:1831-1844. doi: 10.1007/s00404-023-07130-5 \n8. Gebrael G, Sahu KK, Chigarira B, et al., 2023, Enhancing Triage Efficiency and Accuracy \nin Emergency Rooms for Patients with Metastatic Prostate Cancer: A Retrospective  \nAnalysis of Artificial Intelligence -Assisted Triage Using ChatGPT 4.0. Cancers (Basel), \n5:3717. doi: 10.3390/cancers15143717.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f0ae121-c09a-4eb8-9fe3-b0125220c0a2": {"__data__": {"id_": "2f0ae121-c09a-4eb8-9fe3-b0125220c0a2", "embedding": null, "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef10b3f7-7efe-4e64-ab51-25ca3093b75d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d0ea1db3f1bcbf38d4766a562f1ea5a02175c847e1b7710fb6e26da2b5ab9917", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 \n \n9. Arya Rao, John Kim, Meghana Kamineni et al.,  2023, Evaluating GPT as an Adjunct for \nRadiologic Decision Making: GPT -4 Versus GPT-3.5 in a Breast Imaging Pilot . Journal \nof the American College of Radiology, 20. https://doi.org/10.1016/j.jacr.2023.05.003. \n10. Haver HL, Ambinder EB, Bahl M, et al. , 2023, Appropriateness of Breast Cancer  \nPrevention and Screening Recommendations Provided by ChatGPT. Radiology, 307. doi: \n10.1148/radiol.230424. \n11. Sarraju A, Bruemmer D, Van Iterson E, et al., 2023, Appropriateness of Cardiovascular \nDisease Prevention Recommendations Obtained From a Popular Online Chat -Based \nArtificial Intelligence Model. JAMA,329:842-844. doi: 10.1001/jama.2023.1044.  \n12. Schulte B , 2023, Capacity of ChatGPT to Identify Guideline -Based Treatments for \nAdvanced Solid Tumors. Cureus, 15:e37938. doi: 10.7759/cureus.37938.  \n13. Haemmerli J, Sveikata L, Nouri A, et al. , 2023, ChatGPT in glioma adjuvant therapy  \ndecision making: ready to assume the role of a doctor in the tumour board? BMJ Health \nCare Inform., 30. doi: 10.1136/bmjhci-2023-100775.  \n14. Chen S, Kann BH, Foote MB, et al. , 2023, Use of Artificial Intelligence Chatbots for \nCancer Treatment Information. JAMA Oncol. , 9:1459\u20131462. \ndoi:10.1001/jamaoncol.2023.2954 \n15. Yakupu A, Aimaier R, Yuan, B, et al., 2023, The burden of skin and subcutaneous diseases: \nfindings from the global burden of disease study 2019. Front Public Health, 11:1145513. \ndoi: 10.3389/fpubh.2023.1145513.  \n16. Urban K, Chu S, Giesey RL, et al ., 2020, Burden of skin disease and associated  \nsocioeconomic status in Asia: a cross-sectional analysis from the Global Burden of Disease \nStudy 1990-2017. JAAD Int., 2:40\u201350. 10.1016/j.jdin.2020.10.006 \n17. Burlando M, Muracchioli A, Cozzani E, et al., 2021, Biologic Therapy: Case Report and \nNarrative Review. Case Rep. Dermatol., 13, 372\u2013378. \n18. Zhou J, He X, Sun L, et al., 2023, SkinGPT-4: An Interactive Dermatology  Diagnostic \nSystem with Visual Large Language Model. Electrical Engineering and Systems Science, \n1-12. https://arxiv.org/abs/2304.10691 \n19. Dugger, BN., Dickson, DW, 2017, Pathology of Neurodegenerative Disease. Cold Spring \nHarb Perspect Biol., 9:a028035. doi: 10.1101/cshperspect.a028035.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18e3f684-448e-4e44-ab33-df8e8228579d": {"__data__": {"id_": "18e3f684-448e-4e44-ab33-df8e8228579d", "embedding": null, "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5beb6031-2fe1-481b-beff-741786a969c2", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "491b8358875708cbaa635a428f5c18898a99be4faf63e14c1dd838e7869d5179", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "23 \n \n20. Koga S, Martin NB, Dickson DW., 2023, Evaluating the performance of large  language \nmodels: ChatGPT and Google bard in generating differential diagnoses in  \nclinicopathological conferences of neurodegenerative disorders. Brain Pathology , \nhttps://doi.org/10.1111/bpa.13207 \n21. Agbavor F, Liang H., 2022, Predicting dementia from spontaneous speech using  large \nlanguage models. PLOS Digital Health , 1(12), e0000168.  \nhttps://doi.org/10.1371/journal.pdig.0000168 \n22. Luz S, Haider F, de la Fuente S, et al., 2021, Detecting cognitive decline using speech only: \nThe ADReSSo Challenge. ArXiv Prepr ArXiv210409356. \n23. Mao C, Xu J, Rasmussen L, et al., 2023, AD-BERT: Using pre-trained language model to \npredict the progression from mild cognitive impairment to Alzheimer\u2019s disease. Journal of \nBiomedical Informatics, 144, 104442. \n24. Cai H, Huang X, Liu Z, et al., 2023, Exploring Multimodal Approaches for  Alzheimer\u2019s \nDisease Detection Using Patient Speech Transcript and Audio Data.  arXiv preprint \narXiv:2307.02514. \n25. Feng Y, Wang J, Gu X, et al., 2023, Large language models improve Alzheimer\u2019s disease \ndiagnosis using multi-modality data. arXiv preprint arXiv:2305.19280. \n26. Ying Y, Yang  T, Zhou H, 2023, Multimodal fusion for alzheimer\u2019s disease  \nrecognition. Applied Intelligence, 53: 16029-16040. \n27. Mohammad-Rahimi H, Motamedian SR, Rohban MH , et al. , 2022,  Deep learning for \ncaries detection: A systematic review. J Dent , 122:104115. doi: \n10.1016/j.jdent.2022.104115. Epub 2022 Mar 30. PMID: 35367318. \n28. Urban R, et al. , 2023, AI-assisted CBCT data management in modern dental practice: \nbenefits, limitations and innovations. Electronics 12, 1710. \n29. Huang H, Zheng O, Wang D, et al., 2023, ChatGPT for shaping the future of dentistry: The \npotential of multi -modal large language model. International Journal of Oral Science , \n15(1). https://doi.org/10.1038/s41368-023-00239-y \n30. Galatzer-Levy IR, McDuff DN, Karthikesalingam A, Malgaroli M, 2023, The Capability \nof Large Language Models to Measure Psychiatric Functioning.  Computation and \nLanguage, 1-15. https://arxiv.org/abs/2308.01834", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa616148-0587-45b6-82e4-766d343d1652": {"__data__": {"id_": "fa616148-0587-45b6-82e4-766d343d1652", "embedding": null, "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99d5f1a2-401a-42d4-8eb1-b51009ed22eb", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4458e6447c5ddbbe7fff18fa1b6f6cb717897f1ff5d85bcfb1c32ffd97bc2134", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 \n \n31. Xu X, Yao B, Dong Y, et al., 2023, Leveraging large language models for mental  health \nprediction via online text data. arXiv preprint arXiv:2307.14385. \n32. Ma Z, Mei Y, Su Z. , 2024, Understanding the Benefits and Challenges of Using Large  \nLanguage Model -based Conversational Agents for Mental Well -being Support . AMIA  \nAnnu Symp Proc. 11;2023:1105-1114. \n33. Kjell, O., Kjell, K., &amp; Schwartz, H. A. , 2023, AI-based large language models are \nready to transform psychological health assessment. \n34. Wu S, Koo  M, Blum , et al. , 2023, A comparative study of open -source large  language \nmodels, GPT -4 and Claude 2: Multiple -choice test taking in nephrology.  arXiv.org. \nhttps://arxiv.org/abs/2308.04709 \n35. Lahat A, Shachar E, Avidan B, et al., 2023,  Evaluating the utility of a large language  \nmodel in answering common patients\u2019 gastrointestinal health -related questions: Are we  \nthere yet? Diagnostics, 13:1950. https://doi.org/10.3390/diagnostics13111950 \n36. Goktas P, Karakaya G, Kalyoncu, et al., 2023, Artificial intelligence chatbots in  allergy \nand immunology practice: Where have we been and where are we going? The  Journal of \nAllergy and Clinical Immunology: In Practice , 11 :2697-2700. \nhttps://doi.org/10.1016/j.jaip.2023.05.042 \n37. Singhal K, Azizi S, Tu T, et al., 2023, Large language models encode clinical knowledge. \nhttps://www.nature.com/articles/s41586-023-06291-2. \n38. Wang S, Zhao  Z., Ouyang, X., et al. , 2023, ChatCAD: Interactive Computer -Aided \nDiagnosis on Medical Image using Large Language Models. Computer Science , 1 -11. \nhttps://arxiv.org/abs/2302.07257 \n39. Bazi Y, Rahhal  MM, Bashmal  L, Zuair M, 2023, Vision\u2013language model for  visual \nquestion answering in medical imagery. Bioengineering, 10(3), 380.  \nhttps://doi.org/10.3390/bioengineering10030380 \n40. Tan RS, Lin Q, Low GH, et al., 2023, Inferring cancer disease response from  radiology \nreports using large language models with data augmentation and prompting. Journal of the \nAmerican Medical Informatics Association, 1-8. https://doi.org/10.1093/jamia/ocad133. \n41. Chen, Z., Balan, M. M., &amp; Brown, K. , 2023, Language models are few-shot learners \nfor prognostic prediction. arXiv.org. https://arxiv.org/abs/2302.12692", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc826763-bec0-4f45-986a-b93cb6c60a3f": {"__data__": {"id_": "fc826763-bec0-4f45-986a-b93cb6c60a3f", "embedding": null, "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "355fbc9b-0b18-4433-a65f-8a86adec65b5", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "785184cf540be1efb11445e4bca35ff27293635ed1f395dbe7024a01be1db79d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LARGE LANGUAGE MODELS IN HEALTHCARE AND MEDICAL\nDOMAIN : A R EVIEW\nZabir Al Nazi\nUniversity of California, Riverside\nRiverside, CA\nznazi002@ucr.edu\nWei Peng\nStanford University\nPalo Alto, CA\nwepeng@stanford.edu\nABSTRACT\nThe deployment of large language models (LLMs) within the healthcare sector has sparked both\nenthusiasm and apprehension. These models exhibit the remarkable capability to provide profi-\ncient responses to free-text queries, demonstrating a nuanced understanding of professional medical\nknowledge. This comprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development, starting from traditional\nPretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we\nexplore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare appli-\ncations, particularly focusing on clinical language understanding tasks. These tasks encompass a wide\nspectrum, ranging from named entity recognition and relation extraction to natural language inference,\nmulti-modal medical applications, document classification, and question-answering. Additionally, we\nconduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain,\nwhile also assessing the utilization of various open-source LLMs and highlighting their significance\nin healthcare applications. Furthermore, we present the essential performance metrics employed\nto evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large language models\nin the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings.\nThis review provides a comprehensive exploration of the current landscape of LLMs in healthcare,\naddressing their role in transforming medical applications and the areas that warrant further research\nand development.\nKeywords Large Language Model \u00b7 Healthcare \u00b7 Medicine \u00b7 Natural Language Generation \u00b7 Natural Language\nProcessing \u00b7 Machine Learning Applications \u00b7 ChatGPT \u00b7 Generative AI \u00b7 Medical AI\n1 Introduction\nDeep Learning provides an intelligent way to understand human behaviors, emotions and human healthcare [1, 2, 3, 4].\nRecent developments in clinical language understanding have ushered in the potential for a paradigm shift in the\nhealthcare sector. These advancements hold the promise of ushering in a new era characterized by the deployment\nof intelligent systems designed to bolster decision-making, expedite diagnostic processes, and elevate the quality of\npatient care. In essence, these systems have the capacity to serve as indispensable aids to healthcare professionals as\nthey grapple with the ever-expanding body of medical knowledge, decipher intricate patient records, and formulate\nhighly tailored treatment plans. This transformative potential has ignited considerable enthusiasm within the healthcare\ncommunity [5, 6, 7].\nThe immense value of lage language models (LLMs) lies in their ability to process and synthesize colossal volumes of\nmedical literature, patient records, and the ever-expanding body of clinical research. Healthcare data [8, 9] is inherently\ncomplex, heterogeneous, and often overwhelming in scale. LLMs act as a powerful force multiplier, aiding healthcare\nprofessionals struggling with information overload. By automating the analysis of medical texts, extracting crucial\ninsights, and applying that knowledge, LLMs are poised to drive groundbreaking research and enhance patient care,\nsignificantly improving and contributing to the progression of the healthcare and medical domain.\narXiv:2401.06775v2  [cs.CL]  8 Jul 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b80bca60-f6aa-4f27-a855-fe18b88ba2fc": {"__data__": {"id_": "b80bca60-f6aa-4f27-a855-fe18b88ba2fc", "embedding": null, "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21199942-58e0-4859-bc7a-0f6ee3f37bd4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "224d659c81211c845989094cddd60b9bc18a1f7821cfd7aded32a177fa2958df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3afbdeb-3bb7-4df7-8503-e8daa3e6c830", "node_type": "1", "metadata": {}, "hash": "20b712836b90a0a07eda6f071b3ebbe2942421a80a753330c90d29e641e10c0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nNotably, this surge of enthusiasm is attributable, in part, to the exceptional performance of state-of-the-art large\nlanguage models (LLMs) such as OpenAI\u2019s GPT-3.5, GPT-4 [10, 11], and Google\u2019s Bard. These models have exhibited\nremarkable proficiency in a wide spectrum of natural language understanding tasks, highlighting their pivotal role\nin healthcare. Their ability to comprehend and generate human-like text is poised to play a transformative role in\nhealthcare practices, where effective communication and information processing are of paramount importance [12].\nThe trajectory of natural language processing (NLP) has been characterized by a series of noteworthy milestones,\nwith each development building upon the strengths and limitations of its predecessors. In its nascent stages, recurrent\nneural networks (RNNs) laid the foundation for contextual information retention in NLP tasks. However, their inherent\nlimitations in capturing long-range dependencies became evident, thus necessitating a shift in the NLP paradigm.\nThe pivotal moment in NLP\u2019s evolution came with the introduction of Transformers, a groundbreaking architecture\nthat addressed the challenge of capturing distant word relationships effectively. This innovation was a turning point,\nenabling more advanced NLP models. These advancements provided the impetus for the emergence of sophisticated\nlanguage models like Llama 2 [13] and GPT-4, which, underpinned by extensive training data, have elevated NLP to a\nlevel of understanding and text generation that closely approximates human-like language.\nWithin the healthcare domain, tailored adaptations of models like BERT, including BioBERT and ClinicalBERT [14, 15],\nwere introduced to tackle the intricacies of clinical language. The introduction of these models addressed the unique\nchallenges posed by medical text, which frequently features complex medical terminology, lexical ambiguity, and\nvariable usage. However, introducing LLMs into the highly sensitive and regulated domain of healthcare demands\ncareful consideration of ethics, privacy, and security. Patient data must be rigorously protected, while ensuring that\nLLMs don\u2019t perpetuate existing biases or lead to unintended harm. Nevertheless, the potential for LLMs to enhance\nhealthcare practices, better patient outcomes, and spearhead innovative research avenues continues to stimulate ongoing\ninvestigation and growth in this rapidly evolving field.\nAs we navigate this dynamic field, our review aims to function as a comprehensive guide, offering insights to medical\nresearchers and healthcare professionals seeking to optimize their research endeavors and clinical practices. We seek\nto provide a valuable resource for the judicious selection of LLMs tailored to specific clinical requirements. Our\nexamination encompasses a detailed exploration of LLMs within the healthcare domain, elucidating their underlying\ntechnology, diverse healthcare applications, and facilitating discussions on critical topics such as fairness, bias mitigation,\nprivacy, transparency, and ethical considerations. By highlighting these critical aspects, this review aims to illustrate the\nimportance of integrating LLMs into healthcare in a manner that is not only effective but also ethical, fair, and equitable,\nultimately fostering benefits for both patients and healthcare providers.\nThis review paper is organized into distinct sections that systematically address the integration, impact, and limitations\nof large language models (LLMs) in healthcare:\n\u2022 Section 2 provides a foundational understanding of LLMs, covering their key architectures such as Transform-\ners, foundational models, and multi-modal capabilities.\n\u2022 In section 3, the focus shifts to the application of LLMs in healthcare, discussing their use cases and the\nmetrics for assessing their performance within clinical settings.\n\u2022 Section 4 critically examines the challenges associated with LLMs in healthcare, including issues related to\nexplainability, security, bias, and ethical considerations.\n\u2022 The paper concludes by summarizing the findings, highlighting the transformative potential of LLMs while\nacknowledging the need for careful implementation to navigate their limitations and ethical implications.\n2 Review of Large Language Models\nLarge language models have emerged as a notable advancement in the field of natural language processing (NLP)\nand have attracted considerable interest in recent times [ 16, 10]. These models exhibit notable attributes such as\ntheir considerable number of parameters, pre-training on vast collections of textual data, and fine-tuning for specific\ndownstream objectives [17, 18, 13]. By leveraging these key characteristics, large language models demonstrate\nexceptional performance across a wide range of NLP tasks. This section presents a comprehensive discussion of the\nconcept, architecture, and pioneering examples of large language models. Furthermore, we explore the pre-training\nmethodology and the significance of transfer learning in facilitating these models to achieve exceptional performance\nacross diverse tasks [19].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c3afbdeb-3bb7-4df7-8503-e8daa3e6c830": {"__data__": {"id_": "c3afbdeb-3bb7-4df7-8503-e8daa3e6c830", "embedding": null, "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21199942-58e0-4859-bc7a-0f6ee3f37bd4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "224d659c81211c845989094cddd60b9bc18a1f7821cfd7aded32a177fa2958df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b80bca60-f6aa-4f27-a855-fe18b88ba2fc", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0e97444cdb841fedbd88cef3b6a787e2916e7a3e457fa771abc0ab45c2ea5d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These models exhibit notable attributes such as\ntheir considerable number of parameters, pre-training on vast collections of textual data, and fine-tuning for specific\ndownstream objectives [17, 18, 13]. By leveraging these key characteristics, large language models demonstrate\nexceptional performance across a wide range of NLP tasks. This section presents a comprehensive discussion of the\nconcept, architecture, and pioneering examples of large language models. Furthermore, we explore the pre-training\nmethodology and the significance of transfer learning in facilitating these models to achieve exceptional performance\nacross diverse tasks [19].\nLarge Language models, built upon the Transformer architecture, have been specifically engineered to enhance the\nefficiency of natural language data processing in comparison to earlier iterations. The Transformer architecture, as\nproposed by [20], utilizes a self-attention mechanism to capture the contextual relationships between words in a sentence.\n2", "mimetype": "text/plain", "start_char_idx": 4542, "end_char_idx": 5548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a73ba4d8-c6cc-4448-b825-e593aa3aa137": {"__data__": {"id_": "a73ba4d8-c6cc-4448-b825-e593aa3aa137", "embedding": null, "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "893f781a-3ebc-4402-a8a1-ecdea1abce5c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f6f2d2c760526e9fa5c9da5bc2eec306adc9c85bdef37998709a3977b0f192a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0b6c73f-1fc9-4f57-9c7a-e6532bf2403c", "node_type": "1", "metadata": {}, "hash": "b705047d5afd43e217665c9a5365deb60a38299a88e6224268d969bb6a7295d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nThis mechanism facilitates the model\u2019s ability to assign varying degrees of significance to distinct words during the\nprediction process, rendering it especially suitable for handling long-range dependencies in language.\nThe key aspects of large language models encompass their substantial magnitude [ 21, 22], pre-training on vast text\ncorpora [23, 13], and subsequent fine-tuning tailored towards specific tasks [24]. These models possess a substantial\nnumber of parameters, ranging from hundreds of millions to billions, which allows them to effectively capture intricate\npatterns and nuances within language. Pre-training is commonly conducted on diverse datasets devoid of task-specific\nannotations, enabling the model to acquire knowledge from a broad spectrum of linguistic instances and develop a\ncomprehensive grasp of language. Following pre-training, the model undergoes a further fine-tuning process using\nsmaller datasets that are appropriate to the task at hand. This allows the model to successfully adapt to and perform\nwell on specific natural language processing (NLP) tasks.\nThe progression of natural language processing (NLP) has been characterized by a series of significant advancements.\nAt the outset, recurrent neural networks (RNNs) facilitated the retention of context in natural language processing\n(NLP) tasks. Nevertheless, recurrent neural networks (RNNs) were found to have several shortcomings when it comes\nto effectively capturing long-range dependencies. The advent of Transformers has had a transformative impact by\neffectively addressing the challenge of capturing distant word relationships. Subsequently, large language models like\nLlama 2 [13], GPT-4 [11] emerged, powered by extensive training data, significantly advancing NLP capabilities in\nunderstanding and generating human-like text. This progression signifies a continuous cycle of innovation, with each\nstage building upon the strengths and limitations of its predecessor. In the subsequent section, we delineate significant\nphases of development within the continuum of progress in the landscape of natural language processing (NLP).\nIn the domain of healthcare, specialized adaptations of BERT, namely BioBERT [14] and ClinicalBERT [15], were\nintroduced to address a variety of challenges in comprehending clinical language. GPT-3 (Generative Pre-trained\nTransformer 3), developed by OpenAI, is one of the largest language models to date, with 175 billion parameters [10].\nRecently, OpenAI introduced the GPT-3.5 and its successor, GPT-4 (OpenAI, 2023) [11], alongside Google AI\u2019s Bard,\nboth of which have emerged as cutting-edge Large Language Models (LLMs), displaying remarkable capabilities across\ndiverse applications, including healthcare and medicine [6].\n2.1 Transformers\nThe Transformers architecture, introduced in \"Attention is All You Need,\" [20] has revolutionized natural language\nprocessing. The primary novelty of this model is its utilization of the self-attention mechanism, which allows for the\nassessment of the importance of input tokens by considering their relevance to the given task. In this setup, multiple\nattention heads work in parallel, allowing the model to focus on various aspects of the input whereas positional encoding\nconveys relative token positions. Given an input sequenceX of length N, the self-attention mechanism [25, 26, 27]\ncomputes attention scores A(i, j) between all token pairs (i, j). Three learned matrices, Query (Q), Key (K), and\nValue (V ), are obtained by linear projections of X.\nAttention(Q, K, V) =softmax( QKT\n\u221adk\n)V\nHere, dk represents the dimension of key vectors. The softmax function normalizes scores. The output for each token is\nthen computed as a weighted sum of value vectors for all tokens j. Multi-Head Attention extends this mechanism by\ncomputing multiple attention sets in parallel, concatenated and linearly transformed to form the final output.\nTransformers consist of stacked encoder-decoder blocks, adapting to diverse tasks. The training occurs via unsupervised\nor semi-supervised learning on vast text corpora, using gradient-based optimization. Transformers have become\nfoundational in natural language processing due to their capacity to handle sequential data, capture long-range\ndependencies, and adapt to various tasks with minimal fine-tuning. They extend beyond text, finding applications in\nhealthcare, recommendation systems, image generation, and other domains.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0b6c73f-1fc9-4f57-9c7a-e6532bf2403c": {"__data__": {"id_": "d0b6c73f-1fc9-4f57-9c7a-e6532bf2403c", "embedding": null, "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "893f781a-3ebc-4402-a8a1-ecdea1abce5c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f6f2d2c760526e9fa5c9da5bc2eec306adc9c85bdef37998709a3977b0f192a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a73ba4d8-c6cc-4448-b825-e593aa3aa137", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4b940c2421ce678939cd66fbdd5c7849bbe8fdc1a43198d6caf9b38377db8fac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Transformers consist of stacked encoder-decoder blocks, adapting to diverse tasks. The training occurs via unsupervised\nor semi-supervised learning on vast text corpora, using gradient-based optimization. Transformers have become\nfoundational in natural language processing due to their capacity to handle sequential data, capture long-range\ndependencies, and adapt to various tasks with minimal fine-tuning. They extend beyond text, finding applications in\nhealthcare, recommendation systems, image generation, and other domains.\n2.2 Large Foundational Models\nThe advent of Large Foundational Models, exemplified by GPT-3 (Brown et al., 2020) [ 10] and Stable Diffusion\n(Rombach et al., 2022) [28], ushers in a transformative era in the field of machine learning and generative artificial\nintelligence. Researchers have introduced the term \"foundation model\" to delineate machine learning models that\nundergo training on extensive, diverse, and unlabeled datasets, endowing them with the ability to adeptly tackle a broad\nspectrum of general tasks. These encompass tasks related to language comprehension, text and image generation, and\nnatural language dialogue.\n3", "mimetype": "text/plain", "start_char_idx": 3987, "end_char_idx": 5153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46b6babe-b7c4-4f00-984a-f7749b5e87c3": {"__data__": {"id_": "46b6babe-b7c4-4f00-984a-f7749b5e87c3", "embedding": null, "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0354d90d-1077-4f8c-91e7-ba5f57e7145b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cafa242508d2bc9c0408b86cd6c1f7c4d8068e2e31a524ff78ab88e7f9cc3bdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 1: Scale of Medical Language Models: A Size Comparison\nLarge foundational models are massive AI architectures trained on extensive quantities of unlabeled data, predominantly\nemploying self-supervised learning methods. This approach to training yields models of exceptional versatility, enabling\nthem to excel across a wide spectrum of tasks, ranging from image classification and natural language processing to\nquestion-answering, consistently delivering outstanding levels of accuracy.\nThese models particularly shine in tasks demanding generative capabilities and human interaction, including the creation\nof marketing content or intricate artwork based on minimal prompts. Nevertheless, adapting and integrating these\nmodels into enterprise applications may present specific challenges [29].\n2.3 Multi-modal Language Models\nA Multi-Modal Large Language Model (MLLM) represents a groundbreaking advancement in the fields of artificial\nintelligence (AI) and natural language processing (NLP). In contrast to conventional language models focused solely on\ntextual data, MLLMs possess the unique ability to process and generate content across multiple modalities, including\ntext, images, audio, and video. This novel approach significantly expands the capabilities of AI applications, allowing\nmachines not only to comprehend and generate text but also to interpret and integrate information from various sensory\ninputs. The integration of multiple modalities enables MLLMs to bridge the gap between human communication and\nmachine understanding, making them versatile tools with the potential to transform diverse fields. This theoretical\nintroduction highlights the transformative potential of MLLMs and their central role in pushing the boundaries of\nartificial intelligence, affecting areas such as image and speech recognition, content generation, and interactive AI\napplications [30].\nMulti-modal large language models (MLLMs) are designed to process and integrate information from multiple data\nsources, such as text, images, and audio, to perform a variety of tasks. These models leverage deep learning techniques\nto understand and generate content across different modalities, enhancing their applicability in real-world scenarios. For\ninstance, Visual ChatGPT combines text and visual inputs to address complex queries [31], while systems like BLIP-2\nutilize a Qformer to integrate visual features with textual data for enhanced image-text interactions [32]. MLLMs are\nparticularly effective in tasks like visual question answering (VQA), where they can interpret and respond to queries\nbased on visual content. The integration of modalities allows these models to offer more comprehensive responses and\nhandle a broader range of interactions than single-modality models. The iterative training processes, often involving\nstages of freezing certain components while fine-tuning others, enable these models to maintain robust language\ncapabilities while adapting to new modalities and tasks.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60123adf-e9a4-4de4-8985-c2b1920ad22c": {"__data__": {"id_": "60123adf-e9a4-4de4-8985-c2b1920ad22c", "embedding": null, "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf11f421-1744-4c25-b6fa-71afda8feaf6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "41ca496f9ecd7bd33f3f4670592c76f86bbfc47fdf0f7344ef36a2e2eba24c39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 2: Schematic Representation of a Standard Multimodal Large Language Model (MLLM) Architecture\nFigure 2 displays a typical MLLM architecture, comprising an encoder EM , a connector C, and a Large Language\nModel (LLM). Additionally, a generator G can be integrated with the LLM to produce outputs beyond text, such\nas other modalities. The encoder processes inputs like images, audio, or videos into features, which the connector\nrefines to enhance the LLM\u2019s comprehension capabilities. Connectors in these systems come in three main varieties:\nprojection-based, query-based, and fusion-based. The first two types utilize token-level fusion, converting features into\ntokens that are combined with text tokens, whereas the fusion-based connector performs a feature-level fusion directly\nwithin the LLM [30].\nRecently, the integration of the Mixture of Experts (MoE) architecture into multi-modal large language models (MLLMs)\nhas significantly advanced their capabilities. This approach employs multiple specialized sub-models, each fine-tuned\nfor specific types of data or tasks such as image recognition or language processing. By selectively activating the\nmost relevant experts based on the input and task, MoE allows MLLMs to dynamically adapt to the demands of\nmultimodal data integration. This enhances the precision of the model in handling complex multimodal interactions and\noptimizes computational resources. Models like MoV A [33] and MoE-LLaV A [34] leverage MoE strategies effectively,\nimproving performance while maintaining manageable computational costs during both training and inference phases.\nThe adaptability and efficiency of MoE within MLLMs thus contribute significantly to their scalability and efficacy in\nreal-world applications across varied tasks and data types [35].\n3 Large Language Models in Healthcare and Medical Domain\nLanguage models have become a revolutionary force in the constantly changing world of healthcare and medicine,\nrevolutionising how medical researchers and practitioners engage with data, patients, and huge corpus of medical\nknowledge [36]. The use of language models in the medical field has undergone a significant metamorphosis, from\nthe early days of simple rule-based systems, feature extraction, and keyword matching to the arrival of cutting-edge\ntechnologies like Transformers, and Large Language Models (LLMs) such as GPT-v4 [11]. These language models\nhave overcome the constraints of conventional methods, enabling more complex natural language generation and\ninterpretation.\nSeveral pioneering large language models have significantly influenced the landscape of NLP. The emergence of the\nTransformer architecture [20] marked a significant milestone in the realm of natural language processing, leading to the\nemergence of expansive pre-trained language models like the BERT [37] and RoBERTa [38].\nBERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. (2018) [37], revolution-\nized NLP by pre-training a deep bidirectional model on a large corpus and outperforming previous models on various\ntasks. RoBERTa (A Robustly Optimized BERT Pretraining Approach) by Liu et al. (2019) [ 38] demonstrated that\nfurther pre-training improvements and optimization could significantly enhance the performance of BERT.\nIn this section, we will first talk about the current large language models specifically for medical applications, in\nsection 3.1. Then, in section 3.2 we will talk about the use cases of various LLMs that designed mainly for patients,\nexperts, and medical materials.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "91893c9f-a6fd-45d5-afd6-fcbdd72a4577": {"__data__": {"id_": "91893c9f-a6fd-45d5-afd6-fcbdd72a4577", "embedding": null, "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9334f872-2c3f-403c-8b28-a0f1dbe08614", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cc8bd0e15d713fd4fd09e4ccf93d28d2f9f7409707500079a397d99ec614073a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 1: Summary of Large Language Models in the Healthcare Space\nMethod Year Task Institution Source\nCode\nBioMistral [39] 2024 Medical Question Answer-\ning\nAvignon Universit\u00e9,\nNantes Universit\u00e9\nmodel\nMed-PaLM 2\n[40]\n2023 Medical Question Answer-\ning\nGoogle Research, Deep-\nMind\nRadiology-\nLlama2 [41]\n2023 Radiology University of Georgia\nDeID-GPT [42] 2023 De-identification University of Georgia code\nMed-HALT\n[43]\n2023 Hallucination test Saama AI Research code\nChatCAD [44] 2023 Computer-aided diagnosis ShanghaiTech University code\nBioGPT [45] 2023 Classification, relation ex-\ntraction, question answer-\ning, etc.\nMicrosoft Research code\nGatorTron [46] 2022 Semantic textual similarity,\nnatural language inference,\nand medical question an-\nswering\nUniversity of Florida code\nBioMedLM 2022 Biomedical question an-\nswering\nStanford CRFM, Mo-\nsaicML\ncode\nBioBART [47] 2022 Dialogue, summarization,\nentity linking, and NER\nTsinghua University, In-\nternational Digital Econ-\nomy Academy\ncode\nClinicalT5 [48] 2022 Classification, NER University of Oregon,\nBaidu Research\nmodel\nKeBioLM [49] 2021 Biomedical pre-training,\nNER, and relation extrac-\ntion\nTsinghua University, Al-\nibaba Group\ncode\nCRNN [50] 2017 Relation classification Indian Institute of Tech-\nnology\ncode\nLSTM RNN\n[51]\n2017 Named entity recognition Wuhan University code\n3.1 Large Language Models for Medical and Healthcare Applications\nFigure 1 provides a comprehensive overview of the progression in biomedical language model (LM) development\nfrom 2019 to 2023, emphasizing a logarithmic growth in model complexity and parameter count. It describes the\nevolutionary trajectories of various domain-specific adaptations of prominent models such as BioBERT, and GPT-2,\nalong with the inception of more advanced systems like MedPaLM. The sizes of the illustrated models are proportional\nto their parameter volumes, showcasing a consistent trend towards larger, more capable models. This is culminated\nin the emergence of Large Language Models (LLMs) by 2023, which signifies a pivotal shift towards architectures\nwith substantially heightened computational requirements and potential performance in biomedical text analysis and\ngeneration tasks.\nOn the other hand, table 1 provides an insightful overview of leading large language models within the healthcare domain.\nRecently, \"BioMistral\" was published as a a collection of open-source pre-trained large language models for medical\ndomains. In 2023, \"Med-PaLM 2\" and \"Radiology-Llama2\" emerged as key players, addressing medical question\nanswering and radiology tasks, respectively. The \"DeID-GPT\" model extends its capabilities to de-identification, while\n\"Med-HALT\" specializes in hallucination testing. Simultaneously, \"ChatCAD\" offers valuable support in the realm of\ncomputer-aided diagnosis. \"BioGPT\" showcases versatility by handling classification, relation extraction, and question\nanswering. \"GatorTron\" excels in semantic textual similarity and medical question answering, whereas \"BioMedLM\"\nnarrows its focus to biomedical question answering. \"BioBART\" demonstrates prowess in dialogue, summarization,\nentity linking, and NER. \"ClinicalT5\" tackles classification and NER, while \"KeBioLM\" specializes in biomedical\npre-training, NER, and relation extraction. Before the advent of language models or transformers, convolutional and\nrecurrent neural networks represented the state of the art in the field. These models collectively represent remarkable\nstrides in healthcare NLP, providing accessible source code or models for further exploration and practical application.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f07c6912-b33a-4398-b46f-a93bb7fb431f": {"__data__": {"id_": "f07c6912-b33a-4398-b46f-a93bb7fb431f", "embedding": null, "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63571516-7b79-4793-ab36-68eca6b1153c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bcb68eb3b2bb30202ab272f03d6a2321b19f35cbab9612539a1ff98bb8d8f9c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 3: Applications of Large Language Models in Healthcare\n3.2 Use Cases of Large Language Models in Healthcare\nIn recent years, the emergence of large language models has catalyzed a transformative shift in the healthcare landscape,\noffering unprecedented opportunities for innovation and advancement. The capability of comprehending and generating\ntext that resembles that of humans has demonstrated remarkable potential across a wide range of healthcare applications\n[52]. The applications of large language models in the healthcare sector are experiencing rapid growth. These models\nare being utilized for clinical decision support, medical record analysis, patient engagement, health information\ndissemination, etc. Their implementation holds the prospect to improve diagnostic accuracy, streamline administrative\nprocedures, and ultimately enhance the efficiency, personalization, and comprehensiveness of healthcare delivery. This\nsection delves into a comprehensive exploration of the multifaceted applications of large language models in healthcare,\nshedding light on their profound implications these applications bear on the trajectory of medical practices and the\neventual outcomes experienced by patients.\n\u2022 Medical Diagnosis:Certain clinical procedures may depend on the use of data analysis, clinical research, and\nrecommendations [53, 54]. LLMs may potentially contribute to medical diagnosis by conducting analyses\non patient symptoms, medical records, and pertinent data, potentially aiding in the identification of potential\nillnesses or conditions with a certain degree of accuracy. Large language models have the potential to contribute\nto several aspects such as clinical decision assistance, clinical trial recruiting, clinical data administration,\nresearch support, patient education, and other related areas [55, 56]. Corroborating this perspective, authors\nintroduce a methodology that utilizes transformer models, namely BERT, RoBERTa, and DistilBERT, for the\npurpose of predicting COVID-19 diagnosis based on textual descriptions of acute alterations in chemosensation\n[57]. Similarly, a number of alternative investigations have been undertaken within the literature, proposing\nstrategies using large language models for the diagnosis of Alzheimer\u2019s disease [58] and dementia [59]. Fur-\nthermore, a corpus of literature has emerged, advocating the integration of large language model chatbots to\ncater to analogous objective [60, 61, 62, 63].\n\u2022 Patient Care:Large Language Models have emerged as transformative tools with the capacity to significantly\nenhance the realm of patient care [64]. Through the provision of personalised recommendations [ 65], cus-\ntomised treatment strategies, and continual monitoring of patients\u2019 advancements throughout their medical\njourneys [66], LLMs offer the promise of revolutionizing healthcare delivery. By harnessing the capabilities\nof LLMs, healthcare providers can ensure a more personalized and patient-centric approach to care. This\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b38c436-d3d9-4ef9-a0e6-55e1dc212ea2": {"__data__": {"id_": "4b38c436-d3d9-4ef9-a0e6-55e1dc212ea2", "embedding": null, "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "822263a0-8b86-4e38-81dc-307451364eef", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c43ad7dc58753252077fe835f802b0dc45e7ca81a911bf1c3c11e253ee33c9bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7159d3ff-2477-4cb3-9fe0-7ef020effd1a", "node_type": "1", "metadata": {}, "hash": "0f553faabde95c5effbf0946d47b97ee6f9cff13938c798eb02f1ceeb0f0d3d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\ntechnology enables the delivery of precise and well-informed medical guidance [67], aligning interventions\nwith patients\u2019 distinct requirements and circumstances.\nThe effective use of LLMs within clinical practise not only enhances patient outcomes but also enables\nhealthcare professionals to make data-driven decisions, leading to enhanced patient care. As LLMs continue\nto advance, the potential for augmenting patient care through personalized recommendations and ongoing\nmonitoring remains a promising trajectory in modern medicine [68]. In essence, LLMs represent a pivotal\nleap forward, holding the capacity to reshape the landscape of patient care by fostering precision, adaptability,\nand patient-centeredness [69].\n\u2022 Clinical Decision Support: Language models (LMs) have evolved into crucial decision support tools for\nhealthcare professionals. By analyzing extensive medical data, LMs can provide evidence-based recommenda-\ntions, enhancing diagnostic accuracy, treatment selection, and overall patient care. This fusion of artificial\nintelligence with healthcare expertise holds immense promise for improved medical decision-making. A body\nof existing research has illuminated promising prospects for the application of language models within clinical\ndecision support, particularly within the domains of radiology [70], oncology [71] and dermatology [72].\n\u2022 Medical Literature Analysis: Large language models (LLMs) exhibit remarkable efficiency in comprehen-\nsively reviewing and succinctly summarizing extensive volumes of medical literature. This capability aids\nboth researchers and clinicians in maintaining topicality with cutting-edge developments and evidence-based\nmethodologies, ultimately fostering informed and optimized healthcare practices. In a fast-evolving field like\nhealthcare, the ability to maintain currency with the latest advancements is paramount, and LLMs can play a\npivotal role in ensuring that healthcare remains at the forefront of innovation and evidence-based care delivery\n[73, 74].\n\u2022 Drug Discovery: Large Language Models, have a significant impact in facilitating drug discovery through\ntheir capacity to scrutinize intricate molecular structures, discern promising compounds with therapeutic\npotential, and forecast the efficacy and safety profiles of these candidates [75, 76]. Chemical language models\nhave exhibited notable achievements in the domain of de novo drug design [77]. In this corresponding study,\nauthors explore the utilization of pre-trained biochemical language models to initialize targeted molecule\ngeneration models, comparing one-stage and two-stage warm start strategies, as well as evaluating compound\ngeneration using beam search and sampling, ultimately demonstrating that warm-started models outperform\nbaseline models and the one-stage strategy exhibits superior generalization in terms of docking evaluation and\nbenchmark metrics, while beam search proves more effective than sampling for assessing compound quality\n[78].\n\u2022 Virtual Medical Assistants and Health Chatbots: LLMs may also serve as the underlying intelligence for\nhealth chatbots, revolutionizing the healthcare landscape by delivering continuous and personalized health-\nrelated support. These chatbots can offer medical advice, monitor health conditions, and even extend their\nservices to encompass mental health support, a particularly pertinent aspect of healthcare given the growing\nawareness of mental well-being [63, 60].\n\u2022 Radiology and Imaging:Multi-modal visual-language models, through their integration of visual and textual\ndata, hold significant promise for augmenting medical imaging analysis. Radiologists can benefit from these\nmodels as they facilitate the early identification of abnormalities in medical images and contribute to the\ngeneration of more precise and comprehensive diagnostic interpretations, ultimately advancing the accuracy\nand efficiency of diagnostic processes in the field of medical imaging [79, 70, 80, 81, 82, 83, 84].\n\u2022 Automated Medical Report Synthesis from Imaging Data:Automated medical report generation from\nimages is crucial for streamlining the time-consuming and error-prone task faced by pathologists and radiol-\nogists. This emerging field at the intersection of healthcare and artificial intelligence (AI) aims to alleviate\nthe burden on experienced medical practitioners and enhance the accuracy of less-experienced ones. The\nintegration of AI with medical imaging facilitates the automatic drafting of reports, encompassing abnormal\nfindings, relevant normal observations, and patient history. Early efforts employed data-driven neural networks,\ncombining convolutional and recurrent models for single-sentence reports, but limitations arose in capturing\nthe complexity of real medical scenarios [5]. Recent advances leverage large language models (LLMs) such as\nChatCAD [70], enabling more sophisticated applications. ChatCAD enhances medical-image Computer-Aided\nDiagnosis networks, yielding significant improvements in report generation. ChatCAD+ further addresses\nwriting style mismatches, ensuring universality and reliability across diverse medical domains, incorporating a\ntemplate retrieval system for consistency with human expertise [44].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7159d3ff-2477-4cb3-9fe0-7ef020effd1a": {"__data__": {"id_": "7159d3ff-2477-4cb3-9fe0-7ef020effd1a", "embedding": null, "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "822263a0-8b86-4e38-81dc-307451364eef", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c43ad7dc58753252077fe835f802b0dc45e7ca81a911bf1c3c11e253ee33c9bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b38c436-d3d9-4ef9-a0e6-55e1dc212ea2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "484d5b7f4e92a9b22ccf0f4fe828e68ccd69c86a82c17654d0a80d0200a385db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Early efforts employed data-driven neural networks,\ncombining convolutional and recurrent models for single-sentence reports, but limitations arose in capturing\nthe complexity of real medical scenarios [5]. Recent advances leverage large language models (LLMs) such as\nChatCAD [70], enabling more sophisticated applications. ChatCAD enhances medical-image Computer-Aided\nDiagnosis networks, yielding significant improvements in report generation. ChatCAD+ further addresses\nwriting style mismatches, ensuring universality and reliability across diverse medical domains, incorporating a\ntemplate retrieval system for consistency with human expertise [44]. In [85], authors use pre-trained language\nmodel (PLM) and in-context learning (ICL) to generate clinical note from doctor patient conversation. These\nintegrated systems signify a pivotal advancement in automating medical report generation through the strategic\nutilization of LLMs.\n8", "mimetype": "text/plain", "start_char_idx": 4657, "end_char_idx": 5595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8a4ce77-ff5a-447a-9434-5cfa39f360b8": {"__data__": {"id_": "e8a4ce77-ff5a-447a-9434-5cfa39f360b8", "embedding": null, "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ac156d7-1f5b-468f-a8be-d0b9ae53f19d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "76c78cf32de25bd77d1bdacfe9eaa3356888ac274636cc53e5e1180a3637b98b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 2: Summary of Recent XIAI Methods for LLMs in Healthcare\nMethod Year Task XIAI Attributes XIAI Evaluation\nMetric\nMentaLLaMA\n[code] [94]\n2024 Mental health analysis Prompt-based (ChatGPT w/ task-\nspecific instructions)\nBART-score, Human\nEval\nArgMed-\nAgents [93]\n2024 Clinical decision reasoning Prompt-based (Self-argumentation iter-\nations + symbolic solver)\nPred. accuracy with\nLLM evaluator\nDiagnostic rea-\nsoning prompts\n[95]\n2024 Medical Question Answering\n(MedQA)\nPrompt-based (Bayesian, differential di-\nagnosis, analytical, and intuitive reason-\ning)\nExpert Evaluation,\nInter-rater agreement\nSkinGEN [96] 2024 Dermatological diagnosis Visual explanations (Stable Diffusion),\ninteractive framework\nPerceived explainabil-\nity ratings\nDR. KNOWS\n[91]\n2023 Automated diagnosis generationKnowledge Graph (explainable diagnos-\ntic pathway)\n-\nHuman-AI Col-\nlaboration [97]\n2023 Clinical decision making Salient features, counterfactual explana-\ntions\nAgreement Level,\nUsability Question-\nnaires\nChatGPT [92] 2023 Mental health analysis Prompt-based (emotional cues and\nexpert-written few-shot examples)\nBART-score, Human\nEval\nCHiLL [98] 2023 Clinical predictive tasks, Chest\nX-ray report classification\nInterpretable features, linear models Expert Evaluation,\nClinical Judgement\nAlignment\nTrap-VQA [99] 2022 Pathology Visual Question An-\nswering (PathVQA)\nGrad-CAM, SHapley Additive exPlana-\ntions\nQualitative Evalua-\ntion\nVision Trans-\nformer [100]\n2021 Covid-19 diagnosis Saliency maps Visualisation\nClinicalBERT\n[code] [15]\n2019 Predicting hospital readmissionAttention weights Visualisation\n3.3 Explainable AI Methods for Interpreting Healthcare LLMs\nLarge Language Models (LLMs) have significantly advanced the healthcare domain, enhancing tasks such as med-\nical diagnosis and patient monitoring. However, the complexity of these models necessitates interpretability for\nreliable decision-making [86]. This section discusses \"eXplainable and Interpretable Artificial Intelligence\" (XIAI)\nand examines recent XIAI methods by their functionality and scope. Despite challenges, such as the difficulty in\nquantifying interpretability and the lack of standardized evaluation metrics, opportunities exist in integrating XIAI\nto add interpretability for LLMs in healthcare. Notable XIAI methods include SHAP [ 87], which quantifies feature\ncontributions, LIME [88, 89], which generates interpretable models through input perturbations, t-SNE for visualizing\nhigh-dimensional data [90], attention mechanisms that highlight key features [15], and knowledge graphs that structure\ncontextual relationships [91], all of which provide crucial insights into model decision-making processes.\nExisting research delves into explainability for LLMs in the healthcare domain. For instance, Yang et al. (2023) [92]\ninvestigate different prompting strategies using emotional cues and expert-written examples for mental health analysis\nwith LLMs. This study shows that models like ChatGPT can generate near-human-level explanations, enhancing\ninterpretability and performance. Additionally, ArgMedAgents (Hong et al., 2024) [93] is a multi-agent framework\ndesigned for explainable clinical decision reasoning through interaction, utilizing the Argumentation Scheme for\nClinical Discussion and a symbolic solver to provide clear decision explanations. Furthermore, Gao et al. (2023)\npropose enhancing LLM explainability for automated diagnosis by integrating a medical knowledge graph (KG) from\nthe Unified Medical Language System (UMLS), using the DR.KNOWS model to interpret complex medical concepts.\nTheir experiments with real-world hospital data demonstrate a transparent diagnostic pathway. Similarly, TraP-VQA\n[91], a novel vision-language transformer for Pathology Visual Question Answering (PathVQA), employs Grad-CAM\nand SHAP methods to offer visual and textual explanations, ensuring transparency and fostering user trust.\nWe have compiled a list in table 2, detailing XIAI attributes, summarizing recent research works focused on explainability\nmethods for LLMs in the healthcare domain. This table includes evaluations of various models, highlighting their\nunique contributions to enhancing interpretability and reliability in medical applications. Each entry outlines the task,\nmethod, XAI attributes, and evaluation metrics, offering a clear overview of the advancements and effectiveness of\nXIAI techniques in improving decision-making processes in healthcare.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bc52f80-3f46-4a63-a512-8b1708456e63": {"__data__": {"id_": "0bc52f80-3f46-4a63-a512-8b1708456e63", "embedding": null, "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dae1560-c3b2-4c30-8656-d79732c5747e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8d5d01256c22faae2495eeefad480a16262d1fce9db8c801c82db73a5c841827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71591de7-42e7-4c8f-b0dd-2088f2b8be1f", "node_type": "1", "metadata": {}, "hash": "47ca8b4def1a1257a965b17b0db949a4f816f210d421729c69e9cffa70d3c014", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n3.4 Future Trajectories of Large Language Models in Healthcare\nAs large language models (LLMs) continue to integrate into the healthcare sector, future developments promise to\nrevolutionize patient care and medical research. A particularly promising avenue involves enhancing LLMs\u2019 capabilities\nto interpret and generate not only textual but also biomolecular data [101]. This advancement could significantly improve\napplications in genomics and personalized medicine, enabling these models to predict individual responses to treatments\nbased on genetic profiles, thereby advancing the precision of medical interventions. Furthermore, incorporating adaptive\nlearning capabilities in real-time could transform LLMs into dynamic aids during surgical procedures or emergencies,\nwhere they might analyze data from medical devices on-the-fly [102] to offer critical decision support.\nAnother innovative trajectory for LLMs in healthcare is the development of federated learning systems [103]. Such\nsystems could facilitate the secure, privacy-preserving propagation of medical knowledge across institutions, improving\nmodel robustness and applicability across varied demographic groups without direct data sharing. This approach will\nnot only enhance the privacy and security of patient data but will also enable a collective intelligence that could lead to\nmore generalized healthcare solutions.\nThe potential of large language models (LLMs) in healthcare extends into the realms of explainable medical AI [104]\nand the utilization of multi-modal models incorporating sensor data. By integrating LLMs with wearable technologies\n[105], these advanced models can serve as continuous health monitors in non-clinical settings.\nTo further advance explainable medical AI, LLMs can be instrumental in deciphering the complexities of medical\nconditions and treatment outcomes. By processing and interpreting multi-modal data, including sensor readings, these\nmodels can contribute to a deeper understanding of patient health on a granular level. This may aid in the development\nof precise, targeted therapies, improving patient outcomes and enhancing the transparency of medical decisions.\nLarge Language Models (LLMs) are poised to revolutionize the healthcare domain by enhancing diagnostic accuracy,\npersonalizing treatment plans, and optimizing operational efficiencies. By integrating LLMs into electronic health\nrecord systems, healthcare providers can more accurately diagnose conditions through natural language processing\ntechniques that analyze clinical notes and patient histories. Moreover, LLMs assist in generating personalized treatment\nrecommendations by analyzing vast datasets that include genetic information, clinical outcomes, and patient preferences.\nFurthermore, these models streamline administrative tasks by automating documentation, coding, and billing processes,\nthus reducing operational costs and allowing medical staff to focus more on patient care. As generative AI advances,\nits transformative impact on the healthcare sector is becoming increasingly significant. This technology is poised to\nrevolutionize areas such as clinical trials, personalized medicine, and drug discovery. Additionally, its applications\nextend to enhancing natural language processing and understanding, improving medical imaging, and supporting\nvirtual assistants in patient care. Generative AI also plays a crucial role in illness detection and screening, facilitating\nmore accurate diagnostics. Moreover, it is being integrated into medical conversation tasks, voice generation, video\ngeneration, and image synthesis and manipulation within healthcare settings [ 106]. These innovations are not only\nimproving the efficiency of medical services but are also paving the way for new methods of patient interaction and\ntreatment planning. As these applications continue to mature, LLMs will become integral in transforming healthcare\nservices into more efficient, accurate, and personalized systems.\n3.5 Performance Evaluation and Benchmarks\nThe medicine and healthcare industries largely acknowledge the potential of artificial intelligence (AI) to drive\nsubstantial progress in the delivery of healthcare. However, empirical evaluations have demonstrated that numerous\nartificial intelligence (AI) systems do not successfully achieve their desired translation goals, primarily because of\nintrinsic deficiencies that become evident only after implementation [107, 108]. In order to optimize the utilization of\nlanguage models (LLMs) within healthcare settings, it is imperative to develop evaluation frameworks that possess\nthe capacity to thoroughly evaluate their safety and quality. It is important to note that certain highly effective models,\nsuch as ChatGPT and PaLM 2 [109], are now not publicly available. The absence of accessibility gives rise to notable\nproblems pertaining to transparency, which is a crucial factor in the medical domain and hinders the capacity to\nthoroughly examine the structure and results of the model. Consequently, this impedes endeavors to recognize and\naddress biases and hallucinations. Thorough research is necessary to understand the specific performance characteristics\nand ramifications of utilizing publicly accessible, pre-trained language models in addressing the challenges in the\nhealthcare and medical domains. Language models that have been pre-trained using medical data also encounter\ncomparable difficulties.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71591de7-42e7-4c8f-b0dd-2088f2b8be1f": {"__data__": {"id_": "71591de7-42e7-4c8f-b0dd-2088f2b8be1f", "embedding": null, "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dae1560-c3b2-4c30-8656-d79732c5747e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8d5d01256c22faae2495eeefad480a16262d1fce9db8c801c82db73a5c841827", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bc52f80-3f46-4a63-a512-8b1708456e63", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "881ecd6696f01c36fe0880d69eb2f6f9b77bdc1ca8da02be6a865c54451b4a34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The absence of accessibility gives rise to notable\nproblems pertaining to transparency, which is a crucial factor in the medical domain and hinders the capacity to\nthoroughly examine the structure and results of the model. Consequently, this impedes endeavors to recognize and\naddress biases and hallucinations. Thorough research is necessary to understand the specific performance characteristics\nand ramifications of utilizing publicly accessible, pre-trained language models in addressing the challenges in the\nhealthcare and medical domains. Language models that have been pre-trained using medical data also encounter\ncomparable difficulties. Therefore, the careful choice and implementation of suitable performance metrics to evaluate\nthe language model assume great significance.\nIn table 4, we present a comprehensive catalog of performance metrics, including but not limited to the F1 score,\nBLEU, GLUE, and ROGUE, which constitute the standard evaluative criteria employed for the rigorous assessment of\nlarge language models operating within the healthcare and medical domain. This compendium of metrics serves as a\n10", "mimetype": "text/plain", "start_char_idx": 4876, "end_char_idx": 6005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bddc14c7-1bae-47c3-90a4-2f624fdfe14e": {"__data__": {"id_": "bddc14c7-1bae-47c3-90a4-2f624fdfe14e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d07c078-8c04-4b93-b27e-37f0f861b119", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b7b58f645948932809bfe9fb5b4070f30c5cd6525df74ef0c7132b7d1c86cd57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 4: Comparative Performance of Healthcare LLMs\nTable 3: LLM Performance Benchmark\nOrganization Model MMLU Score Coding (HumanEval) Release Date\nOpenAI GPT-4 Opus 88.7 - May 2024\nAnthropic Claude 3.5 Sonnet 88.7 92.0 June 2024\nAnthropic Claude 3 Opus 86.8 - March 2024\nOpenAI GPT-4 Turbo 86.4 85.4 April 2024\nOpenAI GPT-4 86.4 90.2 April 2023\nMeta Llama 3 400B 86.1 - -\nGoogle Gemini 1.5 Pro 85.9 84.1 May 2024\nGoogle Gemini Ultra 83.7 - December 2023\nOpenAI GPT-3.5 Turbo - 73.2 -\nMeta Llama 3 (70B) - 81.7 -\nMeta Llama 3 (8B) - 62.2 -\nGoogle Gemini 1.5 Flash - 74.3 -\nvaluable reference, encapsulating the quantitative and qualitative measures utilized to gauge the efficacy, proficiency,\nand suitability of these models in diverse healthcare applications [108].\n3.6 Quantitative Performance Comparison of LLMs in Healthcare Domain\nRecent advancements in language models have been benchmarked against diverse datasets to evaluate their capabilities\nacross various domains. One such comprehensive benchmark is the MMLU (Massive Multitask Language Under-\nstanding) [110], designed to assess the understanding and problem-solving abilities of language models. The MMLU\ncomprises 57 tasks spanning topics such as elementary mathematics, US history, computer science, and law, requiring\nmodels to demonstrate a broad knowledge base and problem-solving skills. This benchmark provides a standardized\nmethod to test and compare various language models, including OpenAI GPT-4o, Mistral 7b, Google Gemini, and\nAnthropic Claude 3, among others.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f10f0d8f-3a37-4e7a-976b-828da8e042bb": {"__data__": {"id_": "f10f0d8f-3a37-4e7a-976b-828da8e042bb", "embedding": null, "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df6a132f-9aeb-4697-91ac-978e6214a708", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9f0ce6b65b8fa7a82c951d6f5893d7304c8fb1791eb7f158ce80595131e57342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14e832c6-feaf-46ad-9b7b-fdb555a59ccb", "node_type": "1", "metadata": {}, "hash": "de93ff987a61896e56776e66122dc944b282b7c8d4ae19a339a153f106b24244", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nThe HumanEval benchmark is used to measure the functional correctness of code generated by LLMs from docstrings.\nThis benchmark evaluates models based on their ability to generate code that passes provided unit tests, using the\npass@k metric. If any of the \u2019k\u2019 solutions generated by the model pass all unit tests, the model is considered successful\nin solving the problem [111]. Table 3 provides a concise summary of the performance of various LLMs on the MMLU\nand HumanEval (Coding) datasets [112].\nIn the healthcare domain, a variety of LLMs have been developed and evaluated on specific datasets such as MedQA,\nMedNLI [113], Tox21 [114], and PubMedQA [115]. The GPT-4 (2024) model stands out in the MedQA dataset with\nan impressive accuracy of 93.06%, significantly outperforming other models like Med-PaLM 2 (CoT + SC) (2023),\nwhich achieves 83.7%, and Meerkat-7B (Ensemble) (2024), with 74.3%. In the MedNLI dataset, BioELECTRA-Base\n(2021) achieves the highest accuracy of 86.34%, closely followed by CharacterBERT (base, medical) (2020) at 84.95%.\nThe Tox21 dataset highlights elEmBERT-V1 (2023) with an outstanding AUC of 0.961, making it the most effective\nin predicting chemical properties and toxicity. For the PubMedQA dataset, Meditron-70B (CoT + SC) (2023) and\nBioGPT-Large (1.5B) (2023) exhibit strong performance with accuracies of 81.6% and 81.0%, respectively [116]. These\nfindings underscore the variability in performance across different healthcare tasks, emphasizing the need for careful\nselection of models based on specific application requirements [117]. Figure 4 presents a comparative performance\nanalysis of various healthcare LLMs, highlighting their accuracy and AUC metrics across different datasets including\nMedQA, MedNLI, Tox21, and PubMedQA.\n4 Limitations and Open Challenges\nThe integration of large language models (LLMs) in healthcare presents complex challenges, including the need\nfor explainability in model decision-making, robust security and privacy measures to protect sensitive patient data,\naddressing biases and ensuring fairness in medical AI applications, mitigating the issue of hallucinations where models\ngenerate erroneous information, and establishing clear legal frameworks for the responsible use of LLMs in healthcare,\nall of which demand careful scrutiny and resolution to harness the full potential of these models for improving healthcare\noutcomes while upholding ethical and legal standards.\n4.1 Model Explainability and Transparency\nLarge language models face notable challenges when applied to healthcare. Their recommendations often lack\ntransparency due to their opaque nature, which can hinder acceptance among healthcare professionals who prioritize\nexplainability in medical decision-making. Moreover, the presence of biases in the training data may compromise the\naccuracy of these models, potentially leading to incorrect diagnoses or treatment recommendations. It is therefore\ncrucial for medical professionals to exercise caution and thoroughly review and validate the recommendations provided\nby large language models before integrating them into their clinical decision-making processes [127]. In healthcare,\nthe importance of interpretability and explainability for AI models utilized in medical imaging analysis and clinical\nrisk prediction cannot be overstated. Inadequate transparency and explainability have the potential to undermine\ntrustworthiness and hinder the validation of clinical recommendations. Consequently, effective governance underscores\nthe continuous pursuit of transparency and interpretable frameworks, aiming to augment the decision-making process in\nthe realm of healthcare [108]. Large language models (LLMs) often function as \"blackboxes\", rendering it challenging\nto discern the underlying processes leading to specific conclusions or suggestions. In the healthcare context, where the\nrepercussions of decisions are profound, it becomes imperative for practitioners to grasp the logic behind AI-generated\noutputs. The persistent endeavor to create models that are more interpretable and transparent remains an enduring\nchallenge within the healthcare domain [128, 129, 130].\n4.2 Security and Privacy Considerations\nLarge Language Models (LLMs) are used in medical research, which necessitates careful consideration of data privacy\nand security issues. Researchers are entrusted with the duty of managing extremely private patient data while enforcing\nrigorous compliance with current privacy laws.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "14e832c6-feaf-46ad-9b7b-fdb555a59ccb": {"__data__": {"id_": "14e832c6-feaf-46ad-9b7b-fdb555a59ccb", "embedding": null, "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df6a132f-9aeb-4697-91ac-978e6214a708", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "9f0ce6b65b8fa7a82c951d6f5893d7304c8fb1791eb7f158ce80595131e57342", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f10f0d8f-3a37-4e7a-976b-828da8e042bb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7b88cd1a7bbbe3222f36cf6916064f51d9abc2dee3d86b7b1dbd532626618b0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the healthcare context, where the\nrepercussions of decisions are profound, it becomes imperative for practitioners to grasp the logic behind AI-generated\noutputs. The persistent endeavor to create models that are more interpretable and transparent remains an enduring\nchallenge within the healthcare domain [128, 129, 130].\n4.2 Security and Privacy Considerations\nLarge Language Models (LLMs) are used in medical research, which necessitates careful consideration of data privacy\nand security issues. Researchers are entrusted with the duty of managing extremely private patient data while enforcing\nrigorous compliance with current privacy laws. The use of LLMs in this setting raises concerns about a number of\naspects of data processing, including as data protection, the possibility of re-identification, and the moral application\nof patient data. One notable issue is the inadvertent inclusion of personally identifiable information (PII) within\npre-training datasets, which can compromise patient confidentiality. Additionally, LLMs can make privacy-invading\ninferences by deducing sensitive personal attributes from seemingly innocuous data, potentially violating individual\nprivacy [131]. Implementing strong measures like data anonymization, safe data storage procedures, and steadfast\nadherence to ethical standards are essential to addressing these issues. Together, these steps make up crucial safeguards\nmeant to protect research participants\u2019 trust, maintain the integrity of research processes, and protect patient privacy.\nThe importance of these factors is underscored by the necessity of balancing the significant contributions of LLMs\n12", "mimetype": "text/plain", "start_char_idx": 3917, "end_char_idx": 5576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b82ca246-f89f-48df-add4-8f6c29c68c00": {"__data__": {"id_": "b82ca246-f89f-48df-add4-8f6c29c68c00", "embedding": null, "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c94039f9-42a3-4576-9ecf-3f4f8d5757a0", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25f13644f1850113b71dc24fcb1b88f9f497b0170cac5a44f686001236b451c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nTable 4: Evaluation Metrics for Language Models in Healthcare Domain\nEval. Metric Description References Key Highlights\nPerplexity\nPerplexity, a probabilistic metric, quan-\ntifies the uncertainty in the predictions\nof a language model. Lower values indi-\ncate higher prediction accuracy and co-\nherence.\n[118] -\n[119] The federated learning model\nachieved a best perplexity\nvalue of 3.41 for English.\n[120] The Transformer model\nachieved a test perplexity of\n15.6 on the PSVG dataset,\nsignificantly outperforming the\nLSTM\u2019s perplexity of 20.7.\n[91] The lowest perplexity achieved\nwas 3.86e-13 with manually de-\nsigned prompts.\nBLEU The BLEU score assesses the quality of\nmachine translation by comparing it to\nreference translations.\n[121] The best BLEU-1 score\nachieved was 13.9 by the\nClinicalGPT model.\n[122] T-5 (fine-tuned) model\nachieved the best BLEU-1\nscore of 26.63.\nGLEU GLEU score computes mean scores of\nvarious n-grams to assess text generation\nquality.\n[121] The best GLEU score achieved\nwas 2.2 by the Bloom-7B\nmodel.\n[122] T-5 (fine-tuned) model\nachieved the best GLEU score\nof 11.38.\nROUGE ROUGE score evaluates summarization\nand translation by measuring overlap with\nreference summaries.\n[121] The best ROGUE-L score\nachieved was 21.3 by the Clini-\ncalGPT model.\n[122] T-5 (fine-tuned) model\nachieved the best ROGUE-L\nscore of 24.85.\nDistinct n-grams Measures the diversity of generated re-\nsponses by counting unique n-grams.\n[122] On the Huatuo-26M dataset,\nthe fine-tuned T5 model\nachieved Distinct-1 and\nDistinct-2 scores of 0.51 and\n0.68, respectively.\nF1 Score\nThe F1 score balances precision and re-\ncall, measuring a model\u2019s accuracy in\nidentifying positive instances and mini-\nmizing false results.\n[123] The GatorTron-large model\nachieved the best F1 score of\n0.9627 for medical relation ex-\ntraction.\n[46] The GatorTron-large model\nachieved the best F1 score of\n0.9000 for clinical concept ex-\ntraction and 0.9627 for medical\nrelation extraction.\n[124] The multicenter Transformers-\nbased model achieved an over-\nall F1 score of 84.77% on the\nPsyNIT dataset.\n[76] The BERT-D2 model achieved\nan F1 score of 81.97% on the\nDDI Extraction 2013 corpus.\nBERTScore BERTScore calculates similarity scores\nbetween tokens in candidate and refer-\nence sentences, using contextual embed-\ndings.\n[125] -\n[85] The Longformer-Encoder-\nDecoder (LED large-PubMed)\nmodel achieved the best\nBERTScore F1 of 70.7.\nHuman Evaluation Involves expert human assessors rating\nthe quality of model-generated content,\nproviding qualitative insights into its per-\nformance.\n[126] The median performance\nfor all human SCORE users\nwas 65%, whereas ChatGPT\ncorrectly answered 71%\nof multiple-choice SCORE\nquestions and 68% of Data-B\nquestions.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01036672-0490-47fa-ac22-aafb48f3eabc": {"__data__": {"id_": "01036672-0490-47fa-ac22-aafb48f3eabc", "embedding": null, "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b22a842-9dc5-478f-a4a3-6463d49e48a1", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5ada16f09d82194d6c25d48829ae8c7a3187c93a1f33e02e3eb907a2ac40ba38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nFigure 5: Challenges of Large Language Models in Healthcare\nin medical research with the critical requirement to protect private patient information [ 132]. LLMs\u2019 ability to find\npotentially revealing patterns in large amounts of health data, even when anonymized, poses a serious privacy risk. This\nnecessitates strict regulations and technical protections. Anonymizing data more effectively is crucial, as are algorithms\ndesigned to spot and prevent the re-identification of individuals. Ongoing monitoring of what LLMs produce is vital\nto ensure privacy isn\u2019t accidentally compromised. Implementing these measures helps guarantee responsible use of\nsensitive data, allowing LLMs to be used ethically in healthcare while still respecting patient privacy. To ensure the\nethical use of LLMs in healthcare, strong governance frameworks must extend beyond basic privacy laws. Proactive\npolicies should anticipate challenges, and experts need to verify LLMs meet ethical guidelines. Engaging patients and\nhealthcare providers in the development process promotes transparency and maintains trust in how health data is used\nwithin these systems.\n4.3 Bias and Fairness\nResearching ways to tackle and reduce biases in language models, while also comprehending their ethical ramifications,\nrepresents a pivotal research domain. It is imperative to create techniques for identifying, alleviating, and forestalling\nbiases in large language models. A primary concern associated with Large Language Models (LLMs) pertains to\nthe risk of producing misinformation or biased outputs. These models, drawing from extensive text data, encompass\nboth dependable and unreliable sources, which can inadvertently result in the generation of inaccurate or misleading\ninformation. Furthermore, if the training data incorporates biases, such as gender or racial biases prevalent within\nscientific literature, LLMs can perpetuate and magnify these biases in their generated content.\nTo ensure the reliability and accuracy of information derived from LLMs, researchers must exercise caution and\nimplement rigorous validation and verification processes. LLMs have the potential to amplify pre-existing biases\ninherent in their training data, particularly those linked to demographics, disease prevalence, or treatment outcomes.\nConsequently, the generated outputs may inadvertently reflect and perpetuate these biases, posing considerable\nchallenges in achieving equitable and unbiased healthcare outcomes.\nTo address these challenges, researchers must remain vigilant in recognizing and mitigating biases within both the\ntraining data and the outputs generated by LLMs. This diligence is crucial for promoting fairness and inclusivity within\nthe realm of biomedical research and healthcare applications, ultimately enhancing the ethical and equitable utility\nof LLMs in these domains [ 132]. Prioritizing bias mitigation in LLMs is essential. Researchers should curate and\npreprocess training data diligently to reduce inherent biases and address sources of inequality. Routine audits and\nevaluations are necessary to identify and correct biases in model training and deployment. Collaborative efforts between\ndomain experts, ethicists, and data scientists can establish guidelines and best practices for unbiased LLM development,\nfostering fairness and inclusivity in biomedical research and healthcare.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40b1d0c0-c1ce-41d8-9fb0-ddd8a864b5bf": {"__data__": {"id_": "40b1d0c0-c1ce-41d8-9fb0-ddd8a864b5bf", "embedding": null, "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ad143f3-d087-47f3-ab3a-86c01da0a4f3", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5a9391ebe71b8830d1c692a2f5596f93e5499085ce4340ae80b9e212734c99e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n4.4 Hallucinations and Fabricated Information\nLanguage models exhibit a proclivity for generating erroneous content, commonly referred to as hallucinations.\nThis phenomenon is characterized by the production of text that appears plausible but lacks factual accuracy. This\ninherent trait poses a substantial risk when such generated content is employed for critical purposes, such as furnishing\nmedical guidance or contributing to clinical decision-making processes. The consequences of relying on hallucinatory\ninformation in healthcare contexts can be profoundly detrimental, potentially leading to harmful or even catastrophic\noutcomes [133].\nThe gravity of this issue is exacerbated by the continuous advancement of Large Language Models (LLMs), which\ncontinually enhance their capacity to generate increasingly persuasive and believable hallucinations. Moreover, LLMs\nare often critiqued for their opacity, as they provide no discernible link to the original source of information, thereby\ncreating a formidable barrier to the verification of the content they produce. To mitigate these risks, healthcare\nprofessionals must exercise extreme caution when utilizing LLMs to inform their decision-making processes, rigorously\nvalidating the accuracy and reliability of the generated information.\nCurrent research endeavors are dedicated to addressing hallucination issues within Large Language Models (LLMs) in\nthe healthcare and medical domain. The introduction of Med-HALT, a novel benchmark dataset, serves the purpose of\nevaluating hallucination phenomena in LLMs in medical contexts. Med-HALT encompasses two distinct test categories:\nreasoning-based and memory-based hallucination assessments. These tests have been meticulously designed to gauge\nthe problem-solving and information retrieval capabilities of LLMs when operating within the medical domain [43].\n4.5 Legal and Ethical Reasons\nEthical concerns extend to the generation of potentially harmful content by LLMs, especially when delivering distressing\nmedical diagnoses without providing adequate emotional support. Moreover, the blurring line between LLM-generated\nand human-written text poses a risk of misinformation dissemination, plagiarism, and impersonation.\nTo address these challenges, rigorous auditing and evaluation of LLMs are essential, along with the development of\nregulations for their medical use. Thoughtful selection of training datasets, particularly within the medical domain,\nis crucial to ensure the responsible handling of sensitive data. These measures collectively strive to strike a balance\nbetween harnessing LLMs\u2019 potential and safeguarding patient privacy and ethical standards [131].\nThe European Union\u2019s AI Act and the United States\u2019 Health Insurance Portability and Accountability Act (HIPAA)\nare two significant regulatory frameworks impacting the deployment of AI in healthcare. The AI Act introduces\ncomprehensive regulations, including the Artificial Intelligence Liability Directive (AILD), which addresses liability for\nAI-related damages. This directive ensures that victims are compensated and that preventive measures are cost-effective.\nThe AI Act classifies General Purpose AI (GPAI) models and imposes specific obligations on providers, including\ntechnical documentation, risk assessments, and transparency about training data [134].\nIn the United States, HIPAA sets stringent standards for the protection of patient data, impacting how LLMs handle\nsensitive information. Compliance with HIPAA requires robust data encryption, regular security assessments, and strict\naccess controls to protect patient information. These regulations ensure that LLMs used in healthcare settings adhere to\nhigh standards of privacy and security, mitigating risks associated with data breaches and unauthorized access.\nOther relevant laws and compliance frameworks include the General Data Protection Regulation (GDPR) in the EU,\nwhich emphasizes data protection and privacy, and the Medical Device Regulation (MDR) that ensures the safety\nand efficacy of AI-driven medical devices. These regulations collectively impact the deployment of generative AI in\nhealthcare by ensuring legal accountability, protecting patient data, and promoting ethical standards in AI development\nand application.\nThe implementation of regulatory frameworks such as the EU\u2019s AI Act, HIPAA, GDPR, and MDR significantly\nimpacts the deployment of LLMs and generative AI in healthcare by ensuring transparency, data protection, and patient\nsafety. These regulations necessitate detailed documentation of AI models, advanced data encryption, strict access\ncontrols, and rigorous clinical testing, thereby increasing development costs and timelines. However, they also promote\nreliability, legal accountability, and ethical standards in AI development, fostering trust among users and stakeholders\nand encouraging the responsible and wider adoption of AI technologies in healthcare [135].\n5 Conclusion\nIn conclusion, the integration of large language models (LLMs) in healthcare showcases immense potential for enhancing\nclinical language understanding and medical applications. These models offer versatility and sophistication, from\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "510a3324-53c8-473e-9d97-13932633e897": {"__data__": {"id_": "510a3324-53c8-473e-9d97-13932633e897", "embedding": null, "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f0b3733-9ccd-4f0a-bf3e-3281d670693f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7dbae05a2f662f7885c63eeaf9665c07acf83cb0a044161fedc49a0462a0df7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea647641-cb13-4dd9-81e7-8d686d82610c", "node_type": "1", "metadata": {}, "hash": "df9c18b437500e970109c2ab1c876909bc436f4ef3cef6c5654223a361b989fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\nnamed entity recognition to question-answering, bolstering decision support and information retrieval. Comparative\nanalyses of state-of-the-art LLMs and open-source options emphasize their significance in healthcare, promoting\ninnovation and collaboration. Performance metrics drive continuous improvement but call for rigorous evaluation\nstandards, considering potential biases and ethical concerns. However, challenges persist, including the need for robust\ntraining data, bias mitigation, and data privacy. LLMs in healthcare necessitate further research and interdisciplinary\ncooperation. LLMs promise transformative benefits, but their full potential hinges on addressing these challenges and\nupholding ethical standards. The ongoing journey of LLMs in healthcare demands collective efforts to harness their\npower for improved patient care while ensuring ethical and responsible application.\nReferences\n[1] Henglin Shi, Wei Peng, Haoyu Chen, Xin Liu, and Guoying Zhao. Multiscale 3d-shift graph convolution network\nfor emotion recognition from human actions. IEEE Intelligent Systems, 37(4):103\u2013110, 2022.\n[2] Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, and Guoying Zhao. Modality unifying network for visible-infrared\nperson re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n11185\u201311195, 2023.\n[3] Yante Li, Wei Peng, and Guoying Zhao. Micro-expression action unit detection with dual-view attentive\nsimilarity-preserving knowledge distillation. In 2021 16th IEEE International Conference on Automatic Face\nand Gesture Recognition (FG 2021), pages 01\u201308. IEEE, 2021.\n[4] Xiaopeng Hong, Wei Peng, Mehrtash Harandi, Ziheng Zhou, Matti Pietik\u00e4inen, and Guoying Zhao. Char-\nacterizing subtle facial movements via riemannian manifold. ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), 15(3s):1\u201324, 2019.\n[5] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A survey of large\nlanguage models for healthcare: from data, technology, and applications to accountability and ethics. arXiv\npreprint arXiv:2310.05694, 2023.\n[6] Yuqing Wang, Yun Zhao, and Linda Petzold. Are large language models ready for healthcare? a comparative\nstudy on clinical language understanding. arXiv preprint arXiv:2304.05368, 2023.\n[7] Ping Yu, Hua Xu, Xia Hu, and Chao Deng. Leveraging generative ai and large language models: a comprehensive\nroadmap for healthcare integration. In Healthcare, volume 11, page 2776. MDPI, 2023.\n[8] Wei Peng, Li Feng, Guoying Zhao, and Fang Liu. Learning optimal k-space acquisition and reconstruction using\nphysics-informed neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20794\u201320803, 2022.\n[9] Wei Peng, Ehsan Adeli, Tomas Bosschieter, Sang Hyun Park, Qingyu Zhao, and Kilian M Pohl. Generating\nrealistic brain mris via a conditional diffusion probabilistic model. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages 14\u201324. Springer, 2023.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea647641-cb13-4dd9-81e7-8d686d82610c": {"__data__": {"id_": "ea647641-cb13-4dd9-81e7-8d686d82610c", "embedding": null, "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f0b3733-9ccd-4f0a-bf3e-3281d670693f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7dbae05a2f662f7885c63eeaf9665c07acf83cb0a044161fedc49a0462a0df7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "510a3324-53c8-473e-9d97-13932633e897", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "83a95bfd89223a7f70a7ab3a3242be10857422c138c88267155a353b5b70b623", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488, 2023.\n[13] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[14] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36(4):1234\u20131240, 2020.\n[15] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342, 2019.\n[16] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian\nRiedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n16", "mimetype": "text/plain", "start_char_idx": 3342, "end_char_idx": 4614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5023e770-6bb4-47a8-983d-cf4d360cf915": {"__data__": {"id_": "5023e770-6bb4-47a8-983d-cf4d360cf915", "embedding": null, "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfe1f6ca-d68f-4dd9-8597-816e73b0d48e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25aa34720d599191c7347ee0424c03a4528691bdc11628fd4324117e88c5a253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b355eda9-a339-44a9-b6b8-4c54ff24c3f7", "node_type": "1", "metadata": {}, "hash": "8ce819f4026a6fff56d8ff78de6704f896cc0e75572ab23b25bfcd7c8b35a34d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[17] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[21] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\n[22] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In\nInternational Conference on Machine Learning, pages 5547\u20135569. PMLR, 2022.\n[23] Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, and Yu Sun. Pre-trained language models and their applications.\nEngineering, 2022.\n[24] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n[25] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and Jiashi\nFeng. Refiner: Refining self-attention for vision transformers. arXiv preprint arXiv:2106.03714, 2021.\n[26] Zabir Al Nazi, Fazla Rabbi Mashrur, Md Amirul Islam, and Shumit Saha. Fibro-cosanet: pulmonary fibrosis\nprognosis prediction using a convolutional self attention network.Physics in Medicine & Biology, 66(22):225013,\n2021.\n[27] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside\ntransformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963\u201312971,\n2021.\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10684\u201310695, 2022.\n[29] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models. arXiv\npreprint arXiv:2309.05922, 2023.\n[30] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal\nlarge language models. arXiv preprint arXiv:2306.13549, 2023.\n[31] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b355eda9-a339-44a9-b6b8-4c54ff24c3f7": {"__data__": {"id_": "b355eda9-a339-44a9-b6b8-4c54ff24c3f7", "embedding": null, "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfe1f6ca-d68f-4dd9-8597-816e73b0d48e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "25aa34720d599191c7347ee0424c03a4528691bdc11628fd4324117e88c5a253", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5023e770-6bb4-47a8-983d-cf4d360cf915", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6dd8621ae36febb58d051b196dc7bcbf852bee500a84335c9f38a86877c7c8f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal\nlarge language models. arXiv preprint arXiv:2306.13549, 2023.\n[31] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning, pages\n19730\u201319742. PMLR, 2023.\n[33] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu.\nMova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024.\n[34] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.\nMoe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024.\n[35] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin\nWen. Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts. arXiv preprint arXiv:2405.05949,\n2024.\n[36] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and\nDaniel Shu Wei Ting. Large language models in medicine. Nature Medicine, pages 1\u201311, 2023.\n[37] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[38] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n17", "mimetype": "text/plain", "start_char_idx": 2779, "end_char_idx": 4667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "85d6c4a4-5805-4cf9-a363-3e2c4e7efc9c": {"__data__": {"id_": "85d6c4a4-5805-4cf9-a363-3e2c4e7efc9c", "embedding": null, "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24381f1-6ee9-4c34-b556-420b3b8a1a18", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2652d48c1ac68dc06dcc9e8af6e7ba21d61746e6a779fae3618c69353b2eacf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6882cb5-1f1c-4a76-aaf4-baf6b8b4edba", "node_type": "1", "metadata": {}, "hash": "12a6848378b81cb29485068324590b274b74eb25ff65e0dae62f10181b17c79b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[39] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard\nDufour. Biomistral: A collection of open-source pretrained large language models for medical domains. arXiv\npreprint arXiv:2402.10373, 2024.\n[40] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language\nmodels. arXiv preprint arXiv:2305.09617, 2023.\n[41] Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie\nLuo, Cheng Chen, et al. Radiology-llama2: Best-in-class large language model for radiology. arXiv preprint\narXiv:2309.06419, 2023.\n[42] Zhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Wei Liu, Dinggang\nShen, Quanzheng Li, et al. Deid-gpt: Zero-shot medical text de-identification by gpt-4. arXiv preprint\narXiv:2303.11032, 2023.\n[43] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. Med-halt: Medical domain hallucination\ntest for large language models. arXiv preprint arXiv:2307.15343, 2023.\n[44] Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang,\nand Dinggang Shen. Chatcad+: Towards a universal and reliable interactive cad using llms. arXiv preprint\narXiv:2305.15964, 2023.\n[45] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative\npre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):bbac409,\n2022.\n[46] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin\nCompas, Cheryl Martin, Mona G Flores, Ying Zhang, et al. Gatortron: A large clinical language model to unlock\npatient information from unstructured electronic health records. arXiv preprint arXiv:2203.03540, 2022.\n[47] Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, and Sheng Yu. Biobart: Pretraining and\nevaluation of a biomedical generative language model. arXiv preprint arXiv:2204.03905, 2022.\n[48] Qiuhao Lu, Dejing Dou, and Thien Nguyen. Clinicalt5: A generative language model for clinical text. In\nFindings of the Association for Computational Linguistics: EMNLP 2022, pages 5436\u20135443, 2022.\n[49] Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang, and Fei Huang. Improving biomedical pretrained\nlanguage models with knowledge. arXiv preprint arXiv:2104.10344, 2021.\n[50] Desh Raj, Sunil Sahu, and Ashish Anand. Learning local and global contexts using a convolutional recurrent net-\nwork model for relation classification in biomedical text. In Proceedings of the 21st conference on computational\nnatural language learning (CoNLL 2017), pages 311\u2013321, 2017.\n[51] Chen Lyu, Bo Chen, Yafeng Ren, and Donghong Ji. Long short-term memory rnn for biomedical named entity\nrecognition. BMC bioinformatics, 18:1\u201311, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d6882cb5-1f1c-4a76-aaf4-baf6b8b4edba": {"__data__": {"id_": "d6882cb5-1f1c-4a76-aaf4-baf6b8b4edba", "embedding": null, "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a24381f1-6ee9-4c34-b556-420b3b8a1a18", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2652d48c1ac68dc06dcc9e8af6e7ba21d61746e6a779fae3618c69353b2eacf2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85d6c4a4-5805-4cf9-a363-3e2c4e7efc9c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "23374ba6bb4cd372fe5498d4385e262355432ea7e626e130010bca09f0f1680a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[50] Desh Raj, Sunil Sahu, and Ashish Anand. Learning local and global contexts using a convolutional recurrent net-\nwork model for relation classification in biomedical text. In Proceedings of the 21st conference on computational\nnatural language learning (CoNLL 2017), pages 311\u2013321, 2017.\n[51] Chen Lyu, Bo Chen, Yafeng Ren, and Donghong Ji. Long short-term memory rnn for biomedical named entity\nrecognition. BMC bioinformatics, 18:1\u201311, 2017.\n[52] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L\nMcClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint\narXiv:2207.07051, 2022.\n[53] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature,\npages 1\u20139, 2023.\n[54] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Language models are few-shot learners for prognostic\nprediction. arXiv e-prints, pages arXiv\u20132302, 2023.\n[55] Vivian Weiwen Xue, Pinggui Lei, and William C Cho. The potential impact of chatgpt in clinical and translational\nmedicine. Clinical and Translational Medicine, 13(3), 2023.\n[56] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Boosting transformers and language models for clinical\nprediction in immunotherapy. arXiv preprint arXiv:2302.12692, 2023.\n[57] Hongyang Li, Richard C Gerkin, Alyssa Bakke, Raquel Norel, Guillermo Cecchi, Christophe Laudamiel,\nMasha Y Niv, Kathrin Ohla, John E Hayes, Valentina Parma, et al. Text-based predictions of covid-19 diagnosis\nfrom self-reported chemosensory descriptions. Communications Medicine, 3(1):104, 2023.\n[58] Chengsheng Mao, Jie Xu, Luke Rasmussen, Yikuan Li, Prakash Adekkanattu, Jennifer Pacheco, Borna Bonakdar-\npour, Robert Vassar, Li Shen, Guoqian Jiang, et al. Ad-bert: Using pre-trained language model to predict\nthe progression from mild cognitive impairment to alzheimer\u2019s disease. Journal of Biomedical Informatics,\n144:104442, 2023.\n18", "mimetype": "text/plain", "start_char_idx": 2572, "end_char_idx": 4652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfecbefc-244f-46a0-af2a-32081320275b": {"__data__": {"id_": "dfecbefc-244f-46a0-af2a-32081320275b", "embedding": null, "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e72e558c-b782-402d-94cf-d856646050ed", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e7d913ed31eba9bfaf0987c9e073cf10e58ee9f0d2b77e99b04dea63da60e71c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0048285-3393-4215-9e4d-c7045deb669d", "node_type": "1", "metadata": {}, "hash": "89efb4ae692b68956c502972b2bb94666768d24f8ea0d456244407a53424ed08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[59] Felix Agbavor and Hualou Liang. Predicting dementia from spontaneous speech using large language models.\nPLOS Digital Health, 1(12):e0000168, 2022.\n[60] Desir\u00e9e Bill and Theodor Eriksson. Fine-tuning a llm using reinforcement learning from human feedback for a\ntherapy chatbot application, 2023.\n[61] Michael Balas and Edsel B Ing. Conversational ai models for ophthalmic diagnosis: Comparison of chatgpt and\nthe isabel pro differential diagnosis generator. JFO Open Ophthalmology, 1:100005, 2023.\n[62] Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, and Ziqi Wang. Psy-llm: Scaling up global\nmental health psychological services with ai-based large language models. arXiv preprint arXiv:2307.11991,\n2023.\n[63] Maham Bilal, Yumna Jamil, Dua Rana, and Hussain Haider Shah. Enhancing awareness and self-diagnosis of\nobstructive sleep apnea using ai-powered chatbots: The role of chatgpt in revolutionizing healthcare. Annals of\nBiomedical Engineering, pages 1\u20133, 2023.\n[64] Mohd Javaid, Abid Haleem, and Ravi Pratap Singh. Chatgpt for healthcare services: An emerging stage for an\ninnovative perspective. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 3(1):100105,\n2023.\n[65] Stephen R Ali, Thomas D Dobbs, Hayley A Hutchings, and Iain S Whitaker. Using chatgpt to write patient clinic\nletters. The Lancet Digital Health, 5(4):e179\u2013e181, 2023.\n[66] Josh Nguyen and Christopher A Pepping. The application of chatgpt in healthcare progress notes: A commentary\nfrom a clinical and research perspective. Clinical and Translational Medicine, 13(7), 2023.\n[67] Harriet Louise Walker, Shahi Ghani, Christoph Kuemmerli, Christian Andreas Nebiker, Beat Peter M\u00fcller,\nDimitri Aristotle Raptis, and Sebastian Manuel Staubli. Reliability of medical information provided by chatgpt:\nAssessment against clinical guidelines and patient information quality instrument. Journal of Medical Internet\nResearch, 25:e47479, 2023.\n[68] Linta Iftikhar et al. Docgpt: Impact of chatgpt-3 on health services as a virtual doctor. EC Paediatrics,\n12(1):45\u201355, 2023.\n[69] Hao Yang, Jiaxi Li, Siru Liu, Lei Du, Xiali Liu, Yong Huang, Qingke Shi, and Jialin Liu. Exploring the potential\nof large language models in personalized diabetes treatment strategies. medRxiv, pages 2023\u201306, 2023.\n[70] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided\ndiagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257, 2023.\n[71] Vera Sorin, Yiftach Barash, Eli Konen, and Eyal Klang. Large language models for oncological applications.\nJournal of Cancer Research and Clinical Oncology, pages 1\u20134, 2023.\n[72] Rubeta N Matin, Eleni Linos, and Neil Rajan. Leveraging large language models in dermatology, 2023.\n[73] Malik Sallam. The utility of chatgpt as an example of large language models in healthcare education, research\nand practice: Systematic review on the future perspectives and potential limitations. medRxiv, pages 2023\u201302,\n2023.\n[74] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin F Rousseau, et al. Evaluating large language models on medical evidence summarization.\nnpj Digital Medicine, 6(1):158, 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0048285-3393-4215-9e4d-c7045deb669d": {"__data__": {"id_": "d0048285-3393-4215-9e4d-c7045deb669d", "embedding": null, "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e72e558c-b782-402d-94cf-d856646050ed", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e7d913ed31eba9bfaf0987c9e073cf10e58ee9f0d2b77e99b04dea63da60e71c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfecbefc-244f-46a0-af2a-32081320275b", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "266f0c199bb3e796b586c8a09f354673ce85dbe0a78df41c56ac449ac546a4af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "medRxiv, pages 2023\u201302,\n2023.\n[74] Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin F Rousseau, et al. Evaluating large language models on medical evidence summarization.\nnpj Digital Medicine, 6(1):158, 2023.\n[75] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Ruili Huang, and Weida Tong. Ai-based language\nmodels powering drug discovery and development. Drug Discovery Today, 26(11):2593\u20132607, 2021.\n[76] Tanmoy Tapos Datta, Pintu Chandra Shill, and Zabir Al Nazi. Bert-d2: Drug-drug interaction extraction using\nbert. In 2022 International Conference for Advancement in Technology (ICONAT), pages 1\u20136. IEEE, 2022.\n[77] Francesca Grisoni. Chemical language models for de novo drug design: Challenges and opportunities. Current\nOpinion in Structural Biology, 79:102527, 2023.\n[78] G\u00f6k\u00e7e Uludo\u02d8gan, Elif Ozkirimli, Kutlu O Ulgen, Nilg\u00fcn Karal\u0131, and Arzucan \u00d6zg\u00fcr. Exploiting pretrained\nbiochemical language models for targeted drug design. Bioinformatics, 38(Supplement_2):ii155\u2013ii161, 2022.\n[79] Lei Ma, Jincong Han, Zhaoxin Wang, and Dian Zhang. Cephgpt-4: An interactive multimodal cephalometric\nmeasurement and diagnostic system with visual large language model. arXiv preprint arXiv:2307.07518, 2023.\n[80] Firas Khader, Gustav Mueller-Franzes, Tianci Wang, Tianyu Han, Soroosh Tayebi Arasteh, Christoph Haarburger,\nJohannes Stegmaier, Keno Bressem, Christiane Kuhl, Sven Nebelung, et al. Medical diagnosis with large scale\nmultimodal transformers\u2013leveraging diverse data for more accurate diagnosis. arXiv preprint arXiv:2212.09162,\n2022.\n19", "mimetype": "text/plain", "start_char_idx": 3047, "end_char_idx": 4677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e2bada17-28db-4cd1-bc2c-b956a25eb601": {"__data__": {"id_": "e2bada17-28db-4cd1-bc2c-b956a25eb601", "embedding": null, "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48125f8-7c95-47eb-a9a2-1da8a1f8d1d3", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "159919ed9222f14e04e8dde84e60fab7453445ed0b249bd2195dfec7b2179093", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04f32702-802b-4f2c-91e2-fb40c9220fdb", "node_type": "1", "metadata": {}, "hash": "3b2c0c0682b60e68648cc97de34c073c1c091e0bfbe3de794f76c4287297ad3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[81] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer,\nSalman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using\nmedical vision-language models. arXiv preprint arXiv:2306.07971, 2023.\n[82] Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, and Zuozhu Liu. A chatgpt aided explainable\nframework for zero-shot medical image diagnosis. arXiv preprint arXiv:2307.01981, 2023.\n[83] Masoud Monajatipoor, Mozhdeh Rouhsedaghat, Liunian Harold Li, C-C Jay Kuo, Aichi Chien, and Kai-Wei\nChang. Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 725\u2013734. Springer, 2022.\n[84] Alireza Roshanzamir, Hamid Aghajan, and Mahdieh Soleymani Baghshah. Transformer-based deep neural\nnetwork language models for alzheimer\u2019s disease risk assessment from targeted speech.BMC Medical Informatics\nand Decision Making, 21:1\u201314, 2021.\n[85] John Giorgi, Augustin Toma, Ronald Xie, Sondra Chen, Kevin An, Grace Zheng, and Bo Wang. Wanglab at\nmediqa-chat 2023: Clinical note generation from doctor-patient conversations using large language models. In\nProceedings of the 5th Clinical Natural Language Processing Workshop, pages 323\u2013334, 2023.\n[86] Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, and Giorgos Papanastasiou. From explainable to\ninterpretable deep learning for natural language processing in healthcare: How far from reality? Computational\nand Structural Biotechnology Journal, 2024.\n[87] Hans-Christian Thorsen-Meyer, Davide Placido, Benjamin Skov Kaas-Hansen, Anna P Nielsen, Theis Lange,\nAnnelaura B Nielsen, Palle Toft, Jens Schierbeck, Thomas Str\u00f8m, Piotr J Chmura, et al. Discrete-time survival\nanalysis in the critically ill: a deep learning approach using heterogeneous data. NPJ digital medicine, 5(1):142,\n2022.\n[88] Alwin Yaoxian Zhang, Sean Shao Wei Lam, Marcus Eng Hock Ong, Phua Hwee Tang, and Ling Ling Chan.\nExplainable ai: classification of mri brain scans orders for quality improvement. In Proceedings of the 6th\nIEEE/ACM international conference on big data computing, applications and technologies, pages 95\u2013102, 2019.\n[89] Ozan Ozyegen, Devika Kabe, and Mucahit Cevik. Word-level text highlighting of medical texts for telehealth\nservices. Artificial Intelligence in Medicine, 127:102284, 2022.\n[90] Adam Gabriel Dobrakowski, Agnieszka Mykowiecka, Ma\u0142gorzata Marciniak, Wojciech Jaworski, and Prze-\nmys\u0142aw Biecek. Interpretable segmentation of medical free-text records based on word embeddings. Journal of\nIntelligent Information Systems, 57:447\u2013465, 2021.\n[91] Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M Churpek, and Majid Afshar.\nLeveraging a medical knowledge graph into large language models for diagnosis prediction. arXiv preprint\narXiv:2308.14321, 2023.\n[92] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards\ninterpretable mental health analysis with large language models. arXiv preprint arXiv:2304.03347, 2023.\n[93] Shengxin Hong, Liang Xiao, Xin Zhang, and Jianxia Chen.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04f32702-802b-4f2c-91e2-fb40c9220fdb": {"__data__": {"id_": "04f32702-802b-4f2c-91e2-fb40c9220fdb", "embedding": null, "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48125f8-7c95-47eb-a9a2-1da8a1f8d1d3", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "159919ed9222f14e04e8dde84e60fab7453445ed0b249bd2195dfec7b2179093", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2bada17-28db-4cd1-bc2c-b956a25eb601", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "78d187aff6f8c91a0a89a4604da0b462abaf609ea522c8b4364bb5ee62b30ffe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Leveraging a medical knowledge graph into large language models for diagnosis prediction. arXiv preprint\narXiv:2308.14321, 2023.\n[92] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards\ninterpretable mental health analysis with large language models. arXiv preprint arXiv:2304.03347, 2023.\n[93] Shengxin Hong, Liang Xiao, Xin Zhang, and Jianxia Chen. Argmed-agents: Explainable clinical decision\nreasoning with large language models via argumentation schemes. arXiv preprint arXiv:2403.06294, 2024.\n[94] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Mentallama:\ninterpretable mental health analysis on social media with large language models. In Proceedings of the ACM on\nWeb Conference 2024, pages 4489\u20134500, 2024.\n[95] Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H Chen. Diagnostic reasoning\nprompts reveal the potential for large language model interpretability in medicine.NPJ Digital Medicine, 7(1):20,\n2024.\n[96] Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zuyong Zhang, Zhouyang Wang, Jie Zhang, Shuiguang Deng,\nand Jianwei Yin. Skingen: An explainable dermatology diagnosis-to-generation framework with interactive\nvision-language models. arXiv preprint arXiv:2404.14755, 2024.\n[97] Min Hun Lee and Chong Jun Chew. Understanding the effect of counterfactual explanations on trust and\nreliance on ai for human-ai collaborative clinical decision making. Proceedings of the ACM on Human-Computer\nInteraction, 7(CSCW2):1\u201322, 2023.\n[98] Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, and Byron C Wallace. Chill: zero-\nshot custom interpretable feature extraction from clinical notes with large language models. arXiv preprint\narXiv:2302.12343, 2023.\n[99] Usman Naseem, Matloob Khushi, and Jinman Kim. Vision-language transformer for interpretable pathology\nvisual question answering. IEEE Journal of Biomedical and Health Informatics, 27(4):1681\u20131690, 2022.\n20", "mimetype": "text/plain", "start_char_idx": 2881, "end_char_idx": 4881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1bb99826-27d1-456d-9f94-c2b4e3ef8656": {"__data__": {"id_": "1bb99826-27d1-456d-9f94-c2b4e3ef8656", "embedding": null, "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d36d50c-b2f8-4e34-b77c-b8b2ce8c4c7e", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f1cb8a7162d38b6b09b864658ece9c58c4dafa4528216b45e76ce7475cab08a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00a36b21-58a7-474a-a17a-d10b4f8300c3", "node_type": "1", "metadata": {}, "hash": "75456f669499c901351deab2d04c48e6ce4e742f42b1d08d4cb8be70a99baecb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[100] S Park, G Kim, Y Oh, JB Seo, SM Lee, JH Kim, S Moon, JK Lim, and JC Ye. Vision transformer for covid-19\ncxr diagnosis using chest x-ray feature corpus. arxiv 2021. arXiv preprint arXiv:2103.07055.\n[101] Jie Pan. Large language model for molecular chemistry. Nature Computational Science, 3(1):5\u20135, 2023.\n[102] Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang, Xiangbo Wu, and Benyou Wang. Online\ntraining of large language models: Learn while chatting. arXiv preprint arXiv:2403.04790, 2024.\n[103] Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S Sheng, Huaiyu Dai, and Dejing Dou.\nFederated learning of large language models with parameter-efficient prompt tuning and adaptive optimization.\narXiv preprint arXiv:2310.15080, 2023.\n[104] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and\nMengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and\nTechnology, 15(2):1\u201338, 2024.\n[105] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm: Large language\nmodels for health prediction via wearable sensor data. arXiv preprint arXiv:2401.06866, 2024.\n[106] Saurabh Pahune and Noopur Rewatkar. Large language models and generative ai\u2019s expanding role in healthcare.\n2024.\n[107] Sandeep Reddy, Wendy Rogers, Ville-Petteri Makinen, Enrico Coiera, Pieta Brown, Markus Wenzel, Eva\nWeicken, Saba Ansari, Piyush Mathur, Aaron Casey, et al. Evaluation framework to guide implementation of ai\nsystems into healthcare settings. BMJ health & care informatics, 28(1), 2021.\n[108] Sandeep Reddy. Evaluating large language models for use in healthcare: A framework for translational value\nassessment. Informatics in Medicine Unlocked, page 101304, 2023.\n[109] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403,\n2023.\n[110] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n[111] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[112] Klu AI. Mmlu benchmark (massive multi-task language understanding), 2024. Accessed: 2024-07-08.\n[113] Qiao Jin, Bhuwan Dhingra, William W Cohen, and Xinghua Lu. Probing biomedical embeddings from language\nmodels. arXiv preprint arXiv:1904.02181, 2019.\n[114] Andreas Mayr, G\u00fcnter Klambauer, Thomas Unterthiner, and Sepp Hochreiter. Deeptox: toxicity prediction using\ndeep learning. Frontiers in Environmental Science, 3:80, 2016.\n[115] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "00a36b21-58a7-474a-a17a-d10b4f8300c3": {"__data__": {"id_": "00a36b21-58a7-474a-a17a-d10b4f8300c3", "embedding": null, "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d36d50c-b2f8-4e34-b77c-b8b2ce8c4c7e", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f1cb8a7162d38b6b09b864658ece9c58c4dafa4528216b45e76ce7475cab08a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bb99826-27d1-456d-9f94-c2b4e3ef8656", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4ee02bd5907fb75d900046b956a151f7e8c302322ec91d3d76311827d62ac23b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Probing biomedical embeddings from language\nmodels. arXiv preprint arXiv:1904.02181, 2019.\n[114] Andreas Mayr, G\u00fcnter Klambauer, Thomas Unterthiner, and Sepp Hochreiter. Deeptox: toxicity prediction using\ndeep learning. Frontiers in Environmental Science, 3:80, 2016.\n[115] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A dataset for\nbiomedical research question answering. arXiv preprint arXiv:1909.06146, 2019.\n[116] Papers with Code. Medical papers with code, 2024. Accessed: 2024-07-08.\n[117] Jonghyun Lee, In-Soo Myeong, and Yun Kim. The drug-like molecule pre-training strategy for drug discovery.\nIEEE Access, 11:61680\u201361687, 2023.\n[118] Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang\nZhu, Hongmin Cai, Tianming Liu, et al. Differentiate chatgpt-generated and human-written medical texts. arXiv\npreprint arXiv:2304.11567, 2023.\n[119] Andrea Manoel, Mirian del Carmen Hipolito Garcia, Tal Baumel, Shize Su, Jialei Chen, Robert Sim, Dan Miller,\nDanny Karmon, and Dimitrios Dimitriadis. Federated multilingual models for medical transcript analysis. In\nConference on Health, Inference, and Learning, pages 147\u2013162. PMLR, 2023.\n[120] Yuhui Zhang, Allen Nie, Ashley Zehnder, Rodney L Page, and James Zou. Vettag: improving automated\nveterinary diagnosis coding via large-scale language modeling. NPJ digital medicine, 2(1):35, 2019.\n[121] Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. Clinicalgpt: Large language models\nfinetuned with diverse medical data and comprehensive evaluation. arXiv preprint arXiv:2306.09968, 2023.\n[122] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and\nBenyou Wang. Huatuo-26m, a large-scale chinese medical qa dataset. arXiv preprint arXiv:2305.01526, 2023.\n21", "mimetype": "text/plain", "start_char_idx": 2724, "end_char_idx": 4583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "90d36207-4639-4922-94da-64578289536e": {"__data__": {"id_": "90d36207-4639-4922-94da-64578289536e", "embedding": null, "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f06904e-15ed-478e-abb4-9ab5271686aa", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1cd9615f78ec2e6b081dbee5b313f6360b74db233cb3490389b51fedebe7daff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Large Language Models in Healthcare and Medical Domain: A Review\n[123] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin\nCompas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health\nrecords. NPJ Digital Medicine, 5(1):194, 2022.\n[124] Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundar\u00f2,\nMarina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, et al. Advancing italian biomedical\ninformation extraction with large language models: Methodological insights and multicenter practical application.\narXiv preprint arXiv:2306.05323, 2023.\n[125] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675, 2019.\n[126] Brendin R Beaulieu-Jones, Sahaj Shah, Margaret T Berrigan, Jayson S Marwaha, Shuo-Lun Lai, and Gabriel A\nBrat. Evaluating capabilities of large language models: Performance of gpt4 on surgical knowledge assessments.\nmedRxiv, pages 2023\u201307, 2023.\n[127] Hazrat Ali, Junaid Qadir, Tanvir Alam, Mowafa Househ, and Zubair Shah. Chatgpt and large language models\n(llms) in healthcare: Opportunities and risks. 2023.\n[128] Giovanni Briganti. A clinician\u2019s guide to large language models. Future Medicine AI, (0):FMAI, 2023.\n[129] Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Pietro Lio, and Andrija Petrovic.\nInterpretable medical diagnostics with structured data extraction by large language models. arXiv preprint\narXiv:2306.05052, 2023.\n[130] Yan Jiang, Ruihong Qiu, Yi Zhang, and Peng-Fei Zhang. Balanced and explainable social media analysis for\npublic health with large language models. arXiv preprint arXiv:2309.05951, 2023.\n[131] Jesutofunmi A Omiye, Haiwen Gui, Shawheen J Rezaei, James Zou, and Roxana Daneshjou. Large language\nmodels in medicine: the potentials and pitfalls. arXiv preprint arXiv:2309.00087, 2023.\n[132] Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, bard, and large language models for biomedical research:\nOpportunities and pitfalls. Annals of Biomedical Engineering, pages 1\u20135, 2023.\n[133] Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen,\nWon Kim, Donald C Comeau, et al. Opportunities and challenges for chatgpt and large language models in\nbiomedicine and health. arXiv preprint arXiv:2306.10070, 2023.\n[134] Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, and Luciano Floridi. Generative ai in eu\nlaw: liability, privacy, intellectual property, and cybersecurity.arXiv preprint arXiv:2401.07348, 2024.\n[135] Philipp Hacker, Andreas Engel, and Marco Mauer. Regulating chatgpt and other large generative ai models. In\nProceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 1112\u20131123,\n2023.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "826fbcd5-7820-4d18-a1de-f212746aa02a": {"__data__": {"id_": "826fbcd5-7820-4d18-a1de-f212746aa02a", "embedding": null, "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45524569-78a2-4cde-aeb8-f128e646ac8f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0faaa9c20b04d83620bfba436681d4f48b0ff37dc61cc21cb2a1b95623780f03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Developing Healthcare Language Model Embedding Spaces\nNiall Taylora, Dan Schofieldb, Andrey Kormilitzina, Dan W Joycec, Alejo\nNevado-Holgadoa\naDepartment of Psychiatry, University of Oxford, Oxford, United Kingdom\nbTransformation Directorate, NHS England, Leeds, United Kingdom\ncDepartment of Primary Care and Mental Health, University of Liverpool, Liverpool, United Kingdom\nAbstract\nPre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets\nlike healthcare focused text. We explore specialized pre-training to adapt smaller LLMs\nto different healthcare datasets. Three methods are assessed: traditional masked lan-\nguage modeling, Deep Contrastive Learning for Unsupervised Textual Representations\n(DeCLUTR), and a novel pre-training objective utilizing metadata categories from the\nhealthcare settings. These schemes are evaluated on downstream document classifica-\ntion tasks for each dataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification tasks,\ndelivering strong performance from limited labeled data and with fewer model parameter\nupdates required. While metadata-based pre-training does not further improve classifica-\ntions across the datasets, it yields interesting embedding cluster separability. All domain\nadapted LLMs outperform their publicly available general base LLM, validating the\nimportance of domain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational budgets,\nan essential capability for responsible and sustainable deployment in local healthcare\nsettings. We provide pre-training guidelines for specialized healthcare LLMs, motivate\ncontinued inquiry into contrastive objectives, and demonstrates adaptation techniques to\nalign small LLMs with privacy-sensitive medical tasks.\n1. Introduction\nLarge Language Models1 (LLMs) such as the Bidirectional Encoder Representa-\ntion Transformer (BERT) models based on masked language modelling (MLM), and\nGenerative Pretrained Transformers (GPT) models based on autoregressive language\nmodelling, have changed the research landscape entirely [1, 2, 3], and are generally seen\nas offering state of the art results on a number of popular Natural Language Processing\n(NLP) benchmark datasets and tasks [1, 3, 4, 5]. The type of task different LLMs excel\nin generally depends on the architecture and pre-training objective: BERT-like LLMs,\ntypically perform very well at embedding focused tasks like text classification, clustering\n1LLMs refers to all relevant Pre-trained Language Models (PLMs) across all scales\nPreprint submitted to Elsevier April 1, 2024\narXiv:2403.19802v1  [cs.CL]  28 Mar 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "416c2337-af97-4392-9e24-7201eae83534": {"__data__": {"id_": "416c2337-af97-4392-9e24-7201eae83534", "embedding": null, "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54ac6568-4203-4084-88eb-d6769455a89e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "53a3d6d42b7a7ecbfcc50a123c230be73f0e68fe785e1a9eb547623963b92682", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and retrieval [5]. Generative LLMs excel in generative tasks and form foundation for\nmany chat-based APIs dominating the artificial intelligence (AI) landscape.\nOne common problem across LLMs, both embedding and generative focused LLMs,\nis a drop in performance on text data or NLP tasks from a specific domain (e.g. health-\ncare) to the one that the model was originally trained on [6, 7, 8, 9, 10, 11, 12]. Aligning\nopen LLMs with new domains and tasks remains a considerable issue, in particular\nfor private datasets which remain unseen by open LLMs, e.g. UK National Health\nService (NHS) based clinical datasets. Firstly, the LLM needs to be fine-tuned for each\nspecific domain and task, and secondly, the LLM will suffer catastrophic forgetting,\nwhere it loses performance on other data and tasks in which it was originally trained.\nAn intermediate approach to avoid these problems is to pre-train language models to\nproducing good text embeddings that align well with possible downstream tasks to\nreduce the need for further fine-tuning, and thus improve training efficiency. Whilst\ncertain approaches, such as Retrieval Augmented Generation (RAG), are becoming\npopular in attempting to imbue generally trained LLMs with external knowledge sources\nthrough embeddings, and thus reduce the need for domain adaption, the ability of the\nunderlying LLMs involved is still often reliant on its domain pre-training. Moreover,\nthe scale of the state of the art generative LLMs used today has become prohibitively\nlarge, whereas embedding focused LLMs remain much smaller, cheaper and efficient\nfor many traditional NLP tasks.\nIn this paper, we explore the efficacy of altering the pre-training of smaller, BERT-\nlike LLMs to align with the healthcare2 domain and downstream tasks of interest. The\nutility of small and resource-efficient LLMs for the healthcare domain is especially\nattractive where budget for compute may be limited and training on private data is\nrequired. Linked to this problem is the inability to utilise many cloud-based APIs with\nprivate data due to data governance laws and issues of confidentiality and security.\n1.1. Document-level label-free embeddings\nStandard language modelling objectives (e.g. autoregressive, masked language\nmodelling) work on a word or token level, with the aim of representing words based on\nthe context they appear in. For example, the LLM objective is operating on a token level\nloss objective, and embeddings are produced per token of the input sequence. Often\nthe resultant token level embeddings appear performant for the task at hand, but many\ndownstream tasks do not operate on the token level, but rather the sentence or entire\ndocument level e.g. document sentiment analysis, information retrieval, and similarity\nmatching: of which LLMs are consistently state of the art. From a BERT-like LLM, a\ndocument level representation can be achieved in a number of ways based on the token\nlevel embeddings produced, with a common method being mean pooling i.e. the average\nof all token embedding representations of the last transformer layer [1, 13, 14].\nA potential issue with this relatively crude approximation of a document level\nembedding is that the Language Model (LM) pre-training objective did not encourage\nthese representations to be useful or have a clear relationship to the loss function directly.\n2we used the term healthcare to cover the more general NHS patient safety reports dataset used in this\nwork that is not strictly an EHR\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a3703e7a-190d-4d88-b7d8-a648ddf7af4f": {"__data__": {"id_": "a3703e7a-190d-4d88-b7d8-a648ddf7af4f", "embedding": null, "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfe714d2-e1b6-4c25-a61a-518c33a5575c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "90379fc694380adf421087316f0c17f4c9f84d5f1f87ae1e15646defa75e7ac2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A number of approaches have been explored to better train these LMs to produce\nsentence level embeddings through alignment tasks [ 14, 15, 16]. It is common to\nutilise contrastive learning loss functions during pre-training to cluster embeddings from\nsentences or documents that are of the same type or even from the same global document\ncloser together than those that are not. These approaches now incorporate a loss that\nis directly operating on the sentence or document level embedding and subsequently\nencourages the model to produce token embeddings that influence the resultant averaged\nembedding and embedding space. A potential difficulty for contrastive learning is\nderiving class labels that allow the creation of samples of positive and negative pairs\nthat most contrastive loss functions require. A common approach relies upon a dataset\u2019s\nknown labels, such as Natural Language Inference (NLI) datasets used in the training of\nsentence transformers[14].\nHowever, large labelled healthcare text datasets are rare and for the datasets explored\nin this paper we do not have extensive or completely labelled data. For this reason,\nwe focus on methods that can use readily available structured meta data or use an\nunsupervised training regime, so no extra label annotations are required.\n1.2. The Healthcare Text Domain\nThere are two related, but distinct, types of text that are often merged as a single\ncategory in the machine learning literature. These are \"clinical text\", defined as this one\nwritten by clinical professionals in Electronic Health Records (EHRs); and \"biomedical\ntext\", this one written by researchers in scientific papers or books. The clinical domain,\nas opposed to the biomedical domain, is particularly difficult for general purpose LLMs.\nThis is often thought to be due to the complexity and idiosyncrasies of the language used\nin clinical practice that inevitably reflects regional- or specialty-specific nomenclature.\nFor instance, abbreviations are over-prevalent and often non-intuitive (e.g. using\"#\" for\nbroken bone, \"bds\" for twice a day, and many others [17]), and grammar and syntax is\nused sparingly in favour of faster writing. In contrast, the biomedical research literature\ntends to enforce a more consistent and agreed upon vocabulary, text follows a formal\nstyle, and descriptions and arguments try to be complete and analytic.\nThere are several techniques to mitigate the problems that arise from merging these\ntwo domains, and these solutions typically rely on a form of transfer learning or domain\nadaptation. One typical approach is to continue training the LLM in the target specialist\ndomain using the same language modelling objective, to better prepare the model for\ndeployment in the new domain, and has delivered promising results for US based clinical\ndatasets [6, 7, 18].\nIn this paper, we sought to compare and contrast the creation of embedding models\nin different healthcare datasets and in particular, two UK NHS datasets, and compare\nwith general LLM alternatives.\n1.2.1. The UK NHS\nEven in well-constrained use cases, such as pharmaco-vigilance for medication\nadverse events, clinical language patterns, idioms and idiosyncrasies in Electronic Health\nRecord (EHR) data are notoriously difficult to work with [19]. Similarly, consultation\nwith clinical colleagues in the NHS suggests that the routine clinical language recorded\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "329c01f5-9b2e-4679-8727-a2823e175a4d": {"__data__": {"id_": "329c01f5-9b2e-4679-8727-a2823e175a4d", "embedding": null, "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2474c822-8a9e-4dd8-a474-649c2dfbc97c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6bdda40c56cf0a93145f3c794892373283048123ae764e5eb98fc8f22928422f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in EHR systems in the UK differs substantially from that used in large open-source\nmedical text datasets. For instance, \"A&E\" is sometimes used in the UK to denote\nthe emergency department, while \"ED\" is used in the US and is more common in\nopen-sourced datasets. In addition, the text of open-source medical datasets appears\nto consist of the milder examples that are closer to normal writing, and does not use\nabbreviations and grammatical transgressions as seen in the much more specialist text\nseen in actual EHRs. This can severely limit the deployment of current popular medical\nLLMs[6, 20], which are often trained on these datasets. Whilst numerous biomedical or\nclinically trained LLMs exist, no publicly available LLMs exist for UK-based \u201cNHS\nlanguage\u201d and most use data from the United States.\nIn this paper we investigate the influence that different LM pre-training objectives\nhave on the performance of embeddings in downstream tasks. To make this investigation\nsystematic, we examined three pre-training methods across three healthcare datasets,\nand inspected the results using dataset-specific sequence classification tasks, embedding\ndistance metrics and qualitative analyses. Overall, we aim to ascertain what benefits\ndifferent pre-training methods provide when re-using general-domain LLMs to the\nclinical domain, and in particular to NHS datasets, such that deployment becomes more\namenable than in prior work.\n1.3. Motivation and Related Work\nImproving the embeddings representation of LLMs in the general domain has been\nstudied extensively and influences this work directly: notably the Deep Contrastive\nLearning for Unsupervised Textual Representations (DeCLUTR) paper [15], sentence\ntransformers [14] and SimCSE [ 21] works show great promise in improving BERT-\nstyle LLMs representation of sentences and documents through contrastive methods.\nSimilar works investigating the augmentation and changing of the LLM pre-training\nobjectives with external knowledge-graphs include BioLinkBERT [22] and Dragon [23].\nSAPBert sought to improve named-entity-recognition and linkage through alignment of\nembeddings for entity synonyms [24].\nRecently, there has also been promising work showing how task specific contrastive\nloss functions combined with sentence transformers can adapt LLMs to downstream\ntasks with relatively few training samples [ 9]. Work has also sought to enhance the\nability for generative LLMs to produce both good embeddings and generation using a\ncombination of instruction tuning and embedding loss functions [5], however this still\nrelied on comparatively larger models (e.g. those with more than 7 billion parameters).\nThe key contributions of this work are the introduction of a note category pre-\ntraining objective, the development of several LLMs for different NHS datasets, and the\nexploration of resource constrained pre-training and downstream adaptation.\n2. Methods\n2.1. Datasets and downstream tasks\nAs is typical, to explore the effect of different pre-training methods on the LLMs\nwe sought to evaluate them on a set of dataset specific downstream tasks. The focus\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "77f3c97e-985e-43d4-a5ed-f6e4852dba6a": {"__data__": {"id_": "77f3c97e-985e-43d4-a5ed-f6e4852dba6a", "embedding": null, "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06b045bd-cd6d-430f-8c13-ff1213437634", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8a06cf0ce611492a698b7d42adc762769af83270707be6a2d6fa06bbc5fcf21f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "was primarily on the differences in document-wide embeddings dependent on the pre-\ntraining received. Thus we opted to derive document classification tasks using the\ncategorical variables available for each of the datasets, with a focus on potential clinical\nuse-cases. The specific tasks available for each dataset are outlined below, with an\noverview of the data distributions and their respective downstream task is provided in\nTable 1. For brevity, all the downstream tasks are document classification tasks whereby\na document is to be assigned a particular class label.\n2.1.1. MIMIC-III\nThe first dataset we used is the Medical Information Mart for Intensive Care III\n(MIMIC-III) [25], a medical dataset developed by the MIT Lab for Computational\nPhysiology. It is comprised of de-identified EHR records associated with 38,597\ncritical care patients and 58,976 intensive care unit (ICU) admissions at the Beth\nIsrael Deaconess Medical Center between 2001 and 2012. Data includes demographics,\nvital signs, laboratory tests, medications, caregiver notes, imaging reports, and mortality\nin and out of hospital. While the clinical tasks presented here may benefit from utilising\nthe multi-modal data available for each patient, we focus on the use of free text clinical\nnotes.\nICD-9 Triage (M-Tri)\nWe utilise the ICD-9 codes associated with discharge summaries in the MIMIC-\nIII dataset to derive a triage classification task, originally published in our previous\nwork [26]. The motivation for this task is to represent a realistic use-case within a\nhospital setting, whereby patients admitted to an ICU will be treated and then \u201cstepped\ndown\u201d (discharged) to another ward or team to continue treatment when they no longer\nrequire an ICU. The result is a mapping between particular ICD-9 diagnosis codes\nand a corresponding destination department, of which we derive the following seven\npost-ICU destination teams: Cardiology, Obstetrics, Respiratory Medicine, Neurology,\nGastroenterology, Acute or Internal Medicine, and Oncology. For further details, see\noriginal implementation details in [26].\n2.1.2. Oxford Health Foundation Trust - OHFT\nThe second dataset is from the Oxford Health NHS Foundation Trust (OHFT), a\nregional UK-based provider of specialist mental healthcare covering Oxfordshire and\nBuckinghamshire. The dataset contains full historical EHR data for approximately\n200,000 patients spanning over a decade, and within this access to around 8 million\nde-identified clinical notes.\nTriage Team Association (O-Tri)\nIn the UK, mental health services are structured into primary, secondary, and tertiary\nlevels. Most patients (96%) needing specialist care are referred to and treated by\ncommunity mental health teams (CMHTs) [27]. Referrals contain information written\nby the referring doctor or professional and are triaged by the receiving CMHT into: a)\naccept to the team for assessment, b) reject due to insufficient information, or c) route to\na sub-specialty team if warranted.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c02b933-8112-4689-991e-7864cbf19ca2": {"__data__": {"id_": "7c02b933-8112-4689-991e-7864cbf19ca2", "embedding": null, "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f815988-3baf-4cef-bb1f-7d61cd3ecb20", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a29c6adf17855516b76892080f87093762c064268a7a4e603372fbca994619a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Structured EHR data on referral and discharge dates establishes which team accepted\nthe patient, though local administrative variations required developing supplementary\nheuristics in collaboration with OHFT clinicians. We created a classification task to\nidentify the accepting triage team for a given referral from the subset of accepted\nreferrals. Specifically, given a random clinical note, the task is to determine which\nreferral team it likely belongs to based solely on its free text contents.\n2.1.3. NHS Patient Safety Incident Reports - PSIR\nThe third dataset is a large, national collection of free-text documents relating\nto all types of patient safety incidents in the NHS, from the National Reporting and\nLearning System (NRLS). More information about the dataset and collection can be\nfound on the official website [28]. We worked with a sub-sample of approximately 2.3\nmillion de-identified reports produced in the financial year 2019/2020, with the goal of\nsupporting more efficient analysis of the dataset within NHS England, and integration\nof the learning into the newer Learn from Patient Safety Events (LFPSE) service.\nIN05 Level 1 - Incident Category (P-Cat)\nThe labels of incident category at the first level are a standard field provided along-\nside each incident report which is used to detail the type of the incident. There are 13\nclasses present in the dataset, and we have chosen not to include a second level of labels\nwhich is included in the wider NRLS dataset.\nPD09 - Incident Degree of Harm (P-Sev)\nThe degree of incident severity categories are an ordinal scale ranging from no harm\n(1) to death (5) collected for all incidents. We simplify a pseudo task related to incidence\nseverity prediction in the following way: the 1-5 incident severity labels given with the\nfree text reports are skewed, with the vast majority being attributed a 1 (or no harm). We\ncreate a much smaller and balanced binary classification dataset which bins incidence\nlabels into low (severity categories 1-3) and high (severity categories 4-5) severity.\n2.1.4. Note Category - All Datasets\nThe various origins and purposes of these clinical notes are partially captured by\ntheir note category assignment (or associated metadata). These categories can be be\nseen as a form of structured knowledge pertaining to the organisation of the part of the\nhealthcare system of focus. This is a commonly captured field in healthcare datasets\nand while the exact use and meaning of this may differ between the respective datasets,\nbroadly each dataset contains this field to record the professional role of the person who\nproduced the note or the departmental origin. We expect that note category may be a\nvaluable signal for the pre-training itself because, for example:\n\u2022 a note entered by a social worker will contain terminology and describe concepts\nrelevant to a patient\u2019s social circumstances\n\u2022 a note entered by an occupational therapist will emphasise functioning and the\npatient\u2019s capacity for everyday tasks\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8a1f3948-ce71-4a21-bc4a-415be4dd4d50": {"__data__": {"id_": "8a1f3948-ce71-4a21-bc4a-415be4dd4d50", "embedding": null, "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17ce5e70-c8c4-4601-ae49-464564d311be", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fe08befd506b1acc4b2c0d385c4592707530b8709c745155daf4c36589becd9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 a note entered by a physician will emphasise clinical state, examinations and\ntreatment plans\n\u2022 a note entered by a care coordinator will reflect progress on executing a manage-\nment/care plan for the patient\nTherefore, embeddings arising from the contents of documents with different note\ncategories should capture the vocabulary, concepts and semantics of information rou-\ntinely recorded by different healthcare professionals and their intended use. In each\nof our dataset we identify a note category: for both MIMIC-III and OHFT, the note\ncategory reflects the clinical purpose of the document that relates to the profession\nof the individual making the note e.g. Nursing, Doctor, Social worker. Whereas for\nPSIR, the category variable (RP02 on the database) reflects the care setting in which the\npatient safety incident occurred such as: Acute / General Hospital, various Community\ngroupings, and General Practice.\n2.2. Data splits\nFor each of our datasets there are a substantial number of available clinical docu-\nments, even in the smallest there are over 2 million individual samples. To facilitate the\ndevelopment of multiple models and experiments within a resource restricted setting, we\ndeveloped and trained initial pipelines at various smaller scales. We then chose to train\nand test models on a maximum sub-sample of 250,000 documents as this would be large\nenough to show trends in the data, whilst keeping resource utilisation to a manageable\nlevel. Furthermore, where possible we utilised the unique patient identifiers to ensure\nno individuals data was both in the training and evaluation sets. Future work could\nseek to train and evaluate with a larger amount of data and focus on the magnitude of\ndifferences in results found.\nLength of documents\nAn important feature of a text dataset is the number of words, or tokens, contained\nin each individual sample (document). Transformer based language models such as\nRoBERTa[29] can only handle a maximum of 512 tokens on even the most modern\nGPUs, due to the complexity of the self-attention calculations which increases expo-\nnentially with the number of tokens [ 30]. The average document length, as well as\nthe distribution of lengths, varies considerably in our three datasets, although a large\nportion fit within the maximum sequence length for our chosen models. As is standard,\ndocuments that exceed the maximum token length are truncated. Whilst numerous\napproaches have been developed to mitigate this problem, such as introducing local\nsliding windows [30], key-value caching [31], and flash attention [32], we opt to focus\nonly on the standard transformer attention used by RoBERTa. We thus limit the context\nwindow to 512 tokens, which we see as sufficient for the investigations presented in\nthis work. For more details of the document lengths for each respective dataset, see\nAppendix A.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edd9ee8d-07c2-4600-90db-bd22c7974238": {"__data__": {"id_": "edd9ee8d-07c2-4600-90db-bd22c7974238", "embedding": null, "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2997e529-0822-4056-8dbf-671fcb6ccef2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ad632c393fd09fdbec2ea61073855ddd2189f139c5abd6a8b35417a571736f57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1: Downstream classification dataset statistics. Task names are also given short-hand codes used\nthroughout the rest of the paper to better fit tables and figures.*P-Cat is not the note category used in\npre-training for the PSIR dataset\nDataset Task (Acronym) # labels # train samples # test samples\nMimic-III Note category (M-Cat) 8 1,600 4,000\nMimic-III ICD-9 Triage (M-Tri) 7 1,400 3,150\nOHFT Note category (O-Cat) 10 10,000 2,500\nOHFT Referral team relation (O-Tri) 5 6,250 2,500\nPSIR IN05: Category (P-Cat) 13 26,000 2,600\nPSIR PD09: Severity (P-Sev) 2 14,000 14,000\n2.3. Language modelling - Preliminaries\nThe pre-training of a standard LM involves a corpus of D text documents X =\n{X1, X2, ..., XD} and two functions, fenc and fhead. Each document d consists of\na sequence of T tokens Xd = (x1, x2, x3...xT ) which is first passed through fenc to\nproduce a contextualised vector for each token, Hd = (h1, h2, h3...hT ). fhead then uses\nHd for whichever self-supervised pre-training task chosen and to perform subsequent\ndownstream tasks during fine-tuning. Because T may vary from sample d to sample d\u2032,\nto represent the whole sequence of T tokens as a fixed-length vector, as is required for\nmany tasks, we used a pooling function g to take the mean of all token level embeddings\nof the sample, Hd = (h1, h2, ..., hT ), which has proven a reasonable approach to\nrepresent a sequence of text [15, 14]. Accordingly, whole sequence embeddings ed =\ng(fenc(Hd)).\n2.3.1. Continued Masked Language Modelling\nThe first pre-training method is the standard formula for MLM, a commonly used\nobjective for pre-training language models that randomly replaces ormasks a proportion\nof tokens of the input with a special [MASK ] token. This essentially corrupts the\noriginal input and the objective of the model is to predict which tokens should appear in\nthe masked positions, a form of gap filling.\nThe standard MLM loss function is given as follows:\nLmlm(X, Y) =\u2212\nNcX\nn=1\nWn(\n|V |X\ni=1\nY n\ni ln(flm(X)n\ni )) (1)\nwhere X is the input of the model, Y denotes MLM labels which is a collection of Nc\none-hot vectors each with the size of |V | where |V | is the size of the vocabulary of the\nmodel and N is the number of input tokens3 and Wn is 1 for masked tokens and 0 for\nothers. This ensures that only masked tokens will contribute to the computation of loss.\n3Note that one-hot vectors for non-masked tokens are zero vectors.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03026346-e9a1-41c4-b320-6f173c8decf6": {"__data__": {"id_": "03026346-e9a1-41c4-b320-6f173c8decf6", "embedding": null, "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "926f26ab-4fe6-4e4e-b1c6-66a39add1580", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5770275cd70130128d6baca3758ecb8b17218e0bae532c6722c2aba9afadf9f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "flm represents the encoder model with a language modelling head whose output is a\nprobability distribution vector with the size of the vocabulary (|V |) for each token.\n2.3.2. Contrastive Loss Pre-training\nA common approach to improving separation of classes in an embedding space\nutilises contrastive learning, which uses a loss function that aims to encourage semanti-\ncally close or same-class members whilst pushing apart other-class members [33]. It\nassumes a set of paired examples P = {(Xi, X+\nj )}P\ni,j=1, where P are the number of\nsamples, and documents Xi and Xj are semantically related documents. The derivation\nof paired examples is the important, and arguably most difficult part when well-defined\nlabels are not present. We therefore first opt to utilise methods that are unsupervised or\nself-supervised:\nDeCLUTR\nAs we want to generate a model that can produce good document level embeddings,\nwe also explored an self-supervised cluster alignment technique used to produce the De-\nCLUTR model [15]. DeCLUTR stands for Deep Contrastive Learning for Unsupervised\nTextual Representations, and uses a contrastive loss function to encourage sentence or\ndocument level embeddings that are taken from the same document type or class to be\ncloser together in the learned embedding space. DeCLUTR utilises a self-supervised\ncontrastive loss function called InfoNCE (Noise-Contrastive Estimation) which aims to\nidentify positive pairs in a set of samples also containing numerous negatives.\nSampled anchor\nSampled positive\nsi\nsj\nd\nLLM fenc\nMinimise \ndistance\nPooling - g\nLLM fenc ei\nPooling - g\nsj\nShared weights\nFigure 1: Adapted from [15]. Overview of the DeCLUTR training process. We sample anchor spans si and\npositive spans sj from each document d in a minibatch of size N. For simplicity, we show A = P = 1,\nwhere A and P are the number of anchors and positives per document. The spans are encoded by fenc() and\npooled by g(\u00b7) to get embeddings ei = g(f(si)) and ej = g(f(sj)). The encoder and pooler are trained to\nminimize the distance between positive span pairs while maximizing the distance to negatives (omitted for\nsimplicity).\nDuring training, a batch of P anchor-positive span pairs is taken from document\nd. Each of the spans are separately encoded. The anchor and positive embeddings,\nsi and sj, are then compared for their cosine similarity by taking their dot product.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4971ef85-72d8-4528-8643-0f434c8cbed2": {"__data__": {"id_": "4971ef85-72d8-4528-8643-0f434c8cbed2", "embedding": null, "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e89191dc-b97a-4a9c-8461-c1425bdfadff", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bf50a0bcf6519768147c7d78e33e6b554f6f59646ed0d1350fbd2d5ae8d34820", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The objective of the training procedure is to maximise the cosine similarity of the P\nmatching anchor-positive pairs and minimise that of the remainingP2 \u2212P non-matching\npairs. For a given batch, the cosine similarities are then used to calculate the probability\nthat a given pair is a match, which can be defined as:\nP(si, sj; \u03c4) = exp(si \u00b7 sj/\u03c4)P\nk\u0338=i,j exp(si \u00b7 zk/\u03c4) (2)\nwhere \u03c4 is a trainable temperature parameter. This results in the InfoNCE loss which\nsymmetrically measures the success in maximising the similarity of matches and min-\nimising the similarity of non-matches, defined as:\nLInfoNCE = \u22121\n2\n\"\n1\nP\nPX\ni,j=0\nlog P(si, sj; \u03c4) + 1\nP\nPX\ni,j=0\nlog P(sj, si; \u03c4)\n#\n(3)\nDeCLUTR Data Sampling. For the DeCLUTR models, we initially only manipulated\nthe sampling strategy, through changing the maximum length of documents spans to be\nused for the creation of anchor-positive pairs, in our experiments. We kept the actual\nsampling methodology the same. As per the original paper, the minimum number of\ntokens for a document to be sampled is:2\u00d7A\u00d7Smax where A is the number of anchors\nto be sampled, and Smax is the maximum span length. For example, the minimum span\nlength for a document with 2 anchors and a maximum span length of 512 would be\n2048.\nThe span length is thus a key parameter when training using the DeCLUTR regime\nand we have not been able to exhaustively explore all possible combinations. We\ninstead opted for a span length that aligned with our average document length in each\ndataset. The sampling strategy for generating anchors and positive pairs was also fixed\nto adjacent across all datasets. We recognise that future work could seek to explore this\nspace further. Examples of the sample distributions based on the minimum document\nlength for each dataset are presented in the table in Appendix H.\nNote Category as a pre-training signal\nAs a third pre-training method, we utilised a known categorical variable which\nappears in some form across all three datasets, the \u201cnote category\u201d of the document as\ndescribed in Section 2.1.4.\nWe formulate this task as a replacement of the original next sentence prediction\ntask used in BERTs implementation [1]. Whole sequence embeddings, e, are fed to a\nclassification head fhead(\u00b7), which has the task of calculating the logits yj of each of\nthe possible c classes j \u2208 C. A softmax operation \u03c3 is applied to the logits to produce\na normalized probability score that x belongs to each of c possible classes. For one\nsample with the vector representation e, the probability of the sample belonging to class\nj is:\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38e3f1a1-dbc5-4944-a358-5cabd0de0d5c": {"__data__": {"id_": "38e3f1a1-dbc5-4944-a358-5cabd0de0d5c", "embedding": null, "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17f53c4f-575e-44e1-ae90-aeed834cf636", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "0f84f5841923c413889768526f9b51fe8e1caeed33747bdefbef2ea7c7b3d0a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "fhead(e) = [y1, y2, ..., yc] ,\nP(j) =\u03c3([y1, y2, ..., yc]) = exp(yj)Pc\nk=1 exp(yk)\nwhere fhead(\u00b7) :Rm \u2212 \u2192Rc, and yj \u2208 R for all j. The classification head can have any\nnumber of layers (depth) d \u2208 N, but here we have opted to set d = 2throughout.\nFollowing this, the loss for the note category pre-training can be defined by the\nstandard cross-entropy formula:\nLnote = \u2212\nNX\nc=1\nyc log(pc) (4)\nFigure 2: Overview of our note category pre-training approach. On the left side ( A) shows the flow of the\ninput sequence (x) through the standard MLM pipeline, and on the right side (B) shows the integration of the\nassociated note category label in parallel. The MLM and note category classification objectives are jointly\noptimised with each document.\nWith both contrastive pre-training objectives outlined, we combine them with stan-\ndard MLM to form a joint loss function. To discourage the pre-training objective\nto over-represent the note category classification task, we also applied an optional\nweighting w to the loss, as shown in Equation 5, where Lcontrastive is either LInfoNCE (for\nDeCLUTR) or Lnote (for note contrastive).\nL = LMLM + w(Lcontrastive) (5)\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed7f8025-a763-429e-b3c7-364ce0b6e3c5": {"__data__": {"id_": "ed7f8025-a763-429e-b3c7-364ce0b6e3c5", "embedding": null, "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2216b8e9-52cc-4b9f-b043-030039650ed0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "46c5204d228b6e1a028919efcf19137330e88ddd5f8ca6fd4ab98ae0e2ed01dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.4. Adapting LLMs for downstream classification Tasks\nIn order to use the various further pre-trained LLMs for downstream classification\ntasks, we used the traditional fine-tuning approach. Conventional fine-tuning can be\nachieved by adding task-specific layer(s) or an entire multi-layer perceptron (MLPs) to\nthe LLM. The exact approach to processing the LLM output is dependent on the task.\nIn our use case of sequence or document classification, the downstream task head is\nan MLP fMLP(\u00b7) made of up 2 linear layers which takes the pooled sentence embedding\noutput by the LLM, e, as input and generates an n-dimensional vector, where n is the\nnumber of classes. A softmax operation is applied to the resultant output vector in order\nto generate probabilities of each class:\nP(y | x) =\nexp ((fMLP (h(x))y)\nexp (Pn\ni=1 fMLP(h(x))i).\nSince the additional MLP block and LLMs are modular, their respective parameters are\nstored separately, and we can opt to freeze the parameters of one or the other.\n2.5. Efficiency gains from LLM freezing and few-shot training\nWe sought to explore the potential of using the embeddings produced by the different\nLLMs without any further fine-tuning in relation to a given downstream task, keeping\nthe LLM body frozen, or by freezing different numbers of layers of the LLM model.\nMoreover, we explored the performance of the LLMs on the downstream tasks by\nfine-tuning with different numbers of training samples per class in a few-shot training\nsetup, similar to previous works [18].\n2.6. Document Embeddings Analysis\nBeyond downstream classification performance, we chose a selection of different\nmetrics of the LLMs embedding space to discern any clear differences produced by the\nvaried pre-training objectives used.\n2.6.1. Uniformity and Alignment\nWe follow a similar analysis plan to the authors of the SimCSE paper [21] to probe\nthe quality of the embedding space through measures of alignment between in-class\npairs and uniformity across the entire space. Alignment calculates expected distances\nbetween the paired instances, which in our case was embeddings of documents within\nthe same class (a proxy for positive pairs).\n\u2113align \u225c E\n(x,x+)\u223cppos\n\u2225f(x) \u2212 f(x+)\u22252 (6)\nConversely, uniformity measures how well the embeddings for all documents,\nregardless of class, are uniformly distributed. Together these metrics help illuminate\nhow the embedding spaces represent within class samples, which should remain close\ntogether and random, unrelated samples should be scattered.\n\u2113uniform \u225c log E\nx,y\ni.i.d.\n\u223c pdata\ne\u22122\u2225f(x)\u2212f(y)\u22252\n(7)\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e0734d32-68ea-48cf-9c80-13d000238241": {"__data__": {"id_": "e0734d32-68ea-48cf-9c80-13d000238241", "embedding": null, "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "726d259d-7897-4875-8bbc-80122e5e06d4", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e9afff3d172dc35fee1cadeff4cb1a377e7b2f1e22a6b9aa885ce7daedbee027", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.6.2. Cosine Similarity and Clustering\nGiven we produce a set of embeddings for our texts with known labels, we can\nrun a number of analyses utilising the raw vector embeddings for each note, as well\nas dimension reduced embedding spaces. For our analysis we opted to look at simple\ncosine similarity within, and between known classes for each dataset and a simple graph\nnetwork analysis.\nGraph network analysis provides another lens for understanding the structure of\nembedding spaces. In this approach, documents are represented as nodes in a graph,\nand we define our edges connecting notes that have a cosine similarity above a defined\nthreshold. The connectivity of the graph and formation of connected subgraphs can\nreveal insights about how notes are positioned in embedding space.\n3. Implementation details\n3.1. LLM model setups\nWe performed a large and varied number of experiments across the three datasets,\nwith different training regimes and requirements. An overview of these is presented in\nTable 2, although this is a curated selection intended to provide broad coverage of the\nmodels and training objectives used.\n3.2. Training overview\nA secondary objective of this work was to investigate the efficient pre-training of\nsmall LLMs given resource constrained environments (where large suites of GPUs\nare not available nor desirable). Further, Language modelling from scratch is often\nexpensive and hardware dependent, thus we chose to continue the pre-training phase\nextending already pre-trained general domain LLM. Importantly, we conducted training,\nevaluation, and hyperparameter exploration with the use of a single local GPU, similar\nto other works attempting to complete full pre-training with a single GPU given a fixed\namount of time [34]. For more information of the hardware setups in each case, please\nsee Appendix B.\n4. Results\nAfter extending the pre-training of the different LLMs using the methods outlined\nabove (MLM, DeCLUTR, and Note Contrastive), we have 12 LLMs to compare, outlined\nin Table 2. Given these LLMs now produce different representations of text, we evaluate\nthem across each of the respective datasets and associated downstream tasks. We present\ntwo approaches to evaluating these embeddings: downstream classification performance,\nand embedding space analysis.\n4.1. Downstream Classification Performance\nWe report results for using the LLMs in different settings.First, keeping the base\nLLM frozen during fine-tuning, and only update the weights of the classification head to\nassess the utility of the pre-trained embeddings with no further updates (effectively the\nLLMs here are pure feature extractors). We compare this with freezing varying numbers\nof LLM layers, and also fully fine-tuning the entire LLM on the downstream task.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1955189-8c32-4f54-9088-726fdc835e7b": {"__data__": {"id_": "c1955189-8c32-4f54-9088-726fdc835e7b", "embedding": null, "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25e68918-db07-4fbd-a4d2-9649117a93a5", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "051b0b64a4a09f7524e02ad48b865a74442dfbca419447f28b9ddfa732e8e60f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Clinical Dataset Domain Pre-training Model size Model Name\nNone None 124.6 RoBERTa-base\nMimic-III MLM 124.6 RoBERTa-mimic\nMimic-III MLM + DeCLUTR 124.6 RoBERTa-mimic-DeCLUTR\nMimic-III MLM + Note 125.1 RoBERTa-mimic-note\nOHFT MLM 124.6 RoBERTa-OHFT\nOHFT MLM + DeCLUTR 124.6 RoBERTa-OHFT-DeCLUTR\nOHFT MLM + Note 125.1 RoBERTa-OHFT-note\nPSIR MLM 124.6 RoBERTa-PSIR\nPSIR MLM + DeCLUTR 124.6 RoBERTa-PSIR-DeCLUTR\nPSIR MLM + Note 125.1 RoBERTa-PSIR-note\nTable 2: Overview of the different datasets and LLM pre-training. Domain pre-training refers to whether this\nmodel has been explicitly pre-trained using the related clinical dataset. We will use this table to define the\nmodel names that will be used throughout the results section. Each model uses RoBERTa-base as the base\nmodel.\nPLM Domain pre-training M-Cat M-Tri O-Cat O-Tri P-Cat P-Sev\nFrozen None 0.766 0.314 0.319 0.423 0.384 0.65\nMLM 0.867 0.439 0.355 0.552 0.498 0.739\nMLM + DeCLUTR 0.859 0.711 0.411 0.601 0.555 0.748\nMLM + Note - 0.471 - 0.546 0.450 0.714\nFinetuned None 0.991 0.827 0.593 0.766 0.655 0.837\nMLM 0.991 0.846 0.629 0.779 0.660 0.842\nMLM + DeCLUTR 0.988 0.844 0.613 0.765 0.653 0.844\nMLM + Note - 0.836 - 0.757 0.663 0.847\nTable 3: F1 macro across all datasets and classification tasks based on domain pre-training received. We\nreport the maximum F1 macro achieved over 5 epochs of training per model.\n4.1.1. Evaluation - all tasks\nThe evaluation performance for each of the domain pre-training methods across\nthe different datasets and tasks is presented in Table 3 (best over five epochs), with\nDeCLUTR models performing best when the LLM remains frozen during fine-tuning.\nIn the full fine-tuned setting, MLM models generally perform marginally better. Across\nall tasks and fine-tune settings, the models with no domain pre-training achieve the\nlowest performance. For individual performance metrics for each model across each\ndataset and task, including results for alternative open-source clinical LLMs, please see\nAppendix C.\n4.1.2. Few-shot sampling\nAdditionally we focus on training with sub-samples of the training set, representing\na setting where producing annotations is often a major limiting factor (e.g. due to lack of\nexpert time or prohibitive expense) in Fig. 3. For brevity we only include this analysis\nfor the MIMIC-III ICD-9 Triage task, however similar patterns were seen across all\ndatasets and tasks.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4435aba-8a4b-4390-a9f7-c050bb7aac90": {"__data__": {"id_": "f4435aba-8a4b-4390-a9f7-c050bb7aac90", "embedding": null, "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6bf6155-a92c-4749-bc77-db31e0933ae6", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "67f7af2035fd436c8993ba1cfc3c118d1a3a7600282af6b1c98336d37a0f3be5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 32 64 128 200\nSamples per class\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7F1\nAfter 1 epoch\n16 32 64 128 200\nSamples per class\nMax over 5 epochs\nDomain pretraining\nNone MLM Note contrastive DeCLUTR\nFigure 3: F1 macro score on evaluation set for the MIMIC-III ICD-9 Triage task with frozen LLMs trained\nwith different sample sizes per class.\n4.1.3. Effect of freezing layers\nTo investigate the influence of freezing different layers of the LLM, we present\nresults based on freezing an increasing number of consecutive layers of the LLM in Fig.\n4.\n0 2 4 6 8 10 11 12\nT otal number of LLM layers frozen\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8F1\nFirst epoch\n0 2 4 6 8 10 11 12\nT otal number of LLM layers frozen\nMax over 5 epochs\nDomain pretraining\nNone MLM Note contrastive DeCLUTR\nFigure 4: F1 macro score on evaluation set for the MIMIC-III ICD-9 Triage task with varying transformer\nlayers frozen (all models utilised a 12-layer RoBERTa architecture).\n4.2. Document Embeddings Analysis\nThe main result of training a LLM is a model that has captured aspects of how human\nlanguage is organised, encapsulated by its resultant embedding space. The pre-training\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "290db0ce-65cc-43dd-a71c-8ca00e74bac3": {"__data__": {"id_": "290db0ce-65cc-43dd-a71c-8ca00e74bac3", "embedding": null, "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ceae5917-d4c9-4bbf-96fd-1a624d6b468c", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "3a2fd49a20bb2fcfa704baba88bfcb5060a228db4f6d9f043fe8d540c874ff17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "objective has a direct impact on the embedding space, as well as the domain targeted\ndownstream tasks. For any language modelling task or text-based downstream task, the\nword or sentence derived contextualised embeddings are the numerical representation\nof that knowledge obtained during pre-training.\nWe have seen substantial differences in the usefulness of different LLMs embeddings\nfor downstream tasks, especially when no fine-tuning occurs. To attempt to understand\nthis aspect in more detail, we present a exploration of the embedding spaces of different\nLLMs for the MIMIC-III ICD-9 triage task. For more embedding analysis details, see\nE.9.\n4.2.1. Cosine Similarity\nA common approach to measuring the characteristics of an embedding space, espe-\ncially when classes are known, is to look at the distances between embeddings of each\nclass within the embedding space. As expected, the LLMs which utilised a contrastive\nloss function (RoBERTa-mimic-DeCLUTR and RoBERTa-mimic-note) exhibit a much\ngreater separation of embeddings, with a much wider range of cosine similarity values.\nHowever, the differences between and within class members shows a very similar pattern\namongst all models, see Fig. 5.\nbetween within\n0.96\n0.97\n0.98\n0.99\n1.00Cosine Similarity\nNone\nbetween within\n0.96\n0.98\n1.00\nMLM\nbetween within\n0.6\n0.7\n0.8\n0.9\n1.0\nNote contrastive\nbetween within\n0.0\n0.2\n0.4\n0.6\n0.8\nDeCLUTR\nClass membership\nFigure 5: Cosine similarity of document embeddings within and between classes for the MIMIC-III ICD-9\ntriage dataset. Note the y-axis scales are separate for each subplot, this is due to the large differences in value\nranges between models.\n4.2.2. Alignment and uniformity\nAn example of comparing uniformity versus alignment is presented in Fig. 6, which\nhighlights large differences between the LLMs dependent on their pre-training objective.\nMost notably, the DeCLUTR models appear to have produced an embedding space with\na high diversity and hence low uniformity amongst all classes, but with a high alignment\nscore, implying within class embeddings remain relatively far apart.\n4.2.3. Network analysis\nResults of a simple graph network analysis with varying cosine similarity thresholds\nis provided in Table 4.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dc69619-818d-42fc-a539-8713554dc9bd": {"__data__": {"id_": "5dc69619-818d-42fc-a539-8713554dc9bd", "embedding": null, "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e5a266a-b720-4f03-b559-72dae883ad6b", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "5f7bd639f62854536f25a8a929990787a253b5b5b74807eff31b6ebe66129c3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 6: Uniformity vs. alignment for the different LLM setups for the MIMIC-III ICD-9 task embeddings.\nThe colorbar represents the corresponding F1 score on the ICD-9 triage classification task using these frozen\nLLM embeddings.\nModel name Cos. threshold # Components Avg. degree\nRoBERTa-base 0.995 1 317.896\n0.996 4 54.355\n0.997 8 24.128\n0.997 42 46.628\nRoBERTa-mimic 0.994 2 160.150\n0.995 7 168.512\n0.996 16 82.579\n0.997 45 45.913\nRoBERTa-mimic-DeCLUTR 0.538 1 317.300\n0.617 3 54.242\n0.699 24 26.233\n0.742 50 44.607\nRoBERTa-mimic-note 0.959 3 115.873\n0.967 8 127.278\n0.973 23 56.543\n0.977 34 38.244\nTable 4: Quantitative analysis of graph properties for different models. Cosine similarity thresholds are\nderived from the 90th, 95th, 98th, and 99th percentiles. The number of components reflects the count of\nconnected components given the threshold chose, and the average degree of the top Nt refers to the graph\ndegree across the top Nt connected components (here Nt is chosen to be 3).\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f143515-5f74-473e-b90b-14f80f898115": {"__data__": {"id_": "1f143515-5f74-473e-b90b-14f80f898115", "embedding": null, "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ffb5848-f30c-4cd1-acd7-593f5911b540", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4420d835c82ee23c30f334357d800b9b8ab6deb1d3da6253762f262df692dbdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The network analysis highlights that whilst the cosine distances between embeddings\nfor the LLMs that used a contrastive loss during pre-training are greater, they retain a\nsimilar graph network structure to other LLMs. The number of components (sub-graphs)\ncan be considered a good indicator of the diversity of the whole graph and correlates\nstrongly with the cosine similarity threshold.\n5. Discussion\n5.1. General\nThe pre-training of LLMs is a crucial step in the production of a useful embedding\nspace for downstream tasks which rely on sentence or document level embeddings,\nsuch as sequence classification, document information retrieval, clustering and semantic\nsearch. We evaluated various approaches to pre-training LLMs on several downstream\nsequence classification tasks in frozen and full fine-tuned settings.\nWe showed that models pre-trained with contrastive loss functions tended to out-\nperform other pre-training approaches across all our domain-specific datasets, with\nfewer training samples required to obtain reasonable evaluation performance. Whilst\nthe performance in the frozen setting did not match that of the full fine-tuned setting,\nthe pre-training had a clear influence on how usable these LLMs embedding spaces are\nfor the downstream tasks where classification boundaries are important. An attempt to\nintegrate further structured metadata based on the note category information did not\nseem to provide a performance gain in the classification tasks above standard MLM\ntraining. However in the embedding space analysis, the approach yielded better clus-\ntering metrics. The metadata for each dataset differed in nature, and the usefulness of\nthis information for the use-cases presented in this work are difficult to determine and\nwhether or not they hold utility for other tasks would require further investigations.\nThe DeCLUTR based models appear to produce an embedding space quite distinct\nto the other pre-training methods, with large separation between different documents\nand classes. However, it is also clear the the cosine similarity of documents within the\nsame class appears low, potentially highlighting that DeCLUTR did not align well with\nknown classes (recall DeCLUTR is unsupervised and had no access to class labels).\nThe network analysis highlighted a surprising consistency in resultant graph spaces: the\npattern of node numbers, component sizes, and degree of major components remained\nquite stable across each LLM, regardless of the apparent differences in cosine distances.\nThe utility of the embeddings produced by LLMs as direct features for downstream\napplications is particularly sought after in resource-constrained settings, where further\nfine-tuning can be difficult. Domain adaptation of open LLMs to the clinical domain\nwithin the UK remains an important goal, with our results showing the largest per-\nformance gaps between the general LLMs and the UK NHS dataset trained LLMs\npresented.\nThe resource efficiency of domain adaptation through pre-training is not straight-\nforward. Of course, the most resource friendly approach would be to include no domain\nadaption at all, although we suggest the potential performance gain offered by all\ncontinued pre-training approaches presented here is worth the relatively low cost: all\npre-training could be completed in a matter of hours on a single GPU.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a59775cd-a041-406e-8fbc-c108f640e4df": {"__data__": {"id_": "a59775cd-a041-406e-8fbc-c108f640e4df", "embedding": null, "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93566e10-b601-4b41-b65e-649524bd5359", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6c54c67e61c33e300b45c6f601d60b25fb22ca869559d71d295e838d17bd7d11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.2. Limitations\nDeCLUTR sampling. The sampling parameters that produced the best model for each\ndataset studied were not equal, were dependent on the length of the documents, and\nwere not flexible to varying lengths of documents. Further, the sampling procedure also\neliminates documents that are not long enough to satisfy a pre-determined minimum\nlength. This in fact means the DeCLUTR models were trained on a restricted sub-sample\nwhen compared to the other pre-training methods.\nNote category pre-training loss. The number of possible contrastive loss functions\napplicable to our dataset is very large, and we were not able to explore different setups\nnor vary the hyperparameters extensively. Similarly, differential selection of samples\nto use for the pre-training objective was not explored in this work. Further work could\nalso utilise modern transformer architecture adaptations to allow larger batch sizes and\ncontext windows [5, 30, 32].\nStrict training settings. We opted to showcase different pre-training methods in a\nrestricted resource setting, with a single low-end GPU and a limit on training time in\nline with similar budget-oriented LLM training research [34]. For this reason, we have\nnot explored fully the extremes of these approaches and their application to a wider\nrange of datasets and tasks.\n5.3. Conclusion\nThis study underscores the significance of pre-training methodology in domain\naligning language models with downstream tasks reliant on document representations.\nContrastive, self-supervised objectives prove most effective across the sequence classifi-\ncation tasks, outperforming masked language modeling. While incorporating structured\nmetadata during pre-training did not further improve performance, unsupervised meth-\nods like DeCLUTR yield more distinct, clustered embedding spaces. Notably, model\nembedding graph connectivity patterns persist irrespective of pre-training differences,\nimplying consistent high-level structure.\nDomain adaptation to UK NHS data remains critical, with specialized models\nsubstantially improving over general ones. The resource efficiency of this adaptation is\nnon-trivial; no adaptation maximizes efficiency without forfeit to performance gains.\nThe low resource approaches presented still confer valuable improvements worthy of\ntheir marginal cost.\nWhile we assessed a range of pre-training schemes and NHS-adapted models on\nsubsequent classification performance, open questions persist. Future work should\nexplore modern architectures and objectives, optimized sampling for contrastive learning,\nenhanced use of metadata, and applications beyond classification like information\nretrieval. Broader hyperparameter tuning may unveil further gains. Still, this research\nvalidates the utility of pre-trained healthcare language models, provides pre-training best\npractices, and motivates specialized adaptation - advancing practice while illuminating\nareas for additional inquiry.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52b37959-3467-4a41-897c-626d538c467a": {"__data__": {"id_": "52b37959-3467-4a41-897c-626d538c467a", "embedding": null, "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76706c26-ad0f-4ab9-9344-2d9bb4b8f0bf", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b203ada65ffbd62f884304c5774d79b7af9fcd699921069aa88257e21b3ed470", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. Acknowledgements\nThe authors would like to thank the members of the Patient Safety Team in NHS Eng-\nland who engaged with us throughout the project and shared their in-depth knowledge of\nthe area to help shape our exploration. We would also like to acknowledge the work and\nsupport of the Oxford Research Informatics Team: Tanya Smith, Research Informatics\nManager, Adam Pill, Suzanne Fisher, Research Informatics Systems Analysts and Lulu\nKane Research Informatics Administrator.\n7. Funding\nNT was supported by the EPSRC Center for Doctoral Training in Health Data\nScience (EP/S02428X/1) and completed part of this work during a PhD internship with\nNHS England in late 2022. AK, ANH, and DWJ were supported in part by the NIHR\nAI Award for Health and Social Care (AI-AW ARD02183). DWJ is part supported by an\nNIHR Infrastructure Programme (NIHR203316).\nContributions\nN.T, D.S, A.K, and A.N.J conceptualised this work. N.T and D.S curated the\ndatasets. N.T developed pre-processing, experiment running and analysis code. N.T.\nperformed experiments across the three datasets. D.S performed extension experiments\non PSIR data. N.T and D.S explored and evaluated experimental results. N.T drafted\nthe manuscript. D.S, D.W.J, A.K, and A.N.H revised and edited the manuscript. All\nauthors read and approved the final version of the manuscript.\nReferences\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\n[2] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 38\u201345, Online, October 2020. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL\nhttps://aclanthology.org/2020.emnlp-demos.6.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8192c6ae-e949-4627-b834-d8c1bfbee1c7": {"__data__": {"id_": "8192c6ae-e949-4627-b834-d8c1bfbee1c7", "embedding": null, "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7e65899-2610-40f9-b4b4-2854fdb336c2", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "1ae40ae32cf25a924e6c5e5cfa5cfe789a63ebf361e948ca9a23fb7d8211e572", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Atten-\ntion is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , volume 30. Curran Associates,\nInc., 2017. URL https://proceedings.neurips.cc/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[4] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for\nparameter-efficient prompt tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing , pages 3045\u20133059, On-\nline and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL\nhttps://aclanthology.org/2021.emnlp-main.243.\n[5] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\nAmanpreet Singh, and Douwe Kiela. Generative Representational Instruction\nTuning, February 2024. URL http://arxiv.org/abs/2402.09906.\narXiv:2402.09906 [cs].\n[6] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling\nclinical notes and predicting hospital readmission, 2019. URLhttps://arxiv.\norg/abs/1904.05342.\n[7] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan\nNaumann, and Matthew McDermott. Publicly available clinical BERT embeddings.\nIn Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages\n72\u201378, Minneapolis, Minnesota, USA, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/W19-1909. URL https://aclanthology.\norg/W19-1909.\n[8] Milad Moradi, Kathrin Blagec, Florian Haberl, and Matthias Samwald. Gpt-\n3 models are poor few-shot learners in the biomedical domain, 2021. URL\nhttps://arxiv.org/abs/2109.02555.\n[9] Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat,\nMoshe Wasserblat, and Oren Pereg. Efficient Few-Shot Learning Without\nPrompts, September 2022. URL http://arxiv.org/abs/2209.11055.\narXiv:2209.11055 [cs].\n[10] Bernal Jim\u00e9nez Guti\u00e9rrez, Nikolas McNeal, Clay Washington, You Chen, Lang Li,\nHuan Sun, and Yu Su. Thinking about GPT-3 In-Context Learning for Biomedical\nIE? Think Again. 2022. doi: 10.48550/ARXIV .2203.08410. URL https:\n//arxiv.org/abs/2203.08410. Publisher: arXiv Version Number: 3.\n[11] Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and\nGuoyin Wang. Text Classification via Large Language Models, May 2023. URL\nhttp://arxiv.org/abs/2305.08377. arXiv:2305.08377 [cs].\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16590794-e6c7-4a20-9ade-b4c80f1fab62": {"__data__": {"id_": "16590794-e6c7-4a20-9ade-b4c80f1fab62", "embedding": null, "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5fb8a789-0d8e-42f3-a909-641e696f3a64", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a78839e4ec127409fdf5a1eb7095026cb5f2bdce5fd35a4d84860111175afe72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does Synthetic\nData Generation of LLMs Help Clinical Text Mining?, April 2023. URL http:\n//arxiv.org/abs/2303.04360. arXiv:2303.04360 [cs].\n[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. 2019. doi: 10.48550/ARXIV .1907.11692.\nURL https://arxiv.org/abs/1907.11692.\n[14] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using\nSiamese BERT-Networks, August 2019. URL http://arxiv.org/abs/\n1908.10084. arXiv:1908.10084 [cs].\n[15] John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. DeCLUTR: Deep\nContrastive Learning for Unsupervised Textual Representations. In Proceed-\nings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 879\u2013895, Online, 2021. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.72. URL\nhttps://aclanthology.org/2021.acl-long.72.\n[16] Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylip\u00e4\u00e4\nHellqvist, and Magnus Sahlgren. Semantic re-tuning with contrastive tension.\nIn International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=Ov_sMNau-PF.\n[17] UK NHS. Abbreviations you may find in your health records\n- NHS App help and support, June 2021. URL https:\n//www.nhs.uk/nhs-app/nhs-app-help-and-support/\nhealth-records-in-the-nhs-app/abbreviations-commonly-found-in-medical-records/ .\n[18] Niall Taylor, Yi Zhang, Dan W. Joyce, Ziming Gao, Andrey Kormilitzin, and\nAlejo Nevado-Holgado. Clinical Prompt Learning With Frozen Language Mod-\nels. IEEE Transactions on Neural Networks and Learning Systems , pages\n1\u201311, 2023. ISSN 2162-2388. doi: 10.1109/TNNLS.2023.3294633. URL\nhttps://ieeexplore.ieee.org/document/10215061.\n[19] Yan Luo, Yuki Kataoka, Edoardo G. Ostinelli, Andrea Cipriani, and Toshi A.\nFurukawa. National Prescription Patterns of Antidepressants in the Treatment of\nAdults With Major Depression in the US Between 1996 and 2015: A Population\nRepresentative Survey Based Analysis. Frontiers in Psychiatry, 11, 2020. ISSN\n1664-0640.\n[20] Zeming Chen, Alejandro Hern\u00e1ndez Cano, Angelika Romanou, Antoine Bonnet,\nKyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K\u00f6pf,\nAmirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy,\nIgor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne\nHartley, Martin Jaggi, and Antoine Bosselut. MEDITRON-70B: Scaling Medical\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "993587df-319b-476d-a277-af1a9bc2532a": {"__data__": {"id_": "993587df-319b-476d-a277-af1a9bc2532a", "embedding": null, "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54b19a41-4b7c-46b7-ab78-ad6a37f4aea9", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f2733a6b7f4fad8cb728eb21e57ff2848d820e83a33491e6dd1d40892463e300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pretraining for Large Language Models, November 2023. URLhttp://arxiv.\norg/abs/2311.16079. arXiv:2311.16079 [cs].\n[21] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple Contrastive\nLearning of Sentence Embeddings, May 2022. URL http://arxiv.org/\nabs/2104.08821. arXiv:2104.08821 [cs].\n[22] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. Linkbert: Pretraining lan-\nguage models with document links. In Association for Computational Linguistics\n(ACL), 2022.\n[23] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christo-\npher D. Manning, Percy Liang, and Jure Leskovec. Deep Bidirectional Language-\nKnowledge Graph Pretraining, October 2022. URL http://arxiv.org/\nabs/2210.09338. arXiv:2210.09338 [cs].\n[24] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier.\nSelf-alignment pretraining for biomedical entity representations. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 4228\u20134238,\nJune 2021.\n[25] Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li Wei H. Lehman, Mengling\nFeng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\nCeli, and Roger G. Mark. Mimic-iii, a freely accessible critical care database.\nScientific Data, 3, 5 2016. ISSN 20524463. doi: 10.1038/sdata.2016.35.\n[26] Niall Taylor, Yi Zhang, Dan W. Joyce, Ziming Gao, Andrey Kormilitzin, and Alejo\nNevado-Holgado. Clinical prompt learning with frozen language models. IEEE\nTransactions on Neural Networks and Learning Systems, pages 1\u201311, 2023. doi:\n10.1109/TNNLS.2023.3294633.\n[27] NHS Digital. Mental health bulletin: 2019-20 annual report. Technical report,\n2020. URL https://digital.nhs.uk/data-and-information/\npublications/statistical/mental-health-bulletin/\n2019-20-annual-report .\n[28] NHS England \u00bb National patient safety incident reports.\nURL https://www.england.nhs.uk/patient-safety/\nnational-patient-safety-incident-reports/ .\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach, July 2019. URL http:\n//arxiv.org/abs/1907.11692. arXiv:1907.11692 [cs].\n[30] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-\ndocument transformer. 2020. URL https://arxiv.org/abs/2004.\n05150.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b144cc8-10ff-4f9e-be1a-f2c31ab2618d": {"__data__": {"id_": "3b144cc8-10ff-4f9e-be1a-f2c31ab2618d", "embedding": null, "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54e0e8a5-533d-4d9b-8409-cf348235ba04", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "3205ae2da3428904a14af011b274f047871044b81551737e60d07f870281679e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[31] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-\nbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. Efficiently Scaling Transformer Inference, November 2022. URL\nhttp://arxiv.org/abs/2211.05102. arXiv:2211.05102 [cs].\n[32] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAtten-\ntion: Fast and Memory-Efficient Exact Attention with IO-Awareness, June 2022.\nURL http://arxiv.org/abs/2205.14135. arXiv:2205.14135 [cs].\n[33] Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised Con-\ntrastive Learning for Pre-trained Language Model Fine-tuning, April 2021. URL\nhttp://arxiv.org/abs/2011.01403. arXiv:2011.01403 [cs].\n[34] Jonas Geiping and Tom Goldstein. Cramming: Training a Language Model on a\nSingle GPU in One Day, December 2022. URL http://arxiv.org/abs/\n2212.14034. arXiv:2212.14034 [cs].\n[35] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. CoRR, abs/1901.08746, 2019.\nURL http://arxiv.org/abs/1901.08746.\n[36] Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan\nNaumann, and Matthew B. A. McDermott. Publicly Available Clinical BERT\nEmbeddings. April 2019. URL http://arxiv.org/abs/1904.03323.\n[37] David L. Davies and Donald W. Bouldin. A Cluster Separation Measure. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, PAMI-1(2):224\u2013227,\nApril 1979. ISSN 1939-3539. doi: 10.1109/TPAMI.1979.4766909. URL\nhttps://ieeexplore.ieee.org/document/4766909. Conference\nName: IEEE Transactions on Pattern Analysis and Machine Intelligence.\n[38] \u00d6zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2010 i2b2/V A\nchallenge on concepts, assertions, and relations in clinical text. Journal of the\nAmerican Medical Informatics Association : JAMIA, 18(5):552\u2013556, 2011. ISSN\n1067-5027. doi: 10.1136/amiajnl-2011-000203. URL https://www.ncbi.\nnlm.nih.gov/pmc/articles/PMC3168320/.\n[39] Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. Evaluating temporal relations in\nclinical text: 2012 i2b2 Challenge. Journal of the American Medical Informatics\nAssociation : JAMIA , 20(5):806\u2013813, September 2013. ISSN 1067-5027. doi:\n10.1136/amiajnl-2013-001628. URL https://www.ncbi.nlm.nih.gov/\npmc/articles/PMC3756273/.\n[40] Amber Stubbs, Christopher Kotfila, and \u00d6zlem Uzuner. Automated systems\nfor the de-identification of longitudinal clinical narratives: Overview of 2014\ni2b2/UTHealth shared task Track 1. Journal of Biomedical Informatics , 58:\nS11\u2013S19, December 2015. ISSN 1532-0464. doi: 10.1016/j.jbi.2015.06.007.\nURL https://www.sciencedirect.com/science/article/pii/\nS1532046415001173.\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8fbe001-34b6-4a32-bb59-03abd03b578b": {"__data__": {"id_": "e8fbe001-34b6-4a32-bb59-03abd03b578b", "embedding": null, "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c12a383-a724-42b8-ad82-60fa5bb05798", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7791361e6adffaa719cebafa0c3beeeb4d9b6ee47863d6f1c58aa8a01f7b28ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] Omid Rohanian, Mohammadmahdi Nouriborji, Hannah Jauncey, Samaneh\nKouchaki, ISARIC Clinical Characterisation Group, Lei Clifton, Laura Merson,\nand David A. Clifton. Lightweight Transformers for Clinical Natural Language\nProcessing, February 2023. URL http://arxiv.org/abs/2302.04725.\narXiv:2302.04725 [cs].\nAppendix A. Dataset details\nAppendix A.1. Document lengths\nDistributions and median document lengths for each dataset are provided below\nin Fig.A.7. The MIMIC-III dataset has consistently longer documents, with the PSIR\ndataset having relatively short documents.\nFigure A.7: Distribution of the number of tokens in documents for each respective dataset\nAppendix B. Hardware details\nDue to the varying secure locations and computational infrastructure of the hosts\nof the datasets, it was impossible to match the hardware used. However, all training\nwas carried out on one GPU only. The only difference between the setups was the exact\nmodel of GPU. For the PSIR dataset both training and inference were performed on a\nprivate single machine hosted by Microsoft Azure with the following main specifica-\ntions: 1 x NVIDIA Tesla T4 GPU. The OHFT dataset utilised the same Tesla T4 GPU\narchitecture but accessed via a private Amazon Web Services (AWS) instance. The\nMIMIC-III dataset was housed locally, and we utilised single NVIDIA RTX 2080.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "82ac4e76-0d4b-4044-a5a5-9aeac3b8cd2b": {"__data__": {"id_": "82ac4e76-0d4b-4044-a5a5-9aeac3b8cd2b", "embedding": null, "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27aefa1e-9901-4e6a-8636-26162d8e33b9", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95752f6c5bf4a98dcd9a9451cee046368dab718e8115bfd3c6ea074e871c9708", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix C. Downstream classification performance\nAppendix C.1. Patient Safety Incident Reports\nThe evaluation results for the severity classification task (P-Sev) and incident type\n(P-Cat) task are presented in Table C.5.\nFor both tasks we present results in both the frozen LLM and fine-tuned settings.\nThe frozen LLM setting means the parameters of the LLM are set tonot require gradient,\nwhich means gradients will not be computed during the backward pass and keeping\nthese weights fixed and only the newly introduced parameters of the classification head\nare updated during training. Fine-tuning on the other hand will update all parameters,\nincluding the LLM.\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.644 0.643 0.718 0.643 0.644\nRoBERTa-PSIR Frozen 0.704 0.687 0.787 0.679 0.704\nRoBERTa-PSIR-DeCLUTR Frozen 0.786 0.765 0.869 0.752 0.786\nRoBERTa-PSIR-note Frozen 0.699 0.678 0.773 0.669 0.699\nRoBERTa-base Finetuned 0.847 0.636 0.926 0.640 0.646\nRoBERTa-PSIR Finetuned 0.866 0.835 0.942 0.816 0.866\nRoBERTa-PSIR-DeCLUTR Finetuned 0.870 0.850 0.944 0.836 0.870\nRoBERTa-PSIR-note Finetuned 0.863 0.833 0.936 0.814 0.863\n(a) Severity classification task (P-Sev)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.197 0.163 0.794 0.314 0.197\nRoBERTa-PSIR Frozen 0.498 0.474 0.851 0.494 0.499\nRoBERTa-PSIR-DeCLUTR Frozen 0.580 0.549 0.900 0.559 0.580\nRoBERTa-PSIR-note Frozen 0.421 0.354 0.847 0.415 0.421\nRoBERTa-base Finetuned 0.646 0.636 0.926 0.640 0.646\nRoBERTa-PSIR Finetuned 0.665 0.652 0.935 0.655 0.665\nRoBERTa-PSIR-DeCLUTR Finetuned 0.670 0.659 0.935 0.667 0.670\nRoBERTa-PSIR-note Finetuned 0.660 0.647 0.933 0.652 0.660\n(b) Incident category classification task (P-Cat)\nTable C.5: Evaluation metrics for the incident category classification task (P-Cat) after one epoch for various\nmodels in both frozen and full fine-tuned settings\nAppendix C.2. MIMIC-III\nEvaluation of the different LLMs for the MIMIC-III note category task (M-Cat) and\nfor the MIMIC-III ICD-9 triage task (M-Tri) are presented Table C.6.\nAppendix C.3. OHFT\nEvaluation results for the OHFT note category task (O-Cat) and OHFT Accepted\nTriage Team task (O-Tri) are presented in Table C.7.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3e436e3-bc48-4eb7-9314-4f4c3813f316": {"__data__": {"id_": "e3e436e3-bc48-4eb7-9314-4f4c3813f316", "embedding": null, "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f04354b3-730e-4646-be45-1f868be95f12", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "7e58657d9d72111c289ef8b6e8b8ccceca9e3a90905af3ba06882769b6e4afd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.433 0.347 0.949 0.406 0.433\nRoBERTa-mimic Frozen 0.448 0.353 0.979 0.434 0.448\nRoBERTa-mimic-DeCLUTR Frozen 0.778 0.770 0.962 0.791 0.778\nRoBERTa-base Finetuned 0.979 0.979 0.999 0.980 0.979\nRoBERTa-mimic Finetuned 0.978 0.978 0.999 0.979 0.978\nRoBERTa-mimic-DeCLUTR Finetuned 0.985 0.985 0.999 0.986 0.985\n(a) MIMIC-III note category task (M-Cat)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.702 0.264 0.778 0.381 0.293\nRoBERTa-mimic Frozen 0.315 0.188 0.871 0.435 0.303\nRoBERTa-mimic-DeCLUTR Frozen 0.776 0.545 0.893 0.561 0.542\nRoBERTa-mimic-note Frozen 0.591 0.409 0.834 0.439 0.473\nRoBERTa-base Finetuned 0.909 0.827 0.984 0.808 0.855\nRoBERTa-mimic Finetuned 0.912 0.824 0.990 0.797 0.884\nRoBERTa-mimic-DeCLUTR Finetuned 0.917 0.831 0.990 0.794 0.890\nRoBERTa-mimic-note Finetuned 0.906 0.819 0.987 0.788 0.867\n(b) MIMIC-III ICD-9 triage task (M-Tri)\nTable C.6: Evaluation metrics for text classification tasks after one epoch\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.212 0.171 0.624 0.156 0.212\nRoBERTa-OHFT Frozen 0.224 0.163 0.697 0.162 0.224\nRoBERTa-OHFT-DeCLUTR Frozen 0.292 0.255 0.709 0.272 0.292\nRoBERTa-base Finetuned 0.380 0.348 0.800 0.453 0.380\nRoBERTa-OHFT Finetuned 0.455 0.431 0.852 0.463 0.455\nRoBERTa-OHFT-DeCLUTR Finetuned 0.404 0.390 0.821 0.434 0.404\n(a) OHFT note category task (O-Cat)\nModel LLM Accuracy F1 AUC Precision Recall\nRoBERTa-base Frozen 0.424 0.390 0.752 0.415 0.424\nRoBERTa-OHFT Frozen 0.495 0.469 0.830 0.572 0.495\nRoBERTa-OHFT-DeCLUTR Frozen 0.557 0.539 0.831 0.565 0.557\nRoBERTa-OHFT-note Frozen 0.414 0.391 0.767 0.466 0.414\nRoBERTa-base Finetuned 0.677 0.665 0.918 0.692 0.677\nRoBERTa-OHFT Finetuned 0.752 0.753 0.935 0.769 0.752\nRoBERTa-OHFT-DeCLUTR Finetuned 0.738 0.738 0.934 0.750 0.738\nRoBERTa-OHFT-note Finetuned 0.744 0.743 0.933 0.745 0.744\n(b) OHFT Accepted triage task (O-Tri)\nTable C.7: Evaluation metrics for text classification tasks after one epoch\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56a86f1e-9706-4c58-9283-d417a3ea3953": {"__data__": {"id_": "56a86f1e-9706-4c58-9283-d417a3ea3953", "embedding": null, "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a97c2773-c169-44f8-8180-e0e8eb5596cc", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "82b7197a4395c28774faf26fdb1085a001a3211f43d691f2cb8cef3c609f4fe5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix D. Performance of open clinical models\nIn Table. D.8 we provide some further evaluation results of similarly sized open\nLLMs pre-trained on biomedical and clinical text, including models trained on all\nMIMIC-III notes. BioLinkBERT and Bio-ClinicalBERT have previously achieved near\nstate of the art when applied to open medical NLP datasets [6, 35, 36]. We have results\nfor only the MIMIC-III triage task (M-Tri) and the OHFT accepted triage task (O-Tri).\nWe can see a slight performance drop with the open LLMs when compared with our own\non the M-Tri task, and a larger performance drop for the OHFT dataset. This highlights\nthe relative importance of domain pre-training for the UK based dataset.\nModel LLM Accuracy F1 AUC Precision Recall\nBioLinkBERT-base Frozen 0.492 0.298 0.877 0.386 0.413\nBio-ClinicalBERT Frozen 0.615 0.405 0.820 0.435 0.440\nBioLinkBERT-base Finetuned 0.942 0.870 0.988 0.846 0.904\nBio-ClinicalBERT Finetuned 0.919 0.827 0.988 0.812 0.855\n(a) MIMIC-III ICD-9-triage task (M-Tri)\nModel PLM Accuracy F1 AUC Precision Recall\nBioLinkBERT-base Frozen 0.376 0.356 0.706 0.410 0.376\nBio-ClinicalBERT Frozen 0.395 0.382 0.713 0.435 0.395\nBioLinkBERT-base Finetuned 0.703 0.700 0.915 0.714 0.703\nBio-ClinicalBERT Finetuned 0.715 0.714 0.922 0.718 0.715\n(b) OHFT accepted triage team task (O-Tri)\nTable D.8: Evaluation metrics for text classification tasks after one epoch\nAppendix E. Cluster analysis\nA common approach to exploring a dataset through an LLMs embedding space is\nto perform a form of unsupervised clustering analysis. A simple analysis using the\nK-Means clustering technique is provided below. The dataset used here is the MIMIC-III\nICD-9 triage task (M-Tri). The following clustering metrics are reported: David Bouldin\nindex (DBi) [37], Calinski Harabaz Index (CHi), and the silhouette score (SS).\nThe DBi determines the average similarity measure of each derived cluster with its\nmost similar cluster, with similarity defined as the ratio of within-cluster distances to\nbetween-cluster distance (far apart clusters with little dispersion resulting in a better,\nlower score). The CHi is the ratio of the sum of between-cluster and within-cluster\ndispersion, with higher scores indicating more separable clusters. The SS assesses the\noverlap of clusters and ranges from -1 to 1 with 1 being optimal.\nWe find that the model trained on MIMIC-III using MLM only (RoBERTa-mimic)\nappears to perform best, with DeCLUTR models actually fairing worse. This is a little\nsurprising considering the DeCLUTR models generally were optimal for downstream\nclassification adaptation.\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "57b81c63-7d90-4b01-a1d9-6de1aea87228": {"__data__": {"id_": "57b81c63-7d90-4b01-a1d9-6de1aea87228", "embedding": null, "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f2e6bec-c011-4b50-9a7f-c3310682035a", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "09115b6aa4cb15fb629915fd79f689fb18c9491979a5b831b175c0d1b4d71188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model DBi score (<) CH score (>) Silhouette score (>) Optimal K\nRoBERTa-base 2.103 478.670 0.147 5\nDeCLUTR-base 3.031 284.237 0.096 4\nRoBERTa-base-DeCLUTR 2.848 300.310 0.114 4\nRoBERTa-mimic 1.663 911.911 0.235 4\nRoBERTa-mimic-note 1.582 755.344 0.230 6\nTable E.9: K-means cluster analysis results for each pre-trained LLM related to the Mimic-III ICD-9-Triage\ndataset.\nAppendix F. Pre-training effects on token classification\nOne aspect of this work was to determine how the pre-training effected the docu-\nment level embeddings, however it is also interesting to consider how the word level\nembeddings have changed: in particular we may expect the contrastive loss functions to\nhave moved the LLMs objective function away from the original MLM objective which\nshould impact token level tasks.\nWe investigated the performance of each pre-trained model from the Mimic-III\ndataset on three token classification tasks (these were readily available with gold standard\nground truths). The tasks were Named Entity Recognition (NER) tasks formed during\nvarious I2B2 challenges [38, 39, 40].\nMicro averaged F1 scores for each task are provided in Table F.10, and to determine\nthe utility of the models embeddings as features we explore downstream fine-tuning\nwith the LLM frozen and compare with full fine-tuning. Results do not show any major\ndifferences between the models, and generally keeping the LLM frozen did not allow\nany learning of the task in two of the three tasks. In the fine-tuned setting, all models\nconverge on similar performance in line with other studies utilising the same tasks [41].\nModel PLM i2b2 2010 i2b2 2012 i2b2 2014\nRoBERTa-base Frozen 0.052 0.042 0.639\nDeCLUTR-base 0.043 0.065 0.651\nRoBERTa-base-DeCLUTR 0.083 0.092 0.551\nRoBERTa-mimic 0.041 0.08 0.636\nRoBERTa-mimic-note 0.006 0.067 0.616\nRoBERTa-base Fine-tuned 0.847 0.83 0.975\nDeCLUTR-base 0.844 0.833 0.976\nRoBERTa-base-DeCLUTR 0.854 0.837 0.977\nRoBERTa-mimic 0.854 0.847 0.976\nRoBERTa-mimic-note 0.855 0.841 0.979\nTable F.10: Token classification results. For brevity we report F1 micro for each of the models for each dataset\nAppendix G. Training details\nData splits. In order to avoid direct data leakage from the continued language model pre-\ntraining steps and the subsequent downstream classification tasks, we created entirely\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "44e8abd9-f6cf-4189-bf71-0df491339798": {"__data__": {"id_": "44e8abd9-f6cf-4189-bf71-0df491339798", "embedding": null, "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2c6eeba-463b-40a2-85a9-64edec91ea1f", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f41f0fbe4e38bf4e73abf89942ae4658f2d87708c232969584692ee0cb533360", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "separate subsets of data: one partition formed the full training and validation sets for\nthe pre-training and the other formed the entire training and validation sets for the\ndownstream tasks. For the different language model pre-training approaches, we subset\napproximately 250,000 notes for each our datasets.\nPre-processing. For language modelling with transformer based models minimal data\ncleaning is required, as the tokenization of inputs paired with the contextualised repre-\nsentations of words means we want to keep as much of the original input as possible.\nTypical processing steps would include removal of carriage returns, tabs, and white\nspace and any poorly encoded characters.\nHyperparameter choices. Table G.11 reports the main hyperparameter choices for our\nexperiments. Notably, the batch size has direct bearing on the difficulty of the contrastive\nobjective the DeCLUTR and note contrastive models were trained with. In contrastive\nloss paradigms, for any given anchor, the model needs to make the correct match out of\nthe Nb reports in the batch. The probability of successfully finding the correct match by\nchance decreases with the batch size. We set the batch size to 32 on the single-GPU\nmachine as this was the maximum possible across all span length options we tried.\nThe original authors of DeCLUTR found 2 anchors was optimal for training, whereas\nthe number of positives had little effect. The optimal span length differed between each\nof our datasets: 1024 for MIMIC-III, 64 for OHFT and Patient Safety, which appears in\npart related to the average length of document per dataset.\nFor the note contrastive models, we found combining MLM and the note category\nlosses to be optimal, although when given equal weighting the note category loss often\ndominated due to it being new to the already MLM pre-trained model. Thus, we found\nlowering the weighting of that loss function dramatically improved the subsequent\ndownstream performance. Whilst we did not explore this thoroughly, we found a\nweighting of 0.1 for the note category loss performed reasonably. Each of the different\nexperiments and objectives can require different hyperparameters and we opted to follow\nthose used by original implementations where possible.\nParameter MLM DeCLUTR Note Contrastive Downstream tasks\nBatch size 16 32 16 16\nGradient accumulation steps 4 1 1 1\nEmbedding dimension 768 768 768 768\nLearning rate 1 \u00d7 10\u22125 1 \u00d7 10\u22125 1 \u00d7 10\u22125 1 \u00d7 10\u22124\nOptimiser Adam W Adam W Adam W Adam W\nSpan length - [16, 64, 512, 1024] - -\nContrastive loss weight - - [0.1,0.3, 1.0] -\nEpochs 3 3 3 5\nTable G.11: Overview of hyperparameters used in our experiments. All training regimes utilised a linear\nscheduler with warm-up.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3b758dd-070c-4319-89c0-e2f0a70f8988": {"__data__": {"id_": "f3b758dd-070c-4319-89c0-e2f0a70f8988", "embedding": null, "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b84693ac-29f4-4d46-8330-a9ac96f93297", "node_type": "4", "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "8c63570e1af5e658fff674fe5a4dc07a3b1277f8241f88d457c157eaeb4c96f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appendix H. DeCLUTR Sampling\nThe DeCLUTR sampling algorithm used enforces a minimum length of documents\ndependent on the span length, and the number of anchor spans to derive. The effect of\nthe minimum length on number of suitable samples for each dataset is provided in Table\nH.12.\nProportion of 250k LM training dataset\nMin. document length OHFT PSIR MIMIC-III\n16 0.97 0.88 0.99\n32 0.89 0.69 0.95\n64 0.72 0.41 0.97\n128 0.46 0.15 0.92\n256 0.21 0.04 0.81\n512 0.07 0.02 0.57\n1024 0.02 0.0 0.35\n2048 0.002 0.0 0.11\nTable H.12: Sample distributions for different DeCLUTR sampling minimum document lengths for each of\nthe datasets: OHFT, PSIR, and MIMIC-III. The proportion refers to the amount of samples that meet each of\nthe minimum document length thresholds.\nAppendix I. DeCLUTR extended results\nMore granular evaluation results for the incident report severity (P-Sev) and incident\ncategory (P-Cat) tasks, varying numbers of epochs of pre-training with the DeCLUTR\napproach (i.e. both the MLM and contrastive loss objective). The models were trained\nusing documents with a minimum length of 64 tokens, see Table H.12 for more details.\nAppendix J. Ablation results\nIn order to determine the effect of the contrastive loss components of the note\ncategory pre-training, we investigated isolating each of the MLM and note category\nlosses to create two separate models. Due to data access constraints, we perform this\nanalysis only on the MIMIC-III and OHFT datasets.\nFor the MIMIC-III ICD-9 triage task (M-Tri) we found that the MLM only models\ngenerally perform similarly to the combined loss model, with only a 0.05 drop in F1\nmacro in the frozen setting and a 0.1 increase in the full fine-tuned setting. However,\nthe note category loss only model affects downstream task performance dramatically,\nwith a 0.2 and 0.1 drop in F1 macro in the frozen and fine-tuned settings respectively.\nWith the OHFT Accepted triage team task (O-Tri), there was very little difference:\nnote only loss lead to a drop of 0.05 and 0.01 in frozen and fine-tuned settings respec-\ntively. The MLM only loss model had a subtle increase of 0.02 and 0.01 in frozen and\nfine-tuned settings, but this is negligible.\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "28390e55-c742-4d60-8d97-4a9e56e3338d": {"__data__": {"id_": "28390e55-c742-4d60-8d97-4a9e56e3338d", "embedding": null, "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bae20ff-1e7d-453c-8786-99018c8f3aab", "node_type": "4", "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e41e6f3a724ec6ecfff8ef93a6ac7b1bb8262a194d619ee343303dcfa1d2d5b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Performance after 1 epoch\nName PLM Pre. Epochs Trainable params.(m) Accuracy ROC AUC F1 Prec. Recall\nDeCLUTR-base-incident Frozen 2 0.5 0.758 0.842 0.725 0.713 0.758\n3 0.5 0.763 0.847 0.719 0.709 0.763\n5 0.5 0.763 0.848 0.75 0.741 0.763\n10 0.5 0.771 0.855 0.752 0.74 0.771\n25 0.5 0.769 0.859 0.756 0.746 0.769\n50 0.5 0.755 0.840 0.717 0.705 0.755\nRoBERTa-incident-DeCLUTR Frozen 2 0.5 0.775 0.858 0.750 0.736 0.775\n3 0.5 0.786 0.869 0.765 0.752 0.786\n5 0.5 0.780 0.871 0.769 0.760 0.780\n10 0.5 0.782 0.865 0.751 0.731 0.762\n25 0.5 0.776 0.866 0.752 0.740 0.776\n50 0.5 0.765 0.851 0.734 0.720 0.765\nDeCLUTR-base-incident Fine-tuned 2 125 0.859 0.936 0.833 0.816 0.859\n3 125 0.861 0.937 0.823 0.803 0.861\n5 125 0.858 0.936 0.841 0.828 0.858\n10 125 0.859 0.934 0.827 0.808 0.859\n25 125 0.854 0.934 0.846 0.835 0.854\n50 125 0.851 0.931 0.792 0.767 0.851\nRoBERTa-incident-DeCLUTR Fine-tuned 2 125 0.870 0.944 0.850 0.836 0.870\n3 125 0.868 0.942 0.838 0.819 0.868\n5 125 0.867 0.941 0.831 0.811 0.867\n10 125 0.858 0.938 0.825 0.807 0.858\n25 125 0.863 0.939 0.833 0.815 0.863\n50 125 0.857 0.934 0.831 0.815 0.857\nTable I.13: Evaluation metrics for the severity classification task (P-Sev) for various models in both frozen\nand full fine-tuned settings of DeCLUTR models at various different total epochs of further pre-training for\nperformance after one epoch of training on the task.\nThe drop in performance for the MIMIC-III task may be related to the relationship\nbetween the note category meta data and the subsequent downstream tasks: the ICD-9\ntriage task (M-Tri), as it actually utilises only one type of clinical note, discharge\nsummaries and thus the influence of other note types is not examined.\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6c84e63b-65d2-410e-ba83-af14d1e52504": {"__data__": {"id_": "6c84e63b-65d2-410e-ba83-af14d1e52504", "embedding": null, "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "661bed5a-685f-4927-94e4-668b5a14d86b", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "63336ef3720a36a839ca064d32e5366430897631bfee78a82b0d8a313b114595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a4800bb-9d21-4359-b693-dc5655e980e9", "node_type": "1", "metadata": {}, "hash": "2b759f58eda3587eaa2bc6b219a47a9e886935b69f9bcaace9976a852b8bcd2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Performance after 1 epoch\nName PLM Pre. Epochs Trainable params.(m) Accuracy ROC AUC F1 Prec. Recall\nDeCLUTR-base-incident Frozen 2 0.5 0.548 0.878 0.511 0.525 0.548\n3 0.5 0.542 0.883 0.503 0.509 0.542\n5 0.5 0.559 0.886 0.527 0.531 0.559\n10 0.5 0.562 0.888 0.532 0.534 0.562\n25 0.5 0.577 0.893 0.544 0.557 0.577\n50 0.5 0.550 0.877 0.518 0.538 0.550\nRoBERTa-incident-DeCLUTR Frozen 2 0.5 0.555 0.883 0.521 0.527 0.555\n3 0.5 0.568 0.892 0.531 0.541 0.568\n5 0.5 0.580 0.900 0.549 0.559 0.580\n10 0.5 0.578 0.894 0.544 0.551 0.578\n25 0.5 0.574 0.892 0.545 0.545 0.574\n50 0.5 0.572 0.883 0.548 0.552 0.572\nDeCLUTR-base-incident Fine-tuned 2 125 0.648 0.930 0.641 0.651 0.648\n3 125 0.661 0.930 0.651 0.657 0.661\n5 125 0.649 0.931 0.632 0.641 0.649\n10 125 0.658 0.931 0.647 0.647 0.658\n25 125 0.651 0.931 0.640 0.643 0.651\n50 125 0.645 0.929 0.631 0.635 0.645\nRoBERTa-incident-DeCLUTR Fine-tuned 2 125 0.670 0.935 0.659 0.667 0.670\n3 125 0.668 0.933 0.660 0.672 0.668\n5 125 0.660 0.934 0.646 0.660 0.660\n10 125 0.658 0.932 0.644 0.653 0.658\n25 125 0.659 0.935 0.649 0.654 0.659\n50 125 0.653 0.934 0.644 0.654 0.653\nTable I.14: Evaluation metrics for the incident category classification task (P-Cat) for various models in both\nfrozen and full fine-tuned settings of DeCLUTR models at various different total epochs of further pre-training\nfor performance after one epoch of training on the task.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5a4800bb-9d21-4359-b693-dc5655e980e9": {"__data__": {"id_": "5a4800bb-9d21-4359-b693-dc5655e980e9", "embedding": null, "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "661bed5a-685f-4927-94e4-668b5a14d86b", "node_type": "4", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "63336ef3720a36a839ca064d32e5366430897631bfee78a82b0d8a313b114595", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c84e63b-65d2-410e-ba83-af14d1e52504", "node_type": "1", "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "a660afb9978f0b03775344f858cc95dd11ba20c04ff76e6ded0ab9a47fc9a2b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample size 16 32 64 128 200\nDomain pre-training Task\nNone M-Cat 0.028 0.098 0.082 0.113 0.347\nM-Tri 0.028 0.130 0.207 0.013 0.492\nO-Cat NaN 0.028 0.015 0.042 0.171\nO-Tri 0.071 0.109 0.095 0.186 0.079\nP-Cat 0.022 0.011 0.011 0.011 0.011\nP-Sev 0.466 0.430 0.196 0.430 0.196\nMLM M-Cat 0.075 0.184 0.029 0.167 0.353\nM-Tri 0.098 0.212 0.349 0.196 0.510\nO-Cat NaN 0.021 0.081 0.108 0.163\nO-Tri 0.069 0.134 0.086 0.168 0.213\nP-Cat 0.015 0.011 0.031 0.013 0.038\nP-Sev 0.430 0.271 0.440 0.196 0.431\nDeCLUTR M-Cat 0.269 0.287 0.495 0.618 0.770\nM-Tri 0.146 0.095 0.315 0.431 0.683\nO-Cat 0.120 0.097 0.194 0.248 0.255\nO-Tri 0.190 0.238 0.265 0.356 0.453\nP-Cat 0.024 0.031 0.020 0.065 0.119\nP-Sev 0.435 0.519 0.361 0.506 0.494\nNote contrastive M-Tri 0.025 0.038 0.091 0.190 0.426\nO-Tri 0.106 0.212 0.173 0.349 0.413\nP-Cat 0.027 0.011 0.027 0.015 0.015\nP-Sev 0.237 0.467 0.437 0.454 0.196\nTable I.15: F1 macro score on all tasks after one epoch of training with different number of samples per class.\nBase LLMs were frozen and only the classification head received updates.\n33", "mimetype": "text/plain", "start_char_idx": 1394, "end_char_idx": 2457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e1c747c8-f399-4d1b-a0e5-847e262f457f": {"__data__": {"id_": "e1c747c8-f399-4d1b-a0e5-847e262f457f", "embedding": null, "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3c58480-196b-4c10-a19c-b7421d8df1b0", "node_type": "4", "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "623f55bd7fc64dae8ec833db4aa3c72b791f26c056bce730491bab6771bc828b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample size 16 32 64 128 200\nDomain pre-training Task\nNone M-Cat 0.296 0.831 0.878 0.921 0.979\nM-Tri 0.050 0.023 0.748 0.732 0.827\nO-Cat 0.121 0.165 0.238 0.316 0.348\nO-Tri 0.069 0.115 0.331 0.466 0.490\nP-Cat 0.011 0.012 0.011 0.057 0.397\nP-Sev 0.196 0.223 0.196 0.196 0.412\nMLM M-Cat 0.531 0.892 0.968 0.981 0.978\nM-Tri 0.282 0.367 0.596 0.802 0.824\nO-Cat 0.223 0.245 0.367 0.388 0.431\nO-Tri 0.221 0.236 0.494 0.669 0.706\nP-Cat 0.022 0.012 0.017 0.443 0.557\nP-Sev 0.451 0.435 0.404 0.442 0.197\nDeCLUTR M-Cat 0.734 0.921 0.967 0.981 0.985\nM-Tri 0.294 0.493 0.727 NaN 0.831\nO-Cat 0.237 0.247 0.307 0.328 0.390\nO-Tri 0.218 0.380 0.493 0.504 0.605\nP-Cat 0.051 0.037 0.099 0.399 0.452\nP-Sev 0.204 0.450 0.464 0.391 0.401\nNote contrastive M-Tri 0.134 0.258 0.537 0.759 0.827\nO-Tri 0.232 0.210 0.409 0.539 0.618\nP-Cat 0.013 0.012 0.019 0.129 0.519\nP-Sev 0.430 0.196 0.430 0.340 0.196\nTable I.16: F1 macro score on all tasks after one epoch of training with different number of samples per class.\nAll models were fully fine-tuned.\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "634b3a30-1f59-428e-8049-e37875aa1567": {"__data__": {"id_": "634b3a30-1f59-428e-8049-e37875aa1567", "embedding": null, "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a442861f-e7c3-4f77-9b0b-d929e01bc4d3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "89e1845be1e8ed25816d50adf75b7e892db619885bd21a1a058ec0845cb689e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2409.16860v1  [cs.CV]  25 Sep 2024\nTH E RO L E O F LA N G UAG E MO D E L S I N MO D E R N HE A LT H CA R E :\nA C O M P R E H E N S IV E RE V I E W\nAmna Khalid\nCHRISTUS Santa Rosa Hospital\nNew Braunfels, TX\nA yma Khalid\nRiphah International University\nLahore, Pakistan\nUmar Khalid\nPalo Alto, CA\nABSTRACT\nThe application of large language models (LLMs) in healthca re has gained signi\ufb01cant attention due\nto their ability to process complex medical data and provide insights for clinical decision-making.\nThese models have demonstrated substantial capabilities i n understanding and generating natural\nlanguage, which is crucial for medical documentation, diag nostics, and patient interaction. This\nreview examines the trajectory of language models from thei r early stages to the current state-of-\nthe-art LLMs, highlighting their strengths in healthcare a pplications and discussing challenges such\nas data privacy, bias, and ethical considerations. The pote ntial of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensu re their ethical and effective integration\ninto medical practice.\nKeywords Large Language Models, Healthcare, Machine Learning, Natu ral Language Processing, Medical AI,\nEthics in AI, Clinical Decision Support\n1 Introduction\nDeep learning has revolutionized the way we understand human behavior, emotions, and healthcare-related chal-\nlenges [1, 2, 3, 4]. In recent years, breakthroughs in clinic al language processing have paved the way for transfor-\nmative changes in the healthcare industry. These advanceme nts hold great promise for the deployment of intelligent\nsystems that can support decision-making, accelerate diag nostic work\ufb02ows, and enhance the quality of patient care.\nSuch systems have the potential to assist healthcare profes sionals as they navigate the growing body of medical knowl-\nedge, interpret complex patient records, and craft individ ualized treatment plans. The promise of these systems has\ngenerated signi\ufb01cant excitement within the healthcare com munity [5, 6, 7].\nThe power of large language models (LLMs) lies in their abili ty to analyze vast amounts of medical literature, patient\ndata, and the rapidly growing body of clinical research. Hea lthcare data [8, 9] is inherently intricate, heterogeneous ,\nand extensive. LLMs function as critical tools that help all eviate information overload for healthcare professionals .\nBy automating the processing of medical texts, extracting k ey insights, and applying the knowledge, LLMs have\nthe potential to drive signi\ufb01cant research breakthroughs a nd improve patient care, contributing meaningfully to the\nevolution of the medical \ufb01eld.\nThe excitement surrounding LLMs is largely driven by the imp ressive capabilities of advanced models like OpenAI\u2019s\nGPT -3.5, GPT -4 [10, 11], and Google\u2019s Bard. These models hav e shown remarkable pro\ufb01ciency across a broad range\nof natural language understanding tasks, underscoring the ir pivotal role in healthcare applications. With their abil ity to\ncomprehend and generate human-like text, these models are s et to have a transformative impact on healthcare, where\naccurate communication and information management are par amount [12].\nNatural language processing (NLP) has undergone signi\ufb01can t advancements, with each milestone building on the\nstrengths and limitations of previous approaches. Early de velopments, such as recurrent neural networks (RNNs),\nlaid the groundwork for contextual understanding in NLP tas ks. However, their limitations in handling long-range\ndependencies became clear, necessitating new approaches i n the \ufb01eld.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01e408dc-411e-48b7-a22f-d54e295b5e37": {"__data__": {"id_": "01e408dc-411e-48b7-a22f-d54e295b5e37", "embedding": null, "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "772cc019-fd83-444d-b093-cadea3cad6da", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7cf3a297c91158e3e218340590b9983e3e9fc1c030b3d9964ccfaecbc4a5a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7e05597-bc46-4247-b94c-9da413c04f06", "node_type": "1", "metadata": {}, "hash": "16f822e3a76357f724db4404dbef4751c9ec26cf45bfde6b486f0ef01f7bbb0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nThe turning point came with the introduction of the Transfor mer architecture, which effectively addressed the challen ge\nof capturing distant relationships between words. This inn ovation was crucial for the development of more advanced\nNLP models. The advent of sophisticated language models suc h as Llama 2 [13] and GPT -4, both of which bene\ufb01t\nfrom extensive training datasets, has propelled NLP to new h eights, allowing for deeper understanding and near-\nhuman-level text generation.\nWithin healthcare, specialized versions of models like BER T , including BioBER T and ClinicalBER T [14, 15], were de-\nveloped to address the unique challenges of clinical langua ge, such as medical terminology, ambiguity, and variabilit y\nin usage. However, the use of LLMs in the highly sensitive hea lthcare sector requires careful consideration of privacy,\nsecurity, and ethics. Patient data must be rigorously prote cted, and models must be designed to avoid perpetuating\nbiases or causing harm. Despite these challenges, the poten tial for LLMs to improve healthcare outcomes and drive\ninnovation remains a key focus of ongoing research and devel opment.\nThis review serves as a comprehensive guide for medical rese archers and healthcare professionals aiming to optimize\nthe use of LLMs in their practices. It provides a detailed exp loration of LLM technologies, their applications in\nhealthcare, and critical discussions on fairness, bias, pr ivacy, transparency, and ethical considerations. By addre ssing\nthese aspects, this review highlights the importance of int egrating LLMs into healthcare in a responsible, equitable,\nand effective manner to maximize bene\ufb01ts for both patients a nd providers.\nThe paper is organized into the following sections:\n\u2022 Section 2 introduces the fundamental architecture of LLMs, includin g key components such as Transformers,\nfoundational models, and their multi-modal capabilities.\n\u2022 Section 3 explores the application of LLMs in healthcare, detailing t heir various use cases and the perfor-\nmance metrics used to evaluate them in clinical environment s.\n\u2022 Section 4 delves into the challenges that LLMs face in healthcare, foc using on issues such as explainability,\nsecurity, bias, and ethical concerns.\n\u2022 Finally, the paper concludes with a summary of the \ufb01ndings, discussing the transformative potential of LLMs\nwhile addressing the need for careful implementation to mit igate limitations and ethical challenges.\n2 Overview of Large Language Models\nLarge language models (LLMs) have rapidly advanced due to their ability to understand and generate human-like\ntext across a variety of natural language processing (NLP) t asks [16, 10]. These models are distinguished by their\nextensive number of parameters, pre-training on vast text d atasets, and subsequent \ufb01ne-tuning for speci\ufb01c tasks [17,\n18, 13]. In this section, we examine the core architecture of LLMs, highlight key examples, and explore pre-training\nmethodologies as well as the role of transfer learning [19].\nLLMs leverage the Transformer architecture, which excels in capturing long-range dependencies within text [20]. The\nself-attention mechanism inherent to this architecture en ables models to focus on different parts of the input text bas ed\non their relevance, improving the handling of complex lingu istic relationships.\n2.1 T ransformers and Their Role in Language Models\nA hallmark of LLMs is their scale [21, 22], pre-training on im mense text corpora [23, 13], and the \ufb01ne-tuning process\ntailored to particular tasks [24]. These models, composed o f billions of parameters, are designed to recognize intrica te\npatterns in language data. After undergoing broad pre-trai ning, they are re\ufb01ned using smaller, task-speci\ufb01c datasets ,\nresulting in enhanced performance across a variety of NLP ap plications.\nThe introduction of the Transformer framework revolutioni zed the \ufb01eld by addressing the limitations of earlier archi-\ntectures like recurrent neural networks (RNNs) [20]. This e volution led to the development of powerful models like\nGPT -4 [11] and Llama 2 [13], signi\ufb01cantly improving natural language understanding and generation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7e05597-bc46-4247-b94c-9da413c04f06": {"__data__": {"id_": "c7e05597-bc46-4247-b94c-9da413c04f06", "embedding": null, "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "772cc019-fd83-444d-b093-cadea3cad6da", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7cf3a297c91158e3e218340590b9983e3e9fc1c030b3d9964ccfaecbc4a5a5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01e408dc-411e-48b7-a22f-d54e295b5e37", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "c7089467e7e79e4c8f116b55284c233f679f8c98b86c20c9eb70e5b769c4bc61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After undergoing broad pre-trai ning, they are re\ufb01ned using smaller, task-speci\ufb01c datasets ,\nresulting in enhanced performance across a variety of NLP ap plications.\nThe introduction of the Transformer framework revolutioni zed the \ufb01eld by addressing the limitations of earlier archi-\ntectures like recurrent neural networks (RNNs) [20]. This e volution led to the development of powerful models like\nGPT -4 [11] and Llama 2 [13], signi\ufb01cantly improving natural language understanding and generation.\n2.2 Multi-Modal Language Models: Expanding Capabilities\nA signi\ufb01cant progression in AI is the rise of multi-modal lan guage models (MLLMs), which integrate data from mul-\ntiple sources, such as text, images, and audio. These models , such as BLIP-2 [25], extend the traditional capabilities\nof LLMs by incorporating multiple modalities, allowing for more versatile and robust outputs [26]. MLLMs enable\ntasks such as visual question answering (VQA) and cross-mod al content generation, opening up new possibilities for\nreal-world applications.\n2", "mimetype": "text/plain", "start_char_idx": 3730, "end_char_idx": 4776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b91c5ef0-8a42-4eee-b86a-35fc71cee60b": {"__data__": {"id_": "b91c5ef0-8a42-4eee-b86a-35fc71cee60b", "embedding": null, "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9c2260f-e8fc-475b-9cb2-108fec145301", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d097a1fb7909df82c71f98d6909205036412b9179f3fb11763914caf2e9be7c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 1: Summary of Multi-Modal Language Models\nModel Y ear Capabilities Applications\nBLIP-2 [25] 2023 Image-text integration using\nQformer\nV isual question answering, image-\ntext retrieval\nV isual ChatGPT [26] 2023 T ext and image interaction via GPT Complex queries requiring visual\ninputs\nMoV A [27] 2024 Mixture of experts for image and\ntext\nMulti-modal content generation\nand analysis\nT able 2: Overview of Large Language Models in Healthcare\nModel Y ear Use Case Institution Source\nCode\nBioMistral [30] 2024 Medical Question Answer-\ning\nA vignon Universit\u00e9,\nNantes Universit\u00e9\nmodel\nMed-PaLM 2\n[31]\n2023 Medical Question Answer-\ning\nGoogle Research, Deep-\nMind\nRadiology-\nLlama2 [32]\n2023 Radiology Imaging Analy-\nsis\nUniversity of Georgia\nDeID-GPT\n[33]\n2023 Data De-identi\ufb01cation University of Georgia code\nMed-HAL T\n[34]\n2023 Hallucination Detection Saama AI Research code\nChatCAD [35] 2023 Computer-Aided Diagno-\nsis\nShanghaiT ech Univer-\nsity\ncode\nBioGPT [36] 2023 Classi\ufb01cation, Relation Ex-\ntraction, Question Answer-\ning\nMicrosoft Research code\nGatorTron [37] 2022 Medical T extual Similarity,\nInference, Question An-\nswering\nUniversity of Florida code\n2.3 Applications of Large Language Models in Healthcare\nLLMs have also become prominent in healthcare, where they support tasks such as medical diagnostics, patient care,\nand drug discovery [28, 29]. T ailored models like BioBER T [1 4] and ClinicalBER T [15] are designed to handle the\nspecialized language found in medical records and research . Newer models, including GPT -4 and Google\u2019s Bard, are\nsetting new benchmarks in medical question answering and re lated healthcare applications [6].\n2.4 Real-W orld Healthcare Applications of Large Language M odels\nLLMs have been widely adopted across various healthcare fun ctions, with applications continuing to expand rapidly.\nThese models assist in clinical decision-making, analysis of medical records, and improving patient interactions [38 ].\nThe vast capability of LLMs to process medical data offers be ne\ufb01ts in areas such as diagnostics, administrative ef\ufb01-\nciency, and overall healthcare delivery [39, 40].\n\u2022 Medical Diagnostics: LLMs can help physicians diagnose illnesses by analyzing pa tient data, including\nsymptoms and medical histories, to identify potential heal th conditions [41].\n\u2022 Patient Care: Through personalized recommendations and ongoing patient monitoring, LLMs improve the\nquality of patient care by providing real-time insights [42 ].\n\u2022 Clinical Decision Support: LLMs offer healthcare professionals evidence-based recom mendations, enhanc-\ning clinical decision-making and treatment strategies [43 ].\n\u2022 Medical Literature Review: By summarizing large volumes of medical literature, LLMs he lp healthcare\nprofessionals stay current with new developments and best p ractices [44].\n\u2022 Drug Discovery: LLMs facilitate drug discovery by analyzing molecular data to identify potential com-\npounds for new drugs [28, 45].\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c94f9853-bfb3-48bc-92f9-2039a8d7602e": {"__data__": {"id_": "c94f9853-bfb3-48bc-92f9-2039a8d7602e", "embedding": null, "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c632f830-aa83-4fa5-ad9c-786d969f24e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "65c7e94fa2a0ba9b58f1482ba11cb2aefa81b91919d7bd49785ead0c53713e97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 3: Evaluation Metrics for LLMs in Healthcare Applicat ions\nMetric T ask Description Key Results\nPerplexity Language Generation Measures model uncertainty Lower perplexity indicates better\nlanguage generation performance\nBLEU Translation Evaluates overlap between\ngenerated and reference text\nClinicalGPT achieved a BLEU\nscore of 13.9 [48]\nROUGE Summarization Assesses recall of generated\nsummaries\nBioMedLM attained a ROUGE-L\nscore of 24.85 [49]\nF1 Score Classi\ufb01cation Combines precision and re-\ncall for a balanced metric\nGatorTron obtained an F1 score of\n0.9627 for medical relation extrac-\ntion [37]\nT able 4: Benchmark Comparison of Large Language Models\nModel MMLU Score HumanEval (Coding) Release Date\nGPT -4 Turbo 86.4 85.4 April 2024\nClaude 3.5 88.7 92.0 June 2024\nLlama 3 86.1 81.7 March 2024\nGemini Ultra 83.7 74.3 December 2023\n\u2022 Virtual Health Assistants: LLMs serve as the backbone for healthcare chatbots that prov ide continuous\nhealth monitoring and medical advice [46].\n\u2022 Radiology and Imaging: Multi-modal LLMs assist radiologists by analyzing imaging data and improving\ndiagnostic precision [47].\n\u2022 Automated Report Generation: LLMs automate the generation of medical reports from diagno stic images,\nspeeding up work\ufb02ows in radiology and pathology [35].\n2.5 Performance Metrics and Model Comparisons\nBenchmarking LLM performance is crucial for assessing their effectiveness across different healthcare tasks. Com-\nmonly used benchmarks, such as MMLU (Massive Multitask Lang uage Understanding) and HumanEval, evaluate\nLLMs on various tasks, including problem-solving and code g eneration [50, 51]. T able 4 presents a comparison of\nseveral state-of-the-art models based on these benchmarks .\n3 Challenges and Future Directions\nThe incorporation of large language models (LLMs) in health care is not without obstacles. These hurdles include the\nneed for greater transparency in model decisions, ensuring data privacy and security for sensitive patient informatio n,\naddressing biases to guarantee fairness, preventing the ge neration of false or misleading outputs, and establishing\nregulatory frameworks for ethical AI use in medical context s. Overcoming these challenges is vital for fully harnessin g\nLLMs\u2019 potential to improve healthcare while maintaining et hical and legal standards.\n3.1 Improving Model T ransparency and Interpretability\nOne signi\ufb01cant challenge when applying LLMs in healthcare is their lack of interpretability. These models often\nfunction as \"black boxes,\" making it dif\ufb01cult for healthcar e providers to understand how speci\ufb01c recommendations\nor predictions are generated. This lack of clarity can hinde r adoption, as medical professionals require transparent\ndecision-making processes to ensure accuracy and trust. In healthcare, where every decision must be well-founded, the\nopaque nature of LLMs is particularly problematic. T o addre ss this, efforts are underway to develop more interpretable\nmodels that offer insight into their decision-making proce sses, fostering trust in AI-generated recommendations [52 ,\n53]. Enhancing transparency and interpretability remains a key research focus in healthcare AI [54, 55, 56].\n3.2 Data Privacy and Security Risks\nWhen applied in healthcare settings, LLMs handle vast amoun ts of sensitive information, including personally identi-\n\ufb01able data. Ensuring this data is processed and stored secur ely, in compliance with privacy regulations, is a signi\ufb01can t\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0de573cf-e894-4295-a78f-f177ffc49e81": {"__data__": {"id_": "0de573cf-e894-4295-a78f-f177ffc49e81", "embedding": null, "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d3e02ea-4951-4ea8-9eaf-e0a65b47a4e2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fa489a893cc9bcf57eb002d0c799b0263eb80b919810a5f6fc5cbe33692a7464", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nchallenge. One concern is the unintentional exposure of per sonal health information (PHI) during the training pro-\ncess, which could lead to privacy violations. Furthermore, the ability of LLMs to infer sensitive information from\nanonymized data presents additional privacy risks [57]. T o mitigate these threats, it is essential to implement robust\nanonymization techniques, secure data storage, and compli ance with ethical guidelines, ensuring that patient data\nremains protected throughout the use of LLMs in healthcare [ 58].\n3.3 Ensuring Fairness and Reducing Bias\nLLMs can inherit biases from the data they are trained on, particularly if the datasets include unequal representations\nof demographic groups or healthcare outcomes. These biases can lead to disparities in medical recommendations\nand outcomes, which can be harmful in clinical settings. Res earchers must develop strategies to identify, reduce,\nand prevent biases within these models, ensuring that LLMs c ontribute to equitable healthcare solutions. Ongoing\naudits and evaluations are critical for identifying and mit igating biases in both training data and model outputs [58].\nCollaboration between domain experts, data scientists, an d ethicists can foster the development of fair and unbiased\nAI in healthcare.\n3.4 Preventing Hallucinations in Medical AI\nLLMs sometimes generate false or misleading information\u2014commonly referred to as hallucinations\u2014which can be\nparticularly dangerous in healthcare applications where a ccuracy is critical. These models may produce plausible-\nsounding, but factually incorrect, content without provid ing traceable sources [59]. Healthcare professionals must\nbe cautious when using LLMs, validating AI-generated conte nt to avoid the risks associated with incorrect medical\nguidance. Current research is focused on addressing these h allucination challenges, with benchmarks like Med-HAL T\nbeing developed to evaluate how well models perform in medic al reasoning and information retrieval [34].\n3.5 Legal, Ethical, and Regulatory Frameworks\nThe use of LLMs in healthcare also raises signi\ufb01cant legal an d ethical questions. Issues such as the generation of sensi-\ntive or distressing medical content, or the potential for sp reading misinformation, necessitate strict regulatory ov ersight.\nFurthermore, there are concerns about plagiarism, imperso nation, and the overall integrity of LLM-generated content .\nRegulatory frameworks, such as the EU\u2019s AI Act and the U.S. HI P AA, provide essential guidelines for the safe and re-\nsponsible deployment of AI in healthcare [57, 60]. These law s ensure patient data protection and set ethical standards\nfor the use of AI technologies in sensitive environments, fo stering trust and accountability in AI-powered healthcare .\n4 Closing Remarks\nThe adoption of large language models in healthcare present s substantial opportunities for enhancing medical decisio n-\nmaking and information retrieval. These models, equipped w ith advanced capabilities, have the potential to improve\nwork\ufb02ows and patient outcomes across various healthcare ap plications. However, realizing their full potential re-\nquires overcoming key challenges such as ensuring model tra nsparency, protecting sensitive data, reducing biases, an d\npreventing erroneous outputs. As researchers and practiti oners continue to collaborate, the focus must remain on deve l-\noping ethical, trustworthy, and fair AI systems that meet th e rigorous standards of healthcare. Continued innovation,\ncombined with careful consideration of ethical and regulat ory concerns, will shape the future of LLMs in medical\npractice.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7078d875-a724-4cf0-8038-fbd380461fb8": {"__data__": {"id_": "7078d875-a724-4cf0-8038-fbd380461fb8", "embedding": null, "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5fd944c-94cc-4088-83bb-cc9d3ab7a782", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d01cd98e32a11bb6e6a3fc685fbb5df0d104b8f0baa5bcb6428a50631d14af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abed33f9-4a01-4211-bc5d-de0d3f2ac85a", "node_type": "1", "metadata": {}, "hash": "6c6a3d2da056e9e34e71748d6e6f25f827cac04d6b570ac169169ad1f972852c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\nT able 5: Overview of Challenges and Mitigation Strategies f or LLMs in Healthcare\nChallenge Impact Proposed Solution\nTransparency Lack of understanding in AI-\ngenerated decisions\nDevelop interpretable models and provide\ndecision explanations\nData Security Risk of exposing sensitive\npatient information\nUse advanced anonymization and secure\ndata storage protocols\nBias Perpetuation of unfair treat-\nment outcomes\nConduct regular bias audits and collaborate\nwith domain experts\nHallucinations Creation of inaccurate or\nmisleading content\nImplement rigorous validation and special-\nized benchmarks like Med-HAL T\nEthical and Legal\nConcerns\nRisk of misuse and data\nbreaches\nComply with regulations such as HIP AA\nand the AI Act, and ensure ethical use of\nAI\nReferences\n[1] Henglin Shi, W ei Peng, Haoyu Chen, Xin Liu, and Guoying Zh ao. Multiscale 3d-shift graph convolution network\nfor emotion recognition from human actions. IEEE Intelligent Systems , 37(4):103\u2013110, 2022.\n[2] Hao Y u, Xu Cheng, W ei Peng, W eihao Liu, and Guoying Zhao. M odality unifying network for visible-infrared\nperson re-identi\ufb01cation. In Proceedings of the IEEE/CVF International Conference on Co mputer V ision , pages\n11185\u201311195, 2023.\n[3] Y ante Li, W ei Peng, and Guoying Zhao. Micro-expression a ction unit detection with dual-view attentive\nsimilarity-preserving knowledge distillation. In 2021 16th IEEE International Conference on Automatic F ace\nand Gesture Recognition (FG 2021) , pages 01\u201308. IEEE, 2021.\n[4] Xiaopeng Hong, W ei Peng, Mehrtash Harandi, Ziheng Zhou, Matti Pietik\u00e4inen, and Guoying Zhao. Characteriz-\ning subtle facial movements via riemannian manifold. ACM T ransactions on Multimedia Computing, Communi-\ncations, and Applications (TOMM) , 15(3s):1\u201324, 2019.\n[5] Kai He, Rui Mao, Qika Lin, Y ucheng Ruan, Xiang Lan, Mengli ng Feng, and Erik Cambria. A survey of large\nlanguage models for healthcare: from data, technology, and applications to accountability and ethics. arXiv\npreprint arXiv:2310.05694 , 2023.\n[6] Y uqing W ang, Y un Zhao, and Linda Petzold. Are large langu age models ready for healthcare? a comparative\nstudy on clinical language understanding. arXiv preprint arXiv:2304.05368 , 2023.\n[7] Ping Y u, Hua Xu, Xia Hu, and Chao Deng. Leveraging generat ive ai and large language models: a comprehensive\nroadmap for healthcare integration. In Healthcare, volume 11, page 2776. MDPI, 2023.\n[8] W ei Peng, Li Feng, Guoying Zhao, and Fang Liu. Learning op timal k-space acquisition and reconstruction using\nphysics-informed neural networks. In Proceedings of the IEEE/CVF Conference on Computer V ision a nd P attern\nRecognition, pages 20794\u201320803, 2022.\n[9] W ei Peng, Ehsan Adeli, T omas Bosschieter, Sang Hyun Park , Qingyu Zhao, and Kilian M Pohl. Generating\nrealistic brain mris via a conditional diffusion probabili stic model. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention , pages 14\u201324. Springer, 2023.\n[10] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "abed33f9-4a01-4211-bc5d-de0d3f2ac85a": {"__data__": {"id_": "abed33f9-4a01-4211-bc5d-de0d3f2ac85a", "embedding": null, "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5fd944c-94cc-4088-83bb-cc9d3ab7a782", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d01cd98e32a11bb6e6a3fc685fbb5df0d104b8f0baa5bcb6428a50631d14af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7078d875-a724-4cf0-8038-fbd380461fb8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e6f2ebf98bd8f55aaab785e3d1f950f72937f096f0adbad847170ce42096a15a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, 2023.\n[10] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877\u20131901, 2020.\n[11] OpenAI. Gpt-4 technical report, 2023.\n[12] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Y u Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun\nZhang, Jung Uk Kim, Seong T ae Kim, Jinwoo Choi, et al. One smal l step for generative ai, one giant leap for\nagi: A complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488 , 2023.\n[13] Hugo T ouvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al . Llama: Open and ef\ufb01cient foundation language\nmodels. arXiv preprint arXiv:2302.13971 , 2023.\n[14] Jinhyuk Lee, W onjin Y oon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representatio n model for biomedical text mining. Bioinformatics,\n36(4):1234\u20131240, 2020.\n6", "mimetype": "text/plain", "start_char_idx": 3028, "end_char_idx": 4171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a88b5526-c818-43ab-85f3-422de7672090": {"__data__": {"id_": "a88b5526-c818-43ab-85f3-422de7672090", "embedding": null, "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a44aeda0-c79d-44d8-91ff-747169a5cbee", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2d92819a183ee8d538db7bb39b4af15e365157c8c662a35200df793549fff6ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "454b7a86-41ae-4209-8717-88ad3da2c6d2", "node_type": "1", "metadata": {}, "hash": "b01f1334c35940b22077c78eaee0560b554d07376fedf638635407170207deb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[15] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clin icalbert: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342 , 2019.\n[16] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton B akhtin, Y uxiang Wu, Alexander H Miller, and Sebastian\nRiedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 , 2019.\n[17] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya S utskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maa rten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung W on Chung, Charles Sutton, Sebastian Gehrman n, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311 , 2022.\n[19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dari o Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[20] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\n[21] William Fedus, Barret Zoph, and Noam Shazeer. Switch tr ansformers: Scaling to trillion parameter models with\nsimple and ef\ufb01cient sparsity. The Journal of Machine Learning Research , 23(1):5232\u20135270, 2022.\n[22] Nan Du, Y anping Huang, Andrew M Dai, Simon T ong, Dmitry L epikhin, Y uanzhong Xu, Maxim Krikun, Y anqi\nZhou, Adams W ei Y u, Orhan Firat, et al. Glam: Ef\ufb01cient scalin g of language models with mixture-of-experts.\nIn International Conference on Machine Learning , pages 5547\u20135569. PMLR, 2022.\n[23] Haifeng W ang, Jiwei Li, Hua Wu, Eduard Hovy, and Y u Sun. P re-trained language models and their applications.\nEngineering, 2022.\n[24] Jason W ei, Maarten Bosma, V incent Y Zhao, Kelvin Guu, Ad ams W ei Y u, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learn ers. arXiv preprint arXiv:2109.01652 , 2021.\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. B lip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning , pages\n19730\u201319742. PMLR, 2023.\n[26] Chenfei Wu, Shengming Y in, W eizhen Qi, Xiaodong W ang, Z echeng T ang, and Nan Duan. V isual chatgpt:\nT alking, drawing and editing with visual foundation models . arXiv preprint arXiv:2303.04671 , 2023.\n[27] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Ha o Shao, Dongzhi Jiang, Hongsheng Li, and Y u Liu.\nMova: Adapting mixture of vision experts to multimodal cont ext. arXiv preprint arXiv:2404.13046 , 2024.\n[28] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Rui li Huang, and W eida T ong. Ai-based language\nmodels powering drug discovery and development.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "454b7a86-41ae-4209-8717-88ad3da2c6d2": {"__data__": {"id_": "454b7a86-41ae-4209-8717-88ad3da2c6d2", "embedding": null, "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a44aeda0-c79d-44d8-91ff-747169a5cbee", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2d92819a183ee8d538db7bb39b4af15e365157c8c662a35200df793549fff6ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a88b5526-c818-43ab-85f3-422de7672090", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d229650a63533ea189dbaceeb92b874beec828abea520ba6d230ab308f5c65f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[27] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Ha o Shao, Dongzhi Jiang, Hongsheng Li, and Y u Liu.\nMova: Adapting mixture of vision experts to multimodal cont ext. arXiv preprint arXiv:2404.13046 , 2024.\n[28] Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Rui li Huang, and W eida T ong. Ai-based language\nmodels powering drug discovery and development. Drug Discovery T oday , 26(11):2593\u20132607, 2021.\n[29] T anmoy T apos Datta, Pintu Chandra Shill, and Zabir Al Na zi. Bert-d2: Drug-drug interaction extraction using\nbert. In 2022 International Conference for Advancement in T echnolo gy (ICONAT) , pages 1\u20136. IEEE, 2022.\n[30] Y anis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-A ntoine Gourraud, Mickael Rouvier, and Richard Du-\nfour. Biomistral: A collection of open-source pretrained l arge language models for medical domains. arXiv\npreprint arXiv:2402.10373 , 2024.\n[31] Karan Singhal, T ao Tu, Juraj Gottweis, Rory Sayres, Ell ery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, et al. T owards expert-le vel medical question answering with large language\nmodels. arXiv preprint arXiv:2305.09617 , 2023.\n[32] Zhengliang Liu, Y iwei Li, Peng Shu, Aoxiao Zhong, Longt ao Y ang, Chao Ju, Zihao Wu, Chong Ma, Jie\nLuo, Cheng Chen, et al. Radiology-llama2: Best-in-class la rge language model for radiology. arXiv preprint\narXiv:2309.06419 , 2023.\n[33] Zhengliang Liu, Xiaowei Y u, Lu Zhang, Zihao Wu, Chao Cao , Haixing Dai, Lin Zhao, W ei Liu, Dinggang\nShen, Quanzheng Li, et al. Deid-gpt: Zero-shot medical text de-identi\ufb01cation by gpt-4. arXiv preprint\narXiv:2303.11032 , 2023.\n[34] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sank arasubbu. Med-halt: Medical domain hallucination\ntest for large language models. arXiv preprint arXiv:2307.15343 , 2023.\n[35] Zihao Zhao, Sheng W ang, Jinchen Gu, Y itao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian W ang,\nand Dinggang Shen. Chatcad+: T owards a universal and reliab le interactive cad using llms. arXiv preprint\narXiv:2305.15964 , 2023.\n7", "mimetype": "text/plain", "start_char_idx": 2569, "end_char_idx": 4623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8de74f8-009e-4739-a494-85d42f2a5189": {"__data__": {"id_": "b8de74f8-009e-4739-a494-85d42f2a5189", "embedding": null, "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5775bed2-0724-4d6c-9f91-ad2509c057ae", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "597864202a41ebf96c58358911291ab4d501dd7bf1c97582cd068fa3d0e1965e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d18df246-18ee-4893-94fc-c276d0dbbd9d", "node_type": "1", "metadata": {}, "hash": "a8755e306e0abfafc9f5a2e061c6913448ca4ffa4f09858fd81c52a493541d33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[36] Renqian Luo, Liai Sun, Y ingce Xia, T ao Qin, Sheng Zhang, Hoifung Poon, and Tie-Y an Liu. Biogpt: generative\npre-trained transformer for biomedical text generation an d mining. Brie\ufb01ngs in Bioinformatics , 23(6):bbac409,\n2022.\n[37] Xi Y ang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Com-\npas, Cheryl Martin, Mona G Flores, Y ing Zhang, et al. Gatortr on: A large clinical language model to unlock\npatient information from unstructured electronic health r ecords. arXiv preprint arXiv:2203.03540 , 2022.\n[38] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L\nMcClelland, and Felix Hill. Language models show human-lik e content effects on reasoning. arXiv preprint\narXiv:2207.07051 , 2022.\n[39] Hongyang Li, Richard C Gerkin, Alyssa Bakke, Raquel Nor el, Guillermo Cecchi, Christophe Laudamiel,\nMasha Y Niv, Kathrin Ohla, John E Hayes, V alentina Parma, et a l. T ext-based predictions of covid-19 diag-\nnosis from self-reported chemosensory descriptions. Communications Medicine , 3(1):104, 2023.\n[40] Felix Agbavor and Hualou Liang. Predicting dementia fr om spontaneous speech using large language models.\nPLOS Digital Health , 1(12):e0000168, 2022.\n[41] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. Bo osting transformers and language models for clinical\nprediction in immunotherapy. arXiv preprint arXiv:2302.12692 , 2023.\n[42] Stephen R Ali, Thomas D Dobbs, Hayley A Hutchings, and Ia in S Whitaker. Using chatgpt to write patient clinic\nletters. The Lancet Digital Health , 5(4):e179\u2013e181, 2023.\n[43] Rubeta N Matin, Eleni Linos, and Neil Rajan. Leveraging large language models in dermatology, 2023.\n[44] Malik Sallam. The utility of chatgpt as an example of lar ge language models in healthcare education, research\nand practice: Systematic review on the future perspectives and potential limitations. medRxiv, pages 2023\u201302,\n2023.\n[45] G\u00f6k\u00e7e Uludo \u02d8gan, Elif Ozkirimli, Kutlu O Ulgen, Nilg\u00fcn Karal\u0131, and Arzuc an \u00d6zg\u00fcr. Exploiting pretrained\nbiochemical language models for targeted drug design. Bioinformatics, 38(Supplement_2):ii155\u2013ii161, 2022.\n[46] Desir\u00e9e Bill and Theodor Eriksson. Fine-tuning a llm us ing reinforcement learning from human feedback for a\ntherapy chatbot application, 2023.\n[47] Lei Ma, Jincong Han, Zhaoxin W ang, and Dian Zhang. Cephg pt-4: An interactive multimodal cephalometric\nmeasurement and diagnostic system with visual large langua ge model. arXiv preprint arXiv:2307.07518 , 2023.\n[48] Guangyu W ang, Guoxing Y ang, Zongxin Du, Longjun Fan, an d Xiaohu Li. Clinicalgpt: Large language models\n\ufb01netuned with diverse medical data and comprehensive evalu ation. arXiv preprint arXiv:2306.09968 , 2023.\n[49] Jianquan Li, Xidong W ang, Xiangbo Wu, Zhiyi Zhang, Xiao long Xu, Jie Fu, Prayag Tiwari, Xiang W an, and\nBenyou W ang. Huatuo-26m, a large-scale chinese medical qa d ataset. arXiv preprint arXiv:2305.01526 , 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d18df246-18ee-4893-94fc-c276d0dbbd9d": {"__data__": {"id_": "d18df246-18ee-4893-94fc-c276d0dbbd9d", "embedding": null, "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5775bed2-0724-4d6c-9f91-ad2509c057ae", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "597864202a41ebf96c58358911291ab4d501dd7bf1c97582cd068fa3d0e1965e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8de74f8-009e-4739-a494-85d42f2a5189", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2f4d66f8b9e7304368dc74e495b59dd1b9c707e859d7c62f6092b2774ba469d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:2306.09968 , 2023.\n[49] Jianquan Li, Xidong W ang, Xiangbo Wu, Zhiyi Zhang, Xiao long Xu, Jie Fu, Prayag Tiwari, Xiang W an, and\nBenyou W ang. Huatuo-26m, a large-scale chinese medical qa d ataset. arXiv preprint arXiv:2305.01526 , 2023.\n[50] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, M antas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.\n[51] Mark Chen, Jerry T worek, Heewoo Jun, Qiming Y uan, Henri que Ponde De Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Y uri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 , 2021.\n[52] Hazrat Ali, Junaid Qadir, T anvir Alam, Mowafa Househ, a nd Zubair Shah. Chatgpt and large language models\n(llms) in healthcare: Opportunities and risks. 2023.\n[53] Sandeep Reddy. Evaluating large language models for us e in healthcare: A framework for translational value\nassessment. Informatics in Medicine Unlocked , page 101304, 2023.\n[54] Giovanni Briganti. A clinician\u2019s guide to large langua ge models. Future Medicine AI , (0):FMAI, 2023.\n[55] Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaa r, Boris Delibasic, Pietro Lio, and Andrija Petrovic.\nInterpretable medical diagnostics with structured data ex traction by large language models. arXiv preprint\narXiv:2306.05052 , 2023.\n[56] Y an Jiang, Ruihong Qiu, Y i Zhang, and Peng-Fei Zhang. Ba lanced and explainable social media analysis for\npublic health with large language models. arXiv preprint arXiv:2309.05951 , 2023.\n[57] Jesutofunmi A Omiye, Haiwen Gui, Shawheen J Rezaei, Jam es Zou, and Roxana Daneshjou. Large language\nmodels in medicine: the potentials and pitfalls. arXiv preprint arXiv:2309.00087 , 2023.\n[58] Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, b ard, and large language models for biomedical research:\nOpportunities and pitfalls. Annals of Biomedical Engineering , pages 1\u20135, 2023.\n8", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 4794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3ba15767-1938-47c3-8631-73e0a5a74ef1": {"__data__": {"id_": "3ba15767-1938-47c3-8631-73e0a5a74ef1", "embedding": null, "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "191f31e5-8703-4458-bda5-52a255fa0aa8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bdc0bd2fb6675e97778e68b5cd33928a991e03207b2294c07d434db39d20181d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Role of Language Models in Modern Healthcare: A Comprehe nsive Review\n[59] Shubo Tian, Qiao Jin, Lana Y eganova, Po-Ting Lai, Qingq ing Zhu, Xiuying Chen, Y ifan Y ang, Qingyu Chen,\nW on Kim, Donald C Comeau, et al. Opportunities and challenge s for chatgpt and large language models in\nbiomedicine and health. arXiv preprint arXiv:2306.10070 , 2023.\n[60] Claudio Novelli, Federico Casolari, Philipp Hacker, G iorgio Spedicato, and Luciano Floridi. Generative ai in eu\nlaw: liability, privacy, intellectual property, and cyber security. arXiv preprint arXiv:2401.07348 , 2024.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6bbf9615-df32-449f-b330-f05004702450": {"__data__": {"id_": "6bbf9615-df32-449f-b330-f05004702450", "embedding": null, "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de6de8cd-c1b8-43b0-a5c3-3ea94b57675f", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "19d45ee42a544cd56be83f60cb4d198ccb1f0d21cb0f70b80e57eb8925f0d40e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b29da23c-eb0f-47b0-9482-adbf688d83f5": {"__data__": {"id_": "b29da23c-eb0f-47b0-9482-adbf688d83f5", "embedding": null, "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19f073ff-bbee-479b-b397-9ab75a642722", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "22f2fa0e566ecce1b71e1d9c1e623ddef5bf6e657f78b2f68639b53545e03daf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41e50cb1-adba-4455-8b04-249f67cedc5f": {"__data__": {"id_": "41e50cb1-adba-4455-8b04-249f67cedc5f", "embedding": null, "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3dfdfbbe-c85c-4b1e-8b23-2428f4a0c24d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "57c0ce6fd4f492a668ef2e58a74a49ee94c5f86c3881f982b1c48ff266e6e7b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bd9e15ab-3178-4524-be90-d16b1d00628e": {"__data__": {"id_": "bd9e15ab-3178-4524-be90-d16b1d00628e", "embedding": null, "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c871c3f-c01e-470c-b2ba-db433ac6bf80", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d1ecab826e3cc304682bad5f49b438a247e7de7e50db3bd7202a6ef63f30e361", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"models_parameter_years_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9c419638-9c0d-4b3d-98c6-97dab3f1946d": {"__data__": {"id_": "9c419638-9c0d-4b3d-98c6-97dab3f1946d", "embedding": null, "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59bd979f-508f-4fd6-a92f-6351c5c02320", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "77a30f6dd69e402a1d108eb352415c1794a574f643b2544a83940568bb7ff5c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v3.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2179cbe4-62de-463d-b8ba-fb7797e46856": {"__data__": {"id_": "2179cbe4-62de-463d-b8ba-fb7797e46856", "embedding": null, "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90aede91-8f1c-4d72-af2c-99ec4b6e6073", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "48d668e32c7e62162e51f76b6bdf2dd67fb966a5aad1d1e79df325c2e9ddca92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_application_v3_old.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "75e2d352-2b99-417a-a45c-ae8d64493cf2": {"__data__": {"id_": "75e2d352-2b99-417a-a45c-ae8d64493cf2", "embedding": null, "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a24f7f7-192d-4f31-bb42-fea17c558007", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "6af54316744150a7425f2bd8ad895ff9c9fc927cc7f0c00322c097144a6ebe7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_challenges.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "056c79a2-c025-41fa-93a5-d10925dea0d5": {"__data__": {"id_": "056c79a2-c025-41fa-93a5-d10925dea0d5", "embedding": null, "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f84244f5-bd27-4b1d-85c5-7d08aeb152a9", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "936bb7e65df14bdbb68ea1256b7c3aca1c9278070f38626f1bb8fa8c476534cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"llm_challenges_v2.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6e16f9a0-1ca4-4645-8be7-f855a96f7098": {"__data__": {"id_": "6e16f9a0-1ca4-4645-8be7-f855a96f7098", "embedding": null, "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c04d424-1174-42a0-b670-159795dc5edc", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "cfaf0cd5fc93cc72e43f4828c63382bb4c9ec2fc431d76def5689bf2a856592c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"medllm_comp.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d834629b-9224-4491-9e1c-bcdb4d2d31d3": {"__data__": {"id_": "d834629b-9224-4491-9e1c-bcdb4d2d31d3", "embedding": null, "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eec73f3c-8c2a-46c8-9c4c-163ea47a42da", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "b74476f80c7a5b60755b5f49920698f2e946b75633b1c5eeae4a9627ca855da7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cfbf8a9e-d300-42e0-86cd-907d0d600a26": {"__data__": {"id_": "cfbf8a9e-d300-42e0-86cd-907d0d600a26", "embedding": null, "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a187bc3e-a0ee-48b8-8ac6-4b870d598867", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "95e538c631d6b86b84ae381aecbedf608467fca832d0b61184925861faa74835", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"model_size_v3.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e930d448-cce1-4628-9adb-d0a327a6814f": {"__data__": {"id_": "e930d448-cce1-4628-9adb-d0a327a6814f", "embedding": null, "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "170dadfc-4114-4ef7-99d9-c0b70ad30742", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "39dcab11fd47d87ddb16a3b5f96c5f79b40718ce1bc3007f06dc7b05796696bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This figure \"multimodal.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.16860v1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "712198e4-720c-4f74-8f58-483d2c32eed5": {"__data__": {"id_": "712198e4-720c-4f74-8f58-483d2c32eed5", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d488e6fa-23af-4e1e-b57e-a76ae707ba17", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f1d0d195eda002b6ad581bc25912bf5f7e7540c425e89d640ff1b804be51b520", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. J Biomed Inform. 2025 Apr 16:104819. doi: 10.1016/j.jbi.2025.104819. Online \r\nahead of print.\r\n\r\nRoBIn: A Transformer-based model for risk of bias inference with machine reading \r\ncomprehension.\r\n\r\nDias AC(1), Moreira VP(1), Comba JLD(2).\r\n\r\nAuthor information:\r\n(1)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, \r\nPorto Alegre, 91501-970, Rio Grande do Sul, Brazil.\r\n(2)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, \r\nPorto Alegre, 91501-970, Rio Grande do Sul, Brazil. Electronic address: \r\njoao.comba@gmail.com.\r\n\r\nOBJECTIVE: Scientific publications are essential for uncovering insights, \r\ntesting new drugs, and informing healthcare policies. Evaluating the quality of \r\nthese publications often involves assessing their Risk of Bias (RoB), a task \r\ntraditionally performed by human reviewers. The goal of this work is to create a \r\ndataset and develop models that allow automated RoB assessment in clinical \r\ntrials.\r\nMETHODS: We use data from the Cochrane Database of Systematic Reviews (CDSR) as \r\nground truth to label open-access clinical trial publications from PubMed. This \r\nprocess enabled us to develop training and test datasets specifically for \r\nmachine reading comprehension and RoB inference. Additionally, we created \r\nextractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to \r\nextract relevant evidence and classify the RoB effectively.\r\nRESULTS: RoBIn was evaluated across various settings and benchmarked against \r\nstate-of-the-art methods, including large language models (LLMs). In most cases, \r\nthe best-performing RoBIn variant surpasses traditional machine learning and \r\nLLM-based approaches, achieving a AUROC of 0.83.\r\nCONCLUSION: This work addresses RoB assessment in clinical trials by introducing \r\nRoBIn, two Transformer-based models for RoB inference and evidence retrieval, \r\nwhich outperform traditional models and LLMs, demonstrating its potential to \r\nimprove efficiency and scalability in clinical research evaluation. We also \r\nintroduce a public dataset that is automatically annotated and can be used to \r\nenable future research to enhance automated RoB assessment.\r\n\r\nCopyright \u00a9 2025. Published by Elsevier Inc.\r\n\r\nDOI: 10.1016/j.jbi.2025.104819\r\nPMID: 40250743\r\n\r\nConflict of interest statement: Declaration of competing interest The authors \r\ndeclare that they have no known competing financial interests or personal \r\nrelationships that could have appeared to influence the work reported in this \r\npaper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64f5d340-5b50-4d2f-804a-e7ce56414ab8": {"__data__": {"id_": "64f5d340-5b50-4d2f-804a-e7ce56414ab8", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de7cc68f-5f27-42a3-ba6d-baf1f0b5fa6d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bed46a9b302fb775305509cebf7b12d2626f308449adab58d428dfdeb6e9d0a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Br J Oral Maxillofac Surg. 2025 Mar 24:S0266-4356(25)00059-2. doi: \r\n10.1016/j.bjoms.2025.03.006. Online ahead of print.\r\n\r\nThe impact of the large language model ChatGPT in oral and maxillofacial \r\nsurgery: a systematic review.\r\n\r\nde Menezes Torres LM(1), de Morais EF(2), Fernandes Almeida DRM(3), Pagotto \r\nLEC(4), de Santana Santos T(5).\r\n\r\nAuthor information:\r\n(1)Department of Oral and Maxillofacial Surgery, Universidade de Pernambuco, \r\nRecife, PE, Brazil.\r\n(2)Department of Oral Diagnosis, and Graduate Program in Oral Biology, \r\nPiracicaba Dental School, University of Campinas, Piracicaba, S\u00e3o Paulo, Brazil. \r\nElectronic address: evertonfreitas2@hotmail.com.\r\n(3)School of Dentistry, Federal University of Alfenas (Unifal-MG), Alfenas, \r\nMinas Gerais, Brazil.\r\n(4)Oral and Maxillofacial Surgeon, Hospital S\u00edrio-Libanes, S\u00e3o Paulo, Brazil.\r\n(5)Institute Maxillofacial Education, Aracaju, Sergipe, Brazil.\r\n\r\nThis systematic review evaluates the impact of the large language model (LLM) \r\nChatGPT in oral and maxillofacial surgery. Following PRISMA guidelines and \r\nregistered in PROSPERO (CRD42024625882), the study involved a comprehensive \r\nsearch across PubMed/Medline, Embase, Scopus, and Science Direct. Inclusion \r\ncriteria focused on ChatGPT's use in clinical decision-making, surgical \r\nplanning, patient education, and research. Ten studies were reviewed, assessing \r\nChatGPT's performance in diagnostic accuracy, surgical efficiency, and patient \r\nsatisfaction. GPT-4 achieved the highest accuracy (76.8%) in multiple-choice \r\nquestions but showed variability, with lower performance in pharmacology and \r\ncomplex clinical scenarios. It excelled in generating informed consent \r\ndocuments, outperforming other AI models and human residents in accuracy, \r\ncompleteness, and readability. It also provided accurate and supportive \r\nresponses in postoperative follow up. However, limitations were noted in \r\nhandling complex clinical decisions and providing personalised advice for cases \r\nsuch as oral cancer and orthognathic surgery. While ChatGPT shows potential in \r\nimproving patient communication, reducing healthcare workloads, and providing \r\nup-to-date information, its current limitations in accuracy and personalisation \r\nunderscore the need for human supervision and integration with clinical \r\ndatabases. AI tools like ChatGPT can complement, but should not replace, human \r\njudgment in specialised fields such as oral and maxillofacial surgery.\r\n\r\nCopyright \u00a9 2025 The British Association of Oral and Maxillofacial Surgeons. \r\nPublished by Elsevier Ltd. All rights reserved.\r\n\r\nDOI: 10.1016/j.bjoms.2025.03.006\r\nPMID: 40251084", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "09e51e98-b374-42cf-b6f5-4f8a3e65b80d": {"__data__": {"id_": "09e51e98-b374-42cf-b6f5-4f8a3e65b80d", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f40dc871-08ac-414d-aab6-7d6564623b84", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "d56ef5574295784b966d431186670fbb64cd7922e29b7bcd981ab87505c907b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Adv Simul (Lond). 2025 Apr 18;10(1):22. doi: 10.1186/s41077-025-00350-6.\r\n\r\nArtificial intelligence-assisted academic writing: recommendations for ethical \r\nuse.\r\n\r\nCheng A(1), Calhoun A(2), Reedy G(3).\r\n\r\nAuthor information:\r\n(1)Departments of Pediatrics and Emergency Medicine, Alberta Children's \r\nHospital, Cumming School of Medicine, University of Calgary, 28 Oki Drive NW, \r\nCalgary, Alberta, T3B 6A8, Canada. chenger@me.com.\r\n(2)University of Louisville School of Medicine and Norton Children's Medical \r\nGroup, Louisville, KY, USA.\r\n(3)Faculty of Life Sciences and Medicine, King's College London, London, UK.\r\n\r\nGenerative artificial intelligence (AI) tools have been selectively adopted \r\nacross the academic community to help researchers complete tasks in a more \r\nefficient manner. The widespread release of the Chat Generative Pre-trained \r\nTransformer (ChatGPT) platform in 2022 has made these tools more accessible to \r\nscholars around the world. Despite their tremendous potential, studies have \r\nuncovered that large language model (LLM)-based generative AI tools have issues \r\nwith plagiarism, AI hallucinations, and inaccurate or fabricated references. \r\nThis raises legitimate concern about the utility, accuracy, and integrity of AI \r\nwhen used to write academic manuscripts. Currently, there is little clear \r\nguidance for healthcare simulation scholars outlining the ways that generative \r\nAI could be used to legitimately support the production of academic literature. \r\nIn this paper, we discuss how widely available, LLM-powered generative AI tools \r\n(e.g. ChatGPT) can help in the academic writing process. We first explore how \r\nacademic publishers are positioning the use of generative AI tools and then \r\ndescribe potential issues with using these tools in the academic writing \r\nprocess. Finally, we discuss three categories of specific ways generative AI \r\ntools can be used in an ethically sound manner and offer four key principles \r\nthat can help guide researchers to produce high-quality research outputs with \r\nthe highest of academic integrity.\r\n\r\n\u00a9 2025. The Author(s).\r\n\r\nDOI: 10.1186/s41077-025-00350-6\r\nPMCID: PMC12007126\r\nPMID: 40251634\r\n\r\nConflict of interest statement: Declarations. Ethics approval and consent to \r\nparticipate: Not applicable. Consent for publication: Not Applicable. Competing \r\ninterests: Adam Cheng is an editorial board member and a former Associate Editor \r\nof the journal Simulation in Healthcare. Aaron Calhoun is Associate \r\nEditor-in-Chief and an editorial board member of the journal Simulation in \r\nHealthcare. Gabriel Reedy is Editor-in-Chief and an editorial board member of \r\nthe journal Advances in Simulation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "97dcd730-bee1-4952-bf94-32617faeed85": {"__data__": {"id_": "97dcd730-bee1-4952-bf94-32617faeed85", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d44a3620-294e-4edc-ab54-ea79a41cfcc0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "2eeeeb855dba89648403d8f65585aaaecc1259d112ef85d6caaf3b916e56e370", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. SLAS Technol. 2025 Apr 17:100295. doi: 10.1016/j.slast.2025.100295. Online\r\nahead  of print.\r\n\r\nNLP for Computational Insights into Nutritional Impacts on Colorectal Cancer \r\nCare.\r\n\r\nGong S(1), Jin X(2), Guo Y(3), Yu J(4).\r\n\r\nAuthor information:\r\n(1)The Affiliated Hospital of Nantong University, Nantong University, Nantong, \r\nJiangsu, 226001, China. Electronic address: gsn6574172@163.com.\r\n(2)The Affiliated Hospital of Nantong University, Nantong, Jiangsu, 226001, \r\nChina. Electronic address: 1554319715@qq.com.\r\n(3)Nantong University, Nantong, Jiangsu, 226001, China. Electronic address: \r\njyg@ntu.edu.cn.\r\n(4)Nantong University, Nantong, Jiangsu, 226001, China. Electronic address: \r\n1014147829@qq.com.\r\n\r\nColorectal cancer (CRC) is one of the most prominent cancers globally, with its \r\nincidence rising among younger adults due to improved screening practices. \r\nHowever, existing algorithms for CRC prediction are frequently trained on \r\ndatasets that primarily reflect older persons, thus limiting their usefulness in \r\nmore diverse populations. Additionally, the part of nutrition in CRC deterrence \r\nand management is gaining significant attention, although computational \r\napproaches to analyzing the impact of diet on CRC remain underdeveloped. This \r\nresearch introduces the Nutritional Impact on CRC Prediction Framework \r\n(NICRP-Framework), which combines Natural Language Processing (NLP) techniques \r\nwith Adaptive Tunicate Swarm Optimized Large Language Models (ATSO-LLMs) to \r\npresent important insights into the part of the diet in CRC care across diverse \r\npopulations. The colorectal cancer dietary and lifestyle dataset, encompassing \r\nmore than 1000 participants, is collected from multiple regions and sources. The \r\ndataset includes structured and unstructured data, including textual \r\ndescriptions of food ingredients. These descriptions are processed using \r\nstandardization techniques, such as stop word removal, lowercasing, and \r\npunctuation elimination. Relevant terms are then extracted and visualized in a \r\nword cloud. The dataset also contained an imbalanced binary CRC outcome, which \r\nis rebalanced utilizing the random oversampling. ATSO-LLMs are employed to \r\nanalyze the processed dietary data, identifying key nutritional factors and \r\nforecasting CRC and non-CRC phenotypes based on dietary patterns. The results \r\nshow that combining NLP-derived features with ATSO-LLMs significantly enhances \r\nprediction accuracy (98.4%), sensitivity (97.6%) specificity (96.9%) and \r\nF1-Score (96.2%), with minimal misclassification rates. This framework \r\nrepresents a transformative advancement in life science by offering a new, \r\ndata-driven approach to understanding the nutritional determinants of CRC, \r\nempowering healthcare professionals to make more precise predictions and adapted \r\ndietary interventions for diverse populations.\r\n\r\nCopyright \u00a9 2025. Published by Elsevier Inc.\r\n\r\nDOI: 10.1016/j.slast.2025.100295\r\nPMID: 40252975\r\n\r\nConflict of interest statement: Declaration of competing interest The authors \r\ndeclare that they have no known competing financial interests or personal \r\nrelationships that could have appeared to influence the work reported in this \r\npaper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dfa667d2-49e2-4956-b915-40320ed44a35": {"__data__": {"id_": "dfa667d2-49e2-4956-b915-40320ed44a35", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22ac7c1a-0471-474f-9596-d1163ce43b2e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "282c8e59fa837ca5feeed92a5f0a02ee0202298bd69347e167131534d336a495", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Rheumatol Adv Pract. 2024 Sep 18;9(2):rkae119. doi: 10.1093/rap/rkae119. \r\neCollection 2025.\r\n\r\nLarge language models and rheumatology: are we there yet?\r\n\r\nBenavent D(1)(2), Madrid-Garc\u00eda A(3).\r\n\r\nAuthor information:\r\n(1)Rheumatology Department, Hospital Universitari de Bellvitge, Barcelona, \r\nSpain.\r\n(2)Medical Department, Savana Research SL, Madrid, Spain.\r\n(3)Grupo de Patolog\u00eda Musculoesquel\u00e9tica, Hospital Cl\u00ednico San Carlos, Instituto \r\nde Investigaci\u00f3n Sanitaria San Carlos (IdISSC), Madrid, Spain.\r\n\r\nThe last 2 years have marked the beginning of a golden age for natural language \r\nprocessing in medicine. The arrival of large language models (LLMs) and \r\nmultimodal models have raised new opportunities and challenges for research and \r\nclinical practice. In rheumatology, a specialty rich in data and requiring \r\ncomplex decision-making, the use of these tools may transform diagnostic \r\nprocedures, improve patient interaction and simplify data management, leading to \r\nmore personalized and efficient healthcare outcomes. The objective of this \r\narticle is to present an overview of the status of LLMs in the field of \r\nrheumatology while discussing some of the challenges ahead in this area of great \r\npotential.\r\n\r\n\u00a9 The Author(s) 2025. Published by Oxford University Press on behalf of the \r\nBritish Society for Rheumatology.\r\n\r\nDOI: 10.1093/rap/rkae119\r\nPMCID: PMC12007598\r\nPMID: 40256630", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e02463e2-03de-47b2-b9af-da270498626b": {"__data__": {"id_": "e02463e2-03de-47b2-b9af-da270498626b", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272658.txt", "file_name": "pubmed_40272658.txt", "file_type": "text/plain", "file_size": 3810, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d70e8673-2d5c-4f71-831a-8dc443321596", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272658.txt", "file_name": "pubmed_40272658.txt", "file_type": "text/plain", "file_size": 3810, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "3bf3c8ced4386d136a363a78828fe6728a1ad2827bded7ba6343363cb68c924f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Int J Comput Assist Radiol Surg. 2025 Apr 24. doi: 10.1007/s11548-025-03359-4.\r\n Online ahead of print.\r\n\r\nperfDSA: Automatic Perfusion Imaging in Cerebral Digital Subtraction \r\nAngiography.\r\n\r\nSu R(1)(2), van der Sluijs PM(3), Marc FG(4), Te Nijenhuis F(3), Cornelissen \r\nSAP(3), Roozenbeek B(3), van Zwam WH(5), van der Lugt A(3), Ruijters D(6), Pluim \r\nJ(4), van Walsum T(3).\r\n\r\nAuthor information:\r\n(1)Department of Biomedical Engineering, Eindhoven University of Technology, \r\nEindhoven, The Netherlands. r.su@tue.nl.\r\n(2)Department of Radiology & Nuclear Medicine, Erasmus MC, University Medical \r\nCenter Rotterdam, Rotterdam, The Netherlands. r.su@tue.nl.\r\n(3)Department of Radiology & Nuclear Medicine, Erasmus MC, University Medical \r\nCenter Rotterdam, Rotterdam, The Netherlands.\r\n(4)Department of Biomedical Engineering, Eindhoven University of Technology, \r\nEindhoven, The Netherlands.\r\n(5)Department of Radiology & Nuclear Medicine, Maastricht University Medical \r\nCenter, Maastricht, The Netherlands.\r\n(6)Interventional X-Ray (iXR), Philips Healthcare, Best, The Netherlands.\r\n\r\nPURPOSE: Cerebral digital subtraction angiography (DSA) is a standard imaging \r\ntechnique in image-guided interventions for visualizing cerebral blood flow and \r\ntherapeutic guidance thanks to its high spatio-temporal resolution. To date, \r\ncerebral perfusion characteristics in DSA are primarily assessed visually by \r\ninterventionists, which is time-consuming, error-prone, and subjective. To \r\nfacilitate fast and reproducible assessment of cerebral perfusion, this work \r\naims to develop and validate a fully automatic and quantitative framework for \r\nperfusion DSA.\r\nMETHODS: We put forward a framework, perfDSA, that automatically generates \r\ndeconvolution-based perfusion parametric images from cerebral DSA. It \r\nautomatically extracts the arterial input function from the supraclinoid \r\ninternal carotid artery (ICA) and computes deconvolution-based perfusion \r\nparametric images including cerebral blood volume (CBV), cerebral blood flow \r\n(CBF), mean transit time (MTT), and Tmax.\r\nRESULTS: On a DSA dataset with 1006 patients from the multicenter MR CLEAN \r\nregistry, the proposed perfDSA achieves a Dice of 0.73(\u00b10.21) in segmenting the \r\nsupraclinoid ICA, resulting in high accuracy of arterial input function (AIF) \r\ncurves similar to manual extraction. Moreover, some extracted perfusion images \r\nshow statistically significant associations (P=2.62e -\u00a0 5) with favorable \r\nfunctional outcomes in stroke patients.\r\nCONCLUSION: The proposed perfDSA framework promises to aid therapeutic \r\ndecision-making in cerebrovascular interventions and facilitate discoveries of \r\nnovel quantitative biomarkers in clinical practice. The code is available at \r\nhttps://github.com/RuishengSu/perfDSA .\r\n\r\n\u00a9 2025. The Author(s).\r\n\r\nDOI: 10.1007/s11548-025-03359-4\r\nPMID: 40272658\r\n\r\nConflict of interest statement: Declarations. Conflict of interest: Wim H. van \r\nZwam received speaker fees from Philips, Nicolab, Stryker, Penumbra, Medtronic, \r\nMicrovention DSMB for WeTrust (Philips) and ATHENA (Anaconda), all paid to the \r\ninstitution. Aad van der Lugt received research grants from Siemens \r\nHealthineers, GE Healthcare, and Philips Healthcare, all paid to the \r\ninstitution. Danny Ruijters is an employee of Philips. Theo van Walsum received \r\nresearch grants from Philips Healthcare, paid to the institution. The rest of \r\nthe authors have no competing financial or nonfinancial interests to disclose \r\nbeyond the funding sources listed above. Ethics approval: The MR CLEAN Registry \r\nwas approved by the ethics committee of the Erasmus University MC, Rotterdam, \r\nThe Netherlands (MEC-2014-235). The need for individual patient consent has been \r\nwaived. Patient consent for publication: Not applicable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27ceef58-8ed2-4100-8cff-e6473adbc9c9": {"__data__": {"id_": "27ceef58-8ed2-4100-8cff-e6473adbc9c9", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "971f5035-f12e-4e2a-a610-7c84ae471b94", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c5b8739fd78af630c8a64526b788cdc4e6f4cb30bd4104de734ebdb21db461f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40bc5295-5805-4797-bd3c-1e61d1c39dd6", "node_type": "1", "metadata": {}, "hash": "baa796469c67764ccb5137ed7c1a654e3a776aedf1793ca795ba00c796bb80c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Dis Colon Rectum. 2025 Mar 4. doi: 10.1097/DCR.0000000000003677. Online ahead\r\nof  print.\r\n\r\nVisceral Fat Quantified by a Fully Automated Deep-Learning Algorithm and Risk of \r\nIncident and Recurrent Diverticulitis.\r\n\r\nHa J(1)(2), Bridge CP(3)(4), Andriole KP(3)(5), Kambadakone A(4), Clark \r\nMJ(1)(2), Narimiti A(1)(2), Rosenthal MH(5)(6), Fintelmann FJ(4), Gollub \r\nRL(4)(7), Giovannucci EL(8), Strate LL(9), Ma W(1)(2), Chan \r\nAT(1)(2)(10)(11)(12).\r\n\r\nAuthor information:\r\n(1)Clinical and Translational Epidemiology Unit, Massachusetts General Hospital \r\nand Harvard Medical School, Boston, Massachusetts.\r\n(2)Division of Gastroenterology, Department of Medicine, Massachusetts General \r\nHospital and Harvard Medical School, Boston, Massachusetts.\r\n(3)Data Science Office, Mass General Brigham, Boston, Massachusetts.\r\n(4)Department of Radiology, Massachusetts General Hospital and Harvard Medical \r\nSchool, Boston, Massachusetts.\r\n(5)Department of Radiology, Brigham and Women's Hospital and Harvard Medical \r\nSchool, Boston, Massachusetts.\r\n(6)Department of Imaging, Dana-Farber Cancer Institute, Boston, Massachusetts.\r\n(7)Department of Psychiatry, Massachusetts General Hospital and Harvard Medical \r\nSchool, Boston, Massachusetts.\r\n(8)Departments of Epidemiology and Nutrition, Harvard T.H. Chan School of Public \r\nHealth, Boston, Massachusetts.\r\n(9)Division of Gastroenterology, University of Washington School of Medicine, \r\nSeattle, Washington.\r\n(10)Channing Division of Network Medicine, Brigham and Women's Hospital and \r\nHarvard Medical School, Boston, Massachusetts.\r\n(11)Broad Institute of MIT and Harvard, Cambridge, Massachusetts.\r\n(12)Department of Immunology and Infectious Diseases, Harvard T.H. Chan School \r\nof Public Health, Boston, Massachusetts.\r\n\r\nBACKGROUND: Obesity is a risk factor for diverticulitis. However, it remains \r\nunclear whether visceral fat area, a more precise measurement of abdominal fat, \r\nis associated with the risk of diverticulitis.\r\nOBJECTIVE: To estimate the risk of incident and recurrent diverticulitis \r\naccording to visceral fat area.\r\nDESIGN: A retrospective cohort study.\r\nSETTINGS: The Mass General Brigham Biobank.\r\nPATIENTS: 6,654 patients who underwent abdominal CT for clinical indications and \r\nhad no diagnosis of diverticulitis, inflammatory bowel disease, or cancer before \r\nthe scan.\r\nMAIN OUTCOME MEASURES: Visceral fat area, subcutaneous fat area, and skeletal \r\nmuscle area were quantified using a deep-learning model applied to abdominal CT. \r\nThe main exposures were z-scores of body composition metrics, normalized by age, \r\nsex, and race. Diverticulitis cases were defined with the ICD codes for the \r\nprimary or admitting diagnosis from the electronic health records. The risks of \r\nincident diverticulitis, complicated diverticulitis, and recurrent \r\ndiverticulitis requiring hospitalization according to quartiles of body \r\ncomposition metrics z-scores were estimated.\r\nRESULTS: A higher visceral fat area z-score was associated with an increased \r\nrisk of incident diverticulitis (multivariable HR comparing the highest versus \r\nlowest quartile, 2.09; 95% CI, 1.48-2.95; P for trend <.0001), complicated \r\ndiverticulitis (HR, 2.56; 95% CI, 1.10-5.99; P for trend = .02), and recurrence \r\nrequiring hospitalization (HR, 2.76; 95% CI, 1.15-6.62; P for trend = .03). The \r\nassociation between visceral fat area and diverticulitis was not materially \r\ndifferent among different strata of body mass index. Subcutaneous fat area and \r\nskeletal muscle area were not significantly associated with diverticulitis.\r\nLIMITATIONS: The study population was limited to individuals who underwent CT \r\nscans for medical indication.\r\nCONCLUSION: Higher visceral fat area derived from CT was associated with \r\nincident and recurrent diverticulitis. Our findings provide insight into the \r\nunderlying pathophysiology of diverticulitis and may have implications for \r\npreventive strategies. See Video Abstract.\r\n\r\nCopyright \u00a9 The ASCRS 2025.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "40bc5295-5805-4797-bd3c-1e61d1c39dd6": {"__data__": {"id_": "40bc5295-5805-4797-bd3c-1e61d1c39dd6", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "971f5035-f12e-4e2a-a610-7c84ae471b94", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "c5b8739fd78af630c8a64526b788cdc4e6f4cb30bd4104de734ebdb21db461f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27ceef58-8ed2-4100-8cff-e6473adbc9c9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}, "hash": "bc0dd38e388ac4d317cf3179011c532848904e6268bd7b4c8ac86c4aadc5bcb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Subcutaneous fat area and \r\nskeletal muscle area were not significantly associated with diverticulitis.\r\nLIMITATIONS: The study population was limited to individuals who underwent CT \r\nscans for medical indication.\r\nCONCLUSION: Higher visceral fat area derived from CT was associated with \r\nincident and recurrent diverticulitis. Our findings provide insight into the \r\nunderlying pathophysiology of diverticulitis and may have implications for \r\npreventive strategies. See Video Abstract.\r\n\r\nCopyright \u00a9 The ASCRS 2025.\r\n\r\nDOI: 10.1097/DCR.0000000000003677\r\nPMID: 40272787\r\n\r\nConflict of interest statement: Financial Disclosures: Dr. Kambadakone served as \r\na consultant for Bayer and received grants from Bayer, GE Healthcare, and \r\nPanCAN. Dr. Fintelmann is a speaker for Boston Scientific and received grants \r\nfrom Pfizer and the William M. Wood Foundation. Dr. Chan served as a consultant \r\nfor Pfizer Inc., and Boehringer Ingelheim and received grants from Pfizer Inc, \r\nZoe Ltd, and Freenome. Dr. Rosenthal served as a consultant for Merck. The other \r\nauthors have no conflict of interest to declare.", "mimetype": "text/plain", "start_char_idx": 3482, "end_char_idx": 4592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af7ff672-6ab4-4a6d-9684-dfffa5fca5be": {"__data__": {"id_": "af7ff672-6ab4-4a6d-9684-dfffa5fca5be", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f23ac8f5-f0dc-4ee9-843b-7013db1a9663", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f52808e91b4adbc5290a810899400c51d1182902fe1a9751709b23fc84aa58e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0046ba3d-ab8f-498f-9821-b3d1928c7b92", "node_type": "1", "metadata": {}, "hash": "dcf9985fef9140ed1a49e574049e819e0be574e35a65924a800e60a0525d9079", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Slide #0: \nPr\u00e9sentation de projet\nPr\u00e9senter par: Abdoul Fataou Hama et Mahamat Yaya Hissein\nTh\u00e8me: NLP-IA-AIA  \n\n\nSlide #1: \n\n Image: an alarm clock on a computer screen\n\nINTRODUCTION\nL'essor de l'intelligence artificielle et des grands mod\u00e8les de langage (LLM) offre des opportunit\u00e9s in\u00e9dites pour la recherche scientifique et le management d'entreprise. Pourtant, l'acc\u00e8s \u00e0 l'information pertinente et la gestion efficace des connaissances restent des d\u00e9fis majeurs pour les chercheurs, les doctorants, les ing\u00e9nieurs et les managers.\nPourquoi un Assistant IA NLP ?\nChercheurs et doctorants : Difficult\u00e9s \u00e0 trouver et analyser rapidement les travaux pertinents.\nIng\u00e9nieurs : Recherche d'informations techniques et support \u00e0 la r\u00e9daction de documentation.\nEntreprises : Besoin d'un syst\u00e8me de veille strat\u00e9gique et d'aide \u00e0 la prise de d\u00e9cision.\n\n\nSlide #2: \nProbl\u00e9matique et Solution propos\u00e9es\nD\u00e9fis actuels\n\nVolume massif de donn\u00e9es acad\u00e9miques et techniques.\nDifficult\u00e9s \u00e0 extraire des informations pr\u00e9cises et pertinentes.\nTemps n\u00e9cessaire pour analyser et synth\u00e9tiser des documents.\n\n\nSolutions propos\u00e9es\n\nR\u00e9sumer des articles acad\u00e9miques et documents techniques.\nAssister \u00e0 la r\u00e9daction et \u00e0 la reformulation de textes.\nAutomatiser la veille scientifique et technologique.\nOptimiser l'analyse de documents en entreprise.\n\n\n\nSlide #3: \n\n Image: a man holding a video game controller in his hand\n\nFonctionnalit\u00e9s Cl\u00e9s de l'Assistant IA\n\n1\nAnalyse et Recherche Documentaire\nRecherche avanc\u00e9e NLP, exploration s\u00e9mantique, analyse des tendances scientifiques.\n\n2\nR\u00e9sum\u00e9 et Extraction d'Informations\nG\u00e9n\u00e9ration de r\u00e9sum\u00e9s, extraction de citations et r\u00e9f\u00e9rences cl\u00e9s, cr\u00e9ation de fiches de lecture intelligentes.\n\n3\nAssistance \u00e0 la R\u00e9daction\nG\u00e9n\u00e9ration automatique d'introductions, correction grammaticale et am\u00e9lioration du style scientifique.\n\n\nSlide #4: \nSolutions existantes d\u2019assistant IA\nChatGPT (OpenAI) : G\u00e9n\u00e9ration de texte, assistance \u00e0 la r\u00e9daction, r\u00e9sum\u00e9s.\nDeepSeek (DeepSeek AI) : Sp\u00e9cialis\u00e9 dans l'analyse de documents techniques et scientifiques.\nGoogle Gemini : Recherche et traitement avanc\u00e9 d\u2019informations avec int\u00e9gration multimodale.\nMicrosoft Copilot : Assistance int\u00e9gr\u00e9e aux outils Microsoft pour l\u2019optimisation du travail.\n\n\n\nSlide #5: \nComparaison des Assistants IA Existants\u000b\n\n\nSlide #6: \nArchitecture et Technologies de l'IA\nMod\u00e8les NLP et LLM\nDeepSeek, Mistral, LLaMA, GPT, Claude pour l\u2019analyse et la synth\u00e8se. Sentence Transformers, BERT pour la recherche s\u00e9mantique. Spacy, NLTK pour l\u2019analyse linguistique et la tokenization.\nBackend et API\nFastAPI / Flask pour le backend. LangChain pour l\u2019orchestration des mod\u00e8les IA. ElasticSearch / FAISS pour l\u2019indexation et la recherche vectorielle.\n\n\nSlide #7: \n\n Image: a shelf filled with different types of toys\n\nTaxinomie du Projet\n\nExploration et Recherche Documentaire\nMoteur de recherche NLP, filtrage par sujet, analyse des tendances scientifiques.\n\nR\u00e9sum\u00e9 et Extraction d\u2019Informations\nR\u00e9sum\u00e9 automatique, analyse des citations et r\u00e9f\u00e9rences, synth\u00e8se par th\u00e9matique.\n\nAssistance \u00e0 la R\u00e9daction\nG\u00e9n\u00e9ration de contenu scientifique, reformulation et simplification, correction grammaticale.\n\n\nSlide #8: \n\n Image: people sitting at a table with laptops\n\nCas d\u2019Usage Concrets de l'Assistant IA\n\n Image: a black and white photo of a black and white airplane\n\nDoctorant\nUtilise l\u2019IA pour r\u00e9sumer rapidement plusieurs articles de recherche sur son sujet.\n\n Image: a black and white photo of a street sign\n\nChercheur en R&D\nExplore de nouvelles publications et g\u00e9n\u00e8re des revues de litt\u00e9rature automatis\u00e9es.\n\n Image: a black and white photo of a black and white sign\n\nAnalyste en entreprise\nExtrait des tendances strat\u00e9giques \u00e0 partir de milliers de documents de march\u00e9.", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 3746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0046ba3d-ab8f-498f-9821-b3d1928c7b92": {"__data__": {"id_": "0046ba3d-ab8f-498f-9821-b3d1928c7b92", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f23ac8f5-f0dc-4ee9-843b-7013db1a9663", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "f52808e91b4adbc5290a810899400c51d1182902fe1a9751709b23fc84aa58e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af7ff672-6ab4-4a6d-9684-dfffa5fca5be", "node_type": "1", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "653471baac42af8909ab7dff1a267195a5037491a6285857597d9786a902e94e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cas d\u2019Usage Concrets de l'Assistant IA\n\n Image: a black and white photo of a black and white airplane\n\nDoctorant\nUtilise l\u2019IA pour r\u00e9sumer rapidement plusieurs articles de recherche sur son sujet.\n\n Image: a black and white photo of a street sign\n\nChercheur en R&D\nExplore de nouvelles publications et g\u00e9n\u00e8re des revues de litt\u00e9rature automatis\u00e9es.\n\n Image: a black and white photo of a black and white sign\n\nAnalyste en entreprise\nExtrait des tendances strat\u00e9giques \u00e0 partir de milliers de documents de march\u00e9.\n\n\nSlide #9: \nB\u00e9n\u00e9fices attendus\nGain de temps et am\u00e9lioration de la productivit\u00e9.\nAcc\u00e8s facilit\u00e9 \u00e0 des informations pertinentes.\nMeilleure prise de d\u00e9cision pour les chercheurs et les entreprises.\nPerspectives d'\u00e9volution\nInt\u00e9gration d'une synth\u00e8se vocale pour une interaction plus naturelle. \n Fine-tuning sur des corpus sp\u00e9cifiques pour am\u00e9liorer la pr\u00e9cision. \n Extension vers des modules de recommandation personnalis\u00e9s.\n\nCONCLUSION", "mimetype": "text/plain", "start_char_idx": 3235, "end_char_idx": 4183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a82ca2e3-34f6-4756-bee9-e6d6a0cd3bb6": {"__data__": {"id_": "a82ca2e3-34f6-4756-bee9-e6d6a0cd3bb6", "embedding": null, "metadata": {"page_label": "1", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5eb950a6-42f9-4bae-a201-0517e84a621d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "42696f4bf5e68da9412bf087148c4c4543173fe41fec2c4a7ebf62efbd031282", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pipeline RAG Complet \nCe document pr\u00e9sente une vue d'ensemble du pipeline RAG (Retrieval-\nAugmented Generation) avanc\u00e9, con\u00e7u pour am\u00e9liorer l'acc\u00e8s \u00e0 l'information\net la g\u00e9n\u00e9ration de contenu dans le domaine de la recherche scientifique.\nNous explorerons chaque \u00e9tape, de la collecte des donn\u00e9es \u00e0 l'optimisation\ncontinue, en mettant l'accent sur les techniques et les technologies cl\u00e9s\npour chaque phase.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dbafb3ed-b5fd-48de-b6e5-0eeb67a0210d": {"__data__": {"id_": "dbafb3ed-b5fd-48de-b6e5-0eeb67a0210d", "embedding": null, "metadata": {"page_label": "2", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e88584d-da10-4fcf-a713-1351658c2553", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "75024353e3e2b61d2a5ab4939fc063b27af2f35b165339200d6632174e46073b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Collecte et Ingestion des Donn\u00e9es\nIdentification des Sources\nBases de donn\u00e9es scientifiques (PubMed, ArXiv, IEEE, etc.)\nDocuments internes\nArticles de recherche\nExtraction des Donn\u00e9es\nR\u00e9cup\u00e9ration en texte brut ou formats structur\u00e9s (PDF, XML,\nJSON)\nPr\u00e9traitement : nettoyage du texte\nSuppression des m\u00e9tadonn\u00e9es inutiles, correction des\nerreurs OCR\nTokenization et segmentation : division en passages\ncoh\u00e9rents (paragraphes, sections)\nLa premi\u00e8re \u00e9tape cruciale est la collecte et l'ingestion des donn\u00e9es. Nous identifions des sources vari\u00e9es, allant des bases de\ndonn\u00e9es scientifiques renomm\u00e9es aux documents internes. L'extraction des donn\u00e9es est suivie d'un pr\u00e9traitement rigoureux pour\ngarantir la qualit\u00e9 du texte.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "585216c4-3a9b-4764-8210-098419010237": {"__data__": {"id_": "585216c4-3a9b-4764-8210-098419010237", "embedding": null, "metadata": {"page_label": "3", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6577e924-2e07-4990-a65d-10b412405651", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ab7bd9ec33541056f0bb68cb254f06470b6332de62cc299f45234e61b0f15267", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indexation et Stockage des Donn\u00e9es\nCr\u00e9ation d\u2019un Index S\u00e9mantique\nUtilisation d'un moteur de recherche\nvectoriel (FAISS, Weaviate,\nElasticsearch, etc.).\nG\u00e9n\u00e9ration d\u2019Embeddings\nTransformation des textes en\nrepr\u00e9sentations num\u00e9riques avec des\nmod\u00e8les comme OpenAI, Mistral ou\nDeepSeek.\nStockage Hybride\nCombinaison d\u2019un index vectoriel pour la similarit\u00e9 s\u00e9mantique et d\u2019un stockage\nSQL/NoSQL pour la recherche classique.\nL'indexation et le stockage des donn\u00e9es sont essentiels pour une r\u00e9cup\u00e9ration efficace.\nNous cr\u00e9ons un index s\u00e9mantique \u00e0 l'aide de moteurs de recherche vectoriels et\ng\u00e9n\u00e9rons des embeddings avec des mod\u00e8les avanc\u00e9s. Un stockage hybride est mis en\nplace pour combiner la similarit\u00e9 s\u00e9mantique et la recherche classique.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1277b973-1bf5-40b3-83f6-483744ab302d": {"__data__": {"id_": "1277b973-1bf5-40b3-83f6-483744ab302d", "embedding": null, "metadata": {"page_label": "4", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "350aa6fc-67a4-4a4e-94ce-9e76e70901cc", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "e70c6f58736fe699e9a0b21772092d8a62e75b04e29b7f1a204ecf9eb1f0052e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "M\u00e9canisme de Recherche et\nR\u00e9cup\u00e9ration\n1 Compr\u00e9hension des Requ\u00eates\nNLP avanc\u00e9 (reformulation, expansion).\n2 Recherche Hybride\nRecherche s\u00e9mantique : proximit\u00e9 vectorielle\nRecherche lexicale : requ\u00eates classiques (TF-IDF, BM25)\n3 Filtrage et Pond\u00e9ration\nAm\u00e9liorer la pertinence des r\u00e9sultats.\nLe m\u00e9canisme de recherche et de r\u00e9cup\u00e9ration est au c\u0153ur du pipeline RAG. Nous utilisons\nle NLP avanc\u00e9 pour comprendre les requ\u00eates des utilisateurs et combinons la recherche\ns\u00e9mantique et lexicale pour des r\u00e9sultats optimaux. Le filtrage et la pond\u00e9ration des\nr\u00e9sultats am\u00e9liorent la pertinence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e0e599c-8f25-4a3c-b91d-7bf4be37aed4": {"__data__": {"id_": "8e0e599c-8f25-4a3c-b91d-7bf4be37aed4", "embedding": null, "metadata": {"page_label": "5", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0a38699-3a9f-4959-acac-a10f6cd3eedc", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fd6c905990e91a38be2dac26e211cc6d2a574bf6c5d0a7d8f5284afc17abcc15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Enrichissement et Traitement\ndes Donn\u00e9es\nFusion des R\u00e9sultats\nMulti-sources (publications,\nbases priv\u00e9es).\nClassification et\nRegroupement\nTh\u00e9matique des documents.\nExtraction des \u00c9l\u00e9ments Cl\u00e9s\nCitations, figures, tableaux, m\u00e9thodologies.\nL'enrichissement et le traitement des donn\u00e9es r\u00e9cup\u00e9r\u00e9es sont cruciaux\npour fournir des informations compl\u00e8tes. Nous fusionnons les r\u00e9sultats de\nmultiples sources, classifions et regroupons th\u00e9matiquement les\ndocuments, et extrayons les \u00e9l\u00e9ments cl\u00e9s tels que les citations et les\nm\u00e9thodologies. Ce processus enrichit la qualit\u00e9 des donn\u00e9es.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eed2712a-a74f-4863-97d5-f5387e927b28": {"__data__": {"id_": "eed2712a-a74f-4863-97d5-f5387e927b28", "embedding": null, "metadata": {"page_label": "6", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5170a1a-24c4-4478-89ad-4341a0108c6e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "83cc9f3bf638917fad7eae24132fe5fd27478b6ca28aa7c9864fd9df09b172f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "G\u00e9n\u00e9ration Augment\u00e9e par\nR\u00e9cup\u00e9ration (RAG)\nStructuration des Donn\u00e9es\nPr\u00e9paration avant g\u00e9n\u00e9ration.\nG\u00e9n\u00e9ration de Contenu\nR\u00e9sum\u00e9 automatique d\u2019articles et brevets\nExplication et vulgarisation des concepts techniques\nSynth\u00e8se multi-document bas\u00e9e sur plusieurs sources\nR\u00e9duction des Biais et Validation\nMod\u00e8les de contr\u00f4le qualit\u00e9.\nLa g\u00e9n\u00e9ration augment\u00e9e par r\u00e9cup\u00e9ration (RAG) est l'\u00e9tape o\u00f9 nous g\u00e9n\u00e9rons du contenu\nassist\u00e9 par des LLM. Nous structurons les donn\u00e9es r\u00e9cup\u00e9r\u00e9es avant la g\u00e9n\u00e9ration et\nutilisons des mod\u00e8les de contr\u00f4le qualit\u00e9 pour r\u00e9duire les biais et valider la g\u00e9n\u00e9ration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d9e623c1-9ff4-4d8c-8dcc-75fcedab04d6": {"__data__": {"id_": "d9e623c1-9ff4-4d8c-8dcc-75fcedab04d6", "embedding": null, "metadata": {"page_label": "7", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d982ed5-25b9-4858-b7b3-c7f493969896", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "37303712c7812efda4a11c191145f46344d18c3c94808d4bea7d3866f80f9407", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Post-Traitement et\nAm\u00e9lioration du Style\nCorrection grammaticale,\nreformulation.\n1\nAdaptation au Public\nChercheurs, entreprises, ing\u00e9nieurs.\n2\nFiches de Lecture Interactives\nSurlignage des points cl\u00e9s.\n3\nLe post-traitement et la personnalisation sont essentiels pour adapter le contenu g\u00e9n\u00e9r\u00e9 aux besoins sp\u00e9cifiques des utilisateurs.\nNous am\u00e9liorons le style scientifique et technique, adaptons le contenu au public cible (par exemple, chercheurs, entreprises,\ning\u00e9nieurs), et g\u00e9n\u00e9rons des fiches de lecture interactives.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ff21d60b-a167-43b8-8e27-d38ec1832bc0": {"__data__": {"id_": "ff21d60b-a167-43b8-8e27-d38ec1832bc0", "embedding": null, "metadata": {"page_label": "8", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64eac5fd-e635-478d-b227-b7938c8a3ab9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "4c5c955ef4a91b395da4748c9ace84378a417c51ceea940947a32888f36e548b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interfaces et Int\u00e9grations\nAPI\nInt\u00e9gration avec des outils de gestion documentaire (Zotero, Mendeley,\nObsidian).\nInterfaces Utilisateur\nChatbot, recherche avanc\u00e9e, tableau de bord analytique.\nSupport Multilingue\nAnalyse et g\u00e9n\u00e9ration de contenu en plusieurs langues.\nLes interfaces et les int\u00e9grations facilitent l'acc\u00e8s au pipeline RAG. Nous\noffrons une API pour l'int\u00e9gration avec des outils de gestion documentaire,\ndes interfaces utilisateur intuitives (comme un chatbot et une recherche\navanc\u00e9e), et un support multilingue pour l'analyse et la g\u00e9n\u00e9ration de\ncontenu en plusieurs langues.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d77e3e3f-7779-4458-a076-82cee750a74c": {"__data__": {"id_": "d77e3e3f-7779-4458-a076-82cee750a74c", "embedding": null, "metadata": {"page_label": "9", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e087112-5746-4a71-a607-644442d7ef26", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "fe746dc76411b90635b2cea41527c707eae47e294fed040e773e5e58c701185e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Optimisation et Am\u00e9lioration Continue\n10%\nGain de Pr\u00e9cision\nFine-tuning des mod\u00e8les.\n24/7\nSuivi des Performances\nFeedback utilisateur et r\u00e9entra\u00eenement\np\u00e9riodique.\nRGPD\nS\u00e9curit\u00e9 et Conformit\u00e9\nAnonymisation des donn\u00e9es sensibles.\nL'optimisation et l'am\u00e9lioration continue sont essentielles pour maintenir la performance du\npipeline RAG. Nous effectuons un fine-tuning des mod\u00e8les pour am\u00e9liorer la pr\u00e9cision,\nsuivons les performances avec le feedback utilisateur, et assurons la s\u00e9curit\u00e9 et la\nconformit\u00e9 avec le RGPD en anonymisant les donn\u00e9es sensibles.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ca1636da-1fba-4f40-b60c-a8b7c053fa37": {"__data__": {"id_": "ca1636da-1fba-4f40-b60c-a8b7c053fa37", "embedding": null, "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pptx", "file_name": "Pipeline RAG Complet.pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 5663846, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b77ddef4-24d2-4599-8a26-3ceb39d934be", "node_type": "4", "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pptx", "file_name": "Pipeline RAG Complet.pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 5663846, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "bb608fa4321d8c9e0f006624c4c0570d0b0262c0319a41b4665ac056f77713a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Slide #0: \nPresente  par:\nMahamat Yaya Hissein\nAbdoul Fataou \n\n Image: a black and white photo of a sign\n\n\n\nSlide #1: \n\n\n\n\nSlide #2: \n\n\nSlide #3: \n\n\n\nSlide #4: \n\n\n\nSlide #5: \n\n\n\n\nSlide #6: \n\n\n\n\n\n\nSlide #7: \n\n\n\n\n\nSlide #8:", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e357dbe1-1712-466f-a7ba-868095de8c61": {"__data__": {"id_": "e357dbe1-1712-466f-a7ba-868095de8c61", "embedding": null, "metadata": {"page_label": "1", "file_name": "Your paragraph text.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Your paragraph text.pdf", "file_type": "application/pdf", "file_size": 1394120, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47f78103-6b65-4f99-98de-a6c968bc23ab", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Your paragraph text.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Your paragraph text.pdf", "file_type": "application/pdf", "file_size": 1394120, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}, "hash": "ed8aeac9a6d7f68d4b9876cfcac226c3621eadd01ce5f16d662f02ba0e51f1b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\ufee3\ufea4\ufee4\ufeaa  \ufecb\ufe92\ufeaa\u0627\ufedf\ufeae\ufea3\ufef4\ufee2  \u0623\ufeb3\ufeae\u0629  \u0625\ufedf\ufef2  \ufecb\ufe9c\ufee4\ufe8e\u0646  \ufeeb\ufe8e\u0631\u0648\u0646  \u0623\ufeb3\ufeae\u0629  \ufee3\ufee6\n\ufecb\ufe92\ufeaa\u0648  \ufecb\ufe9c\ufee4\ufe8e\u0646  \ufeb3\ufe8e\u062f\ufef3\ufe94  \u0627\ufedf\ufe92\ufee8\ufe96  \u0623\u0645    #  \ufecb\ufe92\ufeaa\u0627\ufedf\ufedc\ufeae\ufef3\ufee2  \u0627\u0631\ufe9f\ufeee\u0646  \u0648\u0627\ufedf\ufeaa  \u0623\u0645 \n\u2640 \u0000  \ufecb\ufe92\ufeaa\u0627\ufedf\ufeae\ufea3\ufef4\ufee2  \ufecb\ufe8e\ufe8b\ufeb8\ufe94  \u0648\u0627\ufedf\ufecc\ufeae\u0648\ufeb3\ufe94   \u2642 \u0000  \ufeeb\ufe8e\u0631\u0648\u0646  \ufea3\ufeb4\ufee6  \u0627\ufedf\ufecc\ufeae\ufef3\ufeb2\n\u2740  \ufed3\ufe8e\ufe97\ufea4\ufe94 \ufea3\ufeb4\ufee6 \ufeeb\ufe8e\u0631\u0648\u0646 \ufed3\ufef2 \ufee3\ufe8e\u0631\u0648\u0643 \u0648 \ufecb\ufe8e\ufe8b\ufeb8\ufe94 \ufecb\ufe92\ufeaa\u0627\ufedf\ufeae\ufea3\ufef4\ufee2 \u2740\n*   \ufedb\ufe8e\ufeb7\ufe8e  \u0648\u062f\u0627\u064a*  \n\ufedb\ufe8e\ufeb7\ufe8e\u0648\ufef3\ufe94  \ufe91\ufee8\ufef2  \ufedf\ufef2  \u0627\ufedf\ufedc\ufe8e\ufeb7\ufe8e\u0648\u064a  \u0648\u062f\u0627\u064a  \u0648\ufedf\ufeaa  \ufee3\ufee6 \n\u0623\ufea7\ufeee\u0627\u0646 \u0627\ufedf\ufecc\ufeae\ufef3\ufeb2  #\n\ufee3\ufea4\ufee4\ufeaa \ufeeb\ufe8e\u0631\u0648\u0646\n\ufea3\ufeb4\ufef4\ufee6 \ufeeb\ufe8e\u0631\u0648\u0646\n\u0623\ufea7\ufeee\u0627\u062a \u0627\ufedf\ufecc\ufeae\u0648\ufeb3\ufe94  #\n\ufeb3\ufeee\u062f\u0629 \ufecb\ufe92\ufeaa \u0627\ufedf\ufeae\ufea3\ufef4\ufee2\n\u0631\u0648\ufee3\ufef4\ufeb4\ufe8e \ufecb\ufe92\ufeaa \u0627\ufedf\ufeae\ufea3\ufef4\ufee2", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"44aae20f-64f6-4313-9d4e-1007f36541d5": {"doc_hash": "f1b3638b44da9c1f15bd0bd60dadef90f3d81797f2b7f19605f73f1b5e1328bd", "ref_doc_id": "1cf0bc84-4f19-48ed-8fb1-6928bc429853"}, "7ea269a8-7360-4d2d-ad75-8f0dd75aca58": {"doc_hash": "07f6c4d28b2dd319e4b631e8f55f5d193cc8e2260eb8b49dfd5c39f12ffc8e41", "ref_doc_id": "fc49187f-8138-4140-b291-bab8c53111c7"}, "a05de66a-a4cd-46c9-925a-f8c71f4d87d0": {"doc_hash": "4c505d793126468e433582a9f8f944db5060dcc7b5900624dc420a0fc890cdf4", "ref_doc_id": "fc49187f-8138-4140-b291-bab8c53111c7"}, "2b7f3b72-f1a5-49dc-968c-4732c031ec90": {"doc_hash": "9cb38311827537a180cfc8aa7a3546062256744ac6125b7e9c44abb6c53d616f", "ref_doc_id": "cad8c60e-57da-4b63-9b61-54b93306b7b8"}, "62ed9e17-9110-429f-b071-a875b0b5b23f": {"doc_hash": "bc0c201292e663e631b1a045f530e0b43d4ddcf14c9d825ef0a0c7488ba649dd", "ref_doc_id": "cad8c60e-57da-4b63-9b61-54b93306b7b8"}, "dadec924-b3e5-4ac9-9d9d-08ce26e51c32": {"doc_hash": "e11cc431edbca232630ad7ba219e1f3e296f3b893bbf617bb4807b2c92e83098", "ref_doc_id": "6c36ef3b-ec00-4991-8993-fc0fa6c9af08"}, "431846f2-a241-491f-a310-f87428bd313f": {"doc_hash": "f879908933a99f4309c36131c5e6e1c048153b637ea041648a94ed86def82aa1", "ref_doc_id": "6c36ef3b-ec00-4991-8993-fc0fa6c9af08"}, "302ddf55-dfde-4527-a4bd-236e2de9f884": {"doc_hash": "0909fc0ca685d50bc8702aaf8b601205beddfa4929a87e7fb9ece4afab80ce16", "ref_doc_id": "3c35c144-0e93-4fbd-b142-36072f24c9b9"}, "3b73e086-deb0-4ec8-8561-72207e630872": {"doc_hash": "73394b46338afddca12a9a0d29889629a66bf4f274045915d477dedb5ae02243", "ref_doc_id": "3c35c144-0e93-4fbd-b142-36072f24c9b9"}, "f7c0e0a0-0161-4430-85c8-30fd55ad4a9e": {"doc_hash": "b8ec506b93a9f56fa8803fd2223e9b8537d7ad24c807fbbfa3f3b5dc2a9b1a2e", "ref_doc_id": "7e0304c1-0d9e-4aa1-afd6-24ecdc69460e"}, "5bbc9c4f-4cbc-45b5-92c2-d115c20a7b65": {"doc_hash": "29f782fe487fae09d78c68af621c798f708bf559ce634e77647449067fe26c07", "ref_doc_id": "2c450a3e-aa63-423d-a940-7237b75165ec"}, "d36f84bd-d341-4972-8494-e520da286a0d": {"doc_hash": "55c533911a7759a98838b0f5d8086fa3e9ed77a0827ab793fda9f66f5732c0cc", "ref_doc_id": "2c450a3e-aa63-423d-a940-7237b75165ec"}, "ea8279ec-6956-463b-8eca-e65f45f30e80": {"doc_hash": "780d50347a831494073e2f113b67660accab332aebb854ee1e2cac23135f6eb0", "ref_doc_id": "ae00076f-441a-4c9b-a5e1-de6e5841889f"}, "4d4f40c4-bf31-4496-986d-aa1649cbf507": {"doc_hash": "031f7539146afa4c2f7c759c95825d69293eebfaaabb9d4a42c8065c703c87aa", "ref_doc_id": "3864b547-5f35-47c3-bd66-9140804d6dd3"}, "7abb6a27-e84b-411a-ac37-d3b2a0feb79d": {"doc_hash": "803dbc83f43ace1160254207ee72728c291174feee30ae2801ef981b733cdf2f", "ref_doc_id": "8b8c8275-cb6c-426b-b691-7d7c3b606aff"}, "9eb6d914-1d45-4049-b338-557573b25ef9": {"doc_hash": "a118e536debe076623639a5ccb1e309a1476c0b18059dca285604aeff751449f", "ref_doc_id": "73733ab9-eb31-41c1-9e8a-ec7a760ba395"}, "1e74bb63-bcd4-4b31-9caf-7e39e0556847": {"doc_hash": "fa3160820b018f36194da86db232d90339406df141c466dbb297026d10ea217f", "ref_doc_id": "aef93de9-8bc6-4d31-8d67-7ab14ba49674"}, "17a668bd-752d-48ab-a8fa-8973d6de5e8e": {"doc_hash": "b1dd6ae4f24e556fa021a05ff20c5d7927ba7c43b236abf809b38b7799bb40b2", "ref_doc_id": "aca900af-3be4-4d8a-afd6-e68003af2128"}, "e12e51ca-83b9-4596-aca8-38344ae5111f": {"doc_hash": "5782489931369e2fb28901e684fc62be10717ba3666cf33f8eca05d544524fd8", "ref_doc_id": "b99a776c-cc1f-4e87-8916-3ac9330f9d24"}, "f218c722-6185-4fbc-b1c4-fc5c944eee5b": {"doc_hash": "ae3529a2de24972c0aa567051c894414ded0d4bd6fa5b5b2b286f08bad2b0621", "ref_doc_id": "4a2addc3-6c7b-4685-b0cb-6a31df257332"}, "f5521f12-f02b-4334-9b01-f472d8ddae7e": {"doc_hash": "fb70d564765a32dbb45b385f33eab47644bc1caaa9813d87d1a3bca0e1d3f1b6", "ref_doc_id": "c3d27641-f4da-41a8-b37a-671561b9682c"}, "a82f34de-5905-47d8-8fb6-6d3b8b023df2": {"doc_hash": "1082e1016aca9424116fc990083fd67e15e277a0fcc274d9fd6a26635324b419", "ref_doc_id": "51e8b4de-84b5-4c2c-959f-2a25de83158d"}, "e340042d-bcb0-4d71-ab9d-0d1d2c9a0bca": {"doc_hash": "5698e65020071fb61169c0eaf3b4274cff305d6f85986fd35c9123ebd3ca8433", "ref_doc_id": "6bff5288-dec1-4e62-a90a-cb3d551b2957"}, "ed321e9b-0c17-4887-8cbc-08e80694c702": {"doc_hash": "986fbb90290b7035fb1d62954e22c2cbd9a1ad26e37b783b833e3599e0037736", "ref_doc_id": "e3cc3918-a8b2-48ac-91e1-52a8f7e11e59"}, "09ceeb29-1dbe-488b-bdc3-f1056a023a6b": {"doc_hash": "7b00d1e3506c5c2c8012681eb112c7d76a27ca00b13c893525e6f5839ad77637", "ref_doc_id": "8dfca318-5a22-4272-bc12-332d28ce54c9"}, "62a1f5dc-7e95-4c1f-9507-c55080f8a1e6": {"doc_hash": "c7d57401c9d5a6956ad2ecec081253af377b64e9e580aa6c766514d34ce65147", "ref_doc_id": "ce7c14e5-0e9d-46d3-8773-1260cb331065"}, "42c32953-702a-4fad-8b80-3a2b86038006": {"doc_hash": "912c1bf572432026414c1800aeb82fc144b6c99f2114d83c126f9010749c653e", "ref_doc_id": "2aacb4cf-4b39-49eb-ac3e-eb423aebc7d9"}, "1e024b56-1659-437a-9700-11cd9b23ff28": {"doc_hash": "4abc81d01662e1f2bbe0cecb76c3b9d79db5dc22b89f6e94750f92c2558e15bd", "ref_doc_id": "a808c423-24ef-4e78-978c-3c8102cb821b"}, "eb6b518b-90ad-45c7-9e3a-0e4998f99022": {"doc_hash": "11a2ccc1bfb9a3c37476f0d3186131b013ee2b76968a04acbf360c97551012d4", "ref_doc_id": "a9e72ea7-21ee-4a1c-a710-a1b9ddfb7674"}, "e1f2fedc-c5fb-4b60-8a69-edf0ce4ba64a": {"doc_hash": "a867742f59be9a1918ae1d92fc4948b6b5fc01b08ce372914dc4c0d47f85454c", "ref_doc_id": "ed344fa9-1b7b-4c6c-b38f-dad78e4591c8"}, "5be40d05-4b33-4e39-a03e-b19688f5d676": {"doc_hash": "e1353b3d7b7ba90cd01be0c5884af5bf86a59f5f17d7cca8a817dd86ce88395d", "ref_doc_id": "ed344fa9-1b7b-4c6c-b38f-dad78e4591c8"}, "2e548b25-ab47-4732-b6d5-92fe237cf2ba": {"doc_hash": "c340d437105cacb53afa736b60cf000f34ca10f3962261e5e01fb7c0707da52b", "ref_doc_id": "99792d17-8ba7-4669-a571-8039beeaae9a"}, "c8cf56dc-da54-4541-89ee-b66233b0c7e1": {"doc_hash": "b26185e076872f1c4a910bc4a81f41c64470410c2963ec87ef2730cc22087371", "ref_doc_id": "ee0485e9-1d43-409c-b790-6169c05ec167"}, "307fa96c-69a4-41e8-beaa-7bfb4cc5f8b8": {"doc_hash": "1cc575c312cc510859893952c3556a0747ffcbdad8a931c471b4a73f1fc63049", "ref_doc_id": "f037292a-85ff-4105-a24b-7455076cea27"}, "6131f19e-cb58-4718-81e4-09698ab9451a": {"doc_hash": "56ce99e8f7cd5e4ec3d7027199123412f8e6ac52a287ccdeae4395a5ad196b6e", "ref_doc_id": "400f475d-5691-4a67-841b-36688db2a4cf"}, "16043eea-1e0f-4d5f-b52b-67ac1a5e8bae": {"doc_hash": "82d56906ffeb07002f334c07ebc367a170b854b0dcb6acb35c9d5817b43581c0", "ref_doc_id": "400f475d-5691-4a67-841b-36688db2a4cf"}, "2620c189-b8b4-4344-a53c-6c9e0328f3b2": {"doc_hash": "0ac4e75504e565d3aad35c7867ddb38cc17cc155be80e65dbcf69b43a625ab9e", "ref_doc_id": "090eca36-29ef-45d9-bf03-ebd78d053609"}, "8453c056-4770-4d6d-a882-862d42c07e47": {"doc_hash": "7635f6104b494edeab1fb9b520ced577891db06e70fd9df9dc9e78c641d3345a", "ref_doc_id": "2eaf6042-758a-4e20-a01c-b126bb27f90b"}, "8d058450-2a2a-4f44-8132-c2bbe5358952": {"doc_hash": "17a922b873b4a741b070008aa017d5fccab80d9c126538ecd91f2c273f344f67", "ref_doc_id": "8b0caec0-65cd-47cb-b812-7deec3927471"}, "a993290a-7898-40f6-b477-0bf30940c1d2": {"doc_hash": "495d8a0e61a8218ba779edd32ea9e41d1680799b30e7618d4a1d4a1aa4eff545", "ref_doc_id": "eedb5d77-eda4-476e-9b3e-e34a862ca41a"}, "9f80178a-f532-4fb2-8bc2-00e1e7cec020": {"doc_hash": "4a5bb0bfcbbbe11fe1ab760ffe8148ab6ef7d831b42e0692ecae13fb15056e67", "ref_doc_id": "a1487321-7801-47a3-8b62-ae34823edc14"}, "7727578f-6869-4fd6-a34a-9d90a2cfca7b": {"doc_hash": "2ca7be9f36b80dedca57f1ae459359584e745f3bd79c6d6530ab1d56dff54068", "ref_doc_id": "fef6a84b-7e1a-4a15-bb77-483ff59ffef5"}, "225cd80f-bd6f-4343-ba5e-c7332f70ab5e": {"doc_hash": "b7e01a68f13e583e61d30e9439127b138dbc5d3d9f1d8f6a4b7aab097e35aca6", "ref_doc_id": "2bad9372-40d9-4b3b-b3f3-f7a8d3793fd2"}, "8578e875-b982-4b96-855a-bfd22cab2e01": {"doc_hash": "dfc49e09503b8d4eaa85f2fb707aab056cc0377826c1f04ab9d06ae893394a61", "ref_doc_id": "1057d7cc-e7e9-4637-992b-69f7f53d80d6"}, "83daae95-38dc-4b8a-a0e9-d5bf1ee59992": {"doc_hash": "0245ff212a5717e00c5159e67bba58c4efabfe01758ccfa5b0b79a648216b91b", "ref_doc_id": "6f7e90e0-2fa3-4da9-90bc-c07182ed2114"}, "e0b0d568-7e95-43ab-9d5b-8f5d0bc8fee5": {"doc_hash": "4aa431bd9fcff934b1a2b97da4c036cb3a0529642dba553604226d519ed389d1", "ref_doc_id": "2691b756-3e5a-4d3c-9200-ffabb02b378a"}, "bc909015-b43a-4271-acdd-d3c0ca16999e": {"doc_hash": "95d68c3925eab0c0747629d09bdfb09a55362b5c18b5591be209dbdf0180df11", "ref_doc_id": "39ade76d-4273-444c-ba32-232ba7a8c3a9"}, "8547566c-ae08-4ca5-bae2-7ebc3eeb1787": {"doc_hash": "84aeba2864a7ab8dbff329ab463f186457311673d7025d8a3a8ce2c5d699cf54", "ref_doc_id": "6b00766c-1d53-4d5b-9a03-87fa07cbaa5d"}, "ccfa5162-5f0e-4cb3-b860-41124cf6187b": {"doc_hash": "f48e23078a3aac6e832d4d2338664d61d8573d8f032f03c8914d4cb9c6bc8fe5", "ref_doc_id": "a7367521-4b50-4ed8-812e-af774415413f"}, "b083478b-45c6-41ae-aff4-fa4f4f98616f": {"doc_hash": "80bfa0cb1a9355e11bfd066ae4b465e4a5c47188af366a7788927dccfe19fbe9", "ref_doc_id": "6088c380-cb3e-4237-8a4a-9a5ec66b22e0"}, "b3857fe9-6a8f-4890-a920-3930b1e7eb3a": {"doc_hash": "d2d8c4d7d9ed21fd494684bbc9366046d58cad7ec8856366bcd493be2b2c93fe", "ref_doc_id": "e20b769f-4c85-4fe8-98d3-95fedf992891"}, "6e45c91e-2517-4388-aee7-8263ef5f5463": {"doc_hash": "f16155eb0159f666a4a97ef4e39311a554a2d4d1aef27c427523f16c144f4d22", "ref_doc_id": "5ee08117-200b-4e1e-9ca5-f5a333460148"}, "b3b655a4-f266-49c6-b548-e9e4ac6d88a8": {"doc_hash": "37b2bcf253a25094028ce06630b55cce1df7da988eb68a3f49522b878e6818e3", "ref_doc_id": "3241ca7d-cf4f-498c-b6ff-fc8ee4a5fc4f"}, "c47b5631-fd71-4912-b7ff-2b7f86c6c8b6": {"doc_hash": "b003f086aa2f698e5740f0a5452ba1525f48f25c73e99fb789b5e20cca7958a8", "ref_doc_id": "c885dd46-1885-4680-b45f-729dc8cbd4c8"}, "acc9b0ba-c380-48fd-bd61-e3e633059e6b": {"doc_hash": "32dee09a4cd11b8567c2c6f086d0a71a90c0fdf7a9de3eee4d38372190052ed6", "ref_doc_id": "c82b2a01-42ac-4a2d-bae2-416181fe7796"}, "1cfb6efe-aa32-41c5-b43b-eaa9077e59d7": {"doc_hash": "df197eef0ff31d5af812fa827be93567b4c43477b799feff1aa3218b9600f976", "ref_doc_id": "c82b2a01-42ac-4a2d-bae2-416181fe7796"}, "90266fdc-f90f-44eb-9ff8-d1babe7026c9": {"doc_hash": "e7a8731f0f564b0f303d4a3612bc11872261f4c9087763dca6e821e90a613515", "ref_doc_id": "0888883f-ca58-44a8-836f-232acbb45464"}, "01dcc353-0a12-4db6-abc7-0cae833ebdf8": {"doc_hash": "32a406d29950b1e3eaad476eb34e5c5763e56197cdf14142511bb9aa4d1de968", "ref_doc_id": "0888883f-ca58-44a8-836f-232acbb45464"}, "3fccce86-a7ff-4ee2-b29d-6748f0fa3cf9": {"doc_hash": "a509eb8c0786e41d8ac44fbb851111f94622b05e90113fff631e876cd5af814f", "ref_doc_id": "94b24187-79c0-4578-b7b9-ed0172f76e2b"}, "906b1dc3-1307-4e75-b83c-b283d3055292": {"doc_hash": "59097885f9e405331e5da7020ec0dad1d5826cd4267cf015d135a56f2b351822", "ref_doc_id": "94b24187-79c0-4578-b7b9-ed0172f76e2b"}, "37aa9c0c-540b-4534-aa85-327a648b5650": {"doc_hash": "d805d6d53dbfdc14dabf191c0b2f92cc55eb72ef77dc22db49173cbf8493d39b", "ref_doc_id": "c8263fa6-df96-418b-81f7-c988498c8f38"}, "06156c45-8aae-4d99-ba85-1e4530cdb14e": {"doc_hash": "358b53bc3524185f4580fa90b03020676129af4d09552e616c82f1f6d12f5ffd", "ref_doc_id": "c8263fa6-df96-418b-81f7-c988498c8f38"}, "d264ac55-67cf-4a98-a719-e9c0bab30979": {"doc_hash": "a00814671e7c10faa2e3732b3c293f1caf631d18cfcb11d280c29032c2c413d4", "ref_doc_id": "ebd0a0b9-efa4-4618-aba2-24c8ca40f252"}, "0e3ad696-6851-4ea8-9361-c73936ebf239": {"doc_hash": "5dc3e395da8c7e45d4dc30c0f1479b360353f5cb42c928acc04551b28e7bc13d", "ref_doc_id": "ebd0a0b9-efa4-4618-aba2-24c8ca40f252"}, "718dd368-afdc-4c7b-9def-9871b0d2300f": {"doc_hash": "901c6e1f59ee881e108605e22f44923c27bfcbf15889f0030cafe1ccd9eda463", "ref_doc_id": "c692eb65-ba63-430c-9f10-7bcd4c88b062"}, "4457c319-ccfc-43fd-a730-5489ce3c62bb": {"doc_hash": "2f9feb4c578fbd500067f46e4d8b75ebcc1d237c02415240313acba1ae3bc4b7", "ref_doc_id": "c692eb65-ba63-430c-9f10-7bcd4c88b062"}, "c907e78c-8170-46e6-8b31-6d48a664c96b": {"doc_hash": "7233ae956998c9d73f65c5a02abd3f7b9947e23e7d48ac13a215f43b8336a2c0", "ref_doc_id": "4d500a24-754d-419a-96c6-39f8e8ff868d"}, "e92e0471-6548-4fb9-ab08-f274275ab898": {"doc_hash": "aa8b9da8c10a4e9f3e7339476a72ef8981a985febf7bb76ac0a79705f9c75644", "ref_doc_id": "4d500a24-754d-419a-96c6-39f8e8ff868d"}, "7ad393b2-3ad1-4625-b08c-b8117ba82f78": {"doc_hash": "e6be317b487b3f3e182c570c96ddf2674fac1926ac455915b7316674bd4c5fa6", "ref_doc_id": "f1b086c1-22e1-4117-8ec1-0dff3d646703"}, "4718c3d7-139e-43ad-8f1e-8b2fdf45f7d3": {"doc_hash": "93562fc09379a5a8886923bd19c9c1575ad26a0a363f4fac1f03f5603a3e375c", "ref_doc_id": "f1b086c1-22e1-4117-8ec1-0dff3d646703"}, "04eec716-5102-43ed-b3b3-ce391a16df5d": {"doc_hash": "9a8face570f5a9579966f98efc10a50ec7911fde2469f32692f6ab14e72a9a2e", "ref_doc_id": "76f442f7-285d-4bce-9e61-ffb24148240c"}, "83b501f3-92f0-4f39-b559-daebd309dd7d": {"doc_hash": "fc0bcae1c438a1891cf5b188e86571c9cce12235f3b29647ae838ef94dc58eb2", "ref_doc_id": "76f442f7-285d-4bce-9e61-ffb24148240c"}, "83c71cca-4da4-4a91-b591-ed7b0892485a": {"doc_hash": "e2df2a9bbc3e4d13ee04e9d74744423d54580752851a148d0749721fab734a37", "ref_doc_id": "bf8f8c83-ed31-499f-99a1-98049710487b"}, "7ff387b3-6e42-4fd4-8e48-3a4d3039e28f": {"doc_hash": "16efda1d5f5852d9f08c444a45da3ff6f0c0b33125c88c4c11c06faf2d6f57da", "ref_doc_id": "6b1fb81b-4525-461c-81fe-8b6300df297f"}, "fde49924-31c9-430b-8aad-34286417b0ec": {"doc_hash": "8ae9340109534c967443281852f50fd2804db7004e91d2c10e4367cea2504bed", "ref_doc_id": "2eebf101-fe44-4e65-9b26-d16a6f3fea72"}, "dc164f5c-b6ca-4cc9-8560-002be969953b": {"doc_hash": "2ea0360b456fdefe2cf15c0de3a1ff5daedb784c42142ec2d1747297c5c928a3", "ref_doc_id": "f4b9e9f5-ffb5-4d9a-8c8f-411ed4f0ee78"}, "3503ad07-253c-43fa-afe8-df305bf5ae27": {"doc_hash": "4ba240beb77dfa93e9545791c0a0e9b24abfc2a42e3a9eec8f207a26d99f9fe9", "ref_doc_id": "658501cc-22a7-431e-a0ba-f0c1d40a13cc"}, "aa7fd295-404c-40a4-a5e3-142ce4b8d30e": {"doc_hash": "bd243882d9ed6dadbc2935820507d955b439d5249d02494f87718c67555a5929", "ref_doc_id": "658501cc-22a7-431e-a0ba-f0c1d40a13cc"}, "2a894447-6661-4d46-b084-c482f50bd1d4": {"doc_hash": "10d7b554bd7527a33c7ea797c2c8f8222c9c4975f97fb1d192943cb0975e191e", "ref_doc_id": "3c4c6d05-91cf-46ef-be46-b6e213e426e3"}, "48ce95a7-fd2c-4ee0-9873-f52975254451": {"doc_hash": "d50ec1a000ea5bc9719f0625029160ad7f272cabf8e5efc4825ad1e437e7b74c", "ref_doc_id": "3c4c6d05-91cf-46ef-be46-b6e213e426e3"}, "c378954c-b525-4deb-8693-091245ecb204": {"doc_hash": "879498b4944f2a3dadbe6201ef0f98bb1463f23bbbc76504c9871c7a541e1c80", "ref_doc_id": "fd498739-8e5a-4c7f-aa1a-3a3abc3eec10"}, "bd922aa5-bc7f-40b1-9c7f-be083170925d": {"doc_hash": "d7c410a75691142e2a13e7e3d8529623a2f70fa52a19a2cd877b5426c6f70b4f", "ref_doc_id": "fd498739-8e5a-4c7f-aa1a-3a3abc3eec10"}, "c8492990-3cc3-4e6b-92ac-db7bac4e1d4b": {"doc_hash": "c0873a3850eec1c312abd0b6fd654dde2be551670bf9d448b72963ceeb8589f8", "ref_doc_id": "96d96ca3-b8cc-4f9d-83ae-990e7310483c"}, "5a0020db-1ff4-420b-94eb-977141939801": {"doc_hash": "964ed70954ba94aa4a1f7d2112dbf154d5511f8f4c9938f70c83f6ce1dc263a0", "ref_doc_id": "08ce4d48-c7d4-484a-bca0-6d99c85818ab"}, "dfa1d953-404e-40c8-bac5-2e3e65ee64a6": {"doc_hash": "406e4185dc1a7db8e37adbf26df27d23d0706cdcd9c7b751a7357083fd86f4c9", "ref_doc_id": "fab280d9-b044-4f1f-8cc8-3af99641d9e6"}, "bc8ded55-c754-47fb-939d-6bcd74a47820": {"doc_hash": "82e82e3e0dd883a244eb8ea5191a5fdf6a3ce79c087afb998ddfe6df6c3709c9", "ref_doc_id": "635cd94a-f448-45d2-871c-552bf4d5d189"}, "bf076420-90f7-428d-b2f9-37e1e98cea38": {"doc_hash": "db21e5c793c542f6ffbb1cdf0edc522832b941be3aeb72a2c85e7cff41526445", "ref_doc_id": "8f8e5fe2-bfbc-4721-81e1-50f7b5e42d1d"}, "c64596a9-9f3e-4437-8f92-72e1cf74c52a": {"doc_hash": "eac156bfff7ec407e951dd6f6ed6e82b1daaabd51bfbd1416304f1765ee6463c", "ref_doc_id": "78a453aa-4ef7-49bd-918b-c021b1a277c3"}, "6b9cf5a8-5b0e-4b15-9fd7-9822cc708161": {"doc_hash": "b5dd31bd445af24f0e0fe4c611ad60e4eb93d96b91b8b90cfd2c564b25f32edb", "ref_doc_id": "78a453aa-4ef7-49bd-918b-c021b1a277c3"}, "0a3f6a32-2797-43f2-8358-9a54d3d840a2": {"doc_hash": "6aba8fbbbb461ac98391d0e15603e49c7ff828958c05303f78c4b18cb73066a2", "ref_doc_id": "805f8da0-1849-4e62-86ae-138388034d4e"}, "2c3d612c-b103-4c1f-9fa5-b782e7074002": {"doc_hash": "e3c0791ae78f199f36982f0e006444171fbdc09c36f28d6bd06672f765b00de1", "ref_doc_id": "9c430753-7c2d-414f-b707-e50f9c43f353"}, "1e0e1eba-71a0-4a00-b27a-0c2907614df0": {"doc_hash": "2e2f8a4283abeb8f655bba0216425d0fd3a6417dff05efd2cb8f393871273d99", "ref_doc_id": "0d097ee2-813f-447d-b568-38598951c098"}, "c86744b6-f16a-48ee-8ade-d146439499cd": {"doc_hash": "1318515494a68012231c23e58f5cd2acb3eed0e4981d895b4f7ccfdee560490e", "ref_doc_id": "a8aaa4c8-dbb4-4d68-9958-a815ca1a9348"}, "0654b99f-2090-43af-80b6-8f932d69533b": {"doc_hash": "89e48bf129e54fd325dfcf6f7d34f1b4929f9d0c8b6675a326558e45095ea876", "ref_doc_id": "dafd02e1-cc66-4f4d-9953-e2860149c41b"}, "f95f138a-ac5c-4368-9c1a-914766bf44aa": {"doc_hash": "a79ce2725d9893bd3e0a33bb9cf840182c118206c823c81f50be6f8acd1eb177", "ref_doc_id": "496a0755-6c23-4b1c-9cbf-6f8480be3815"}, "b648c630-f976-400b-ae29-23a2dd6c5d5f": {"doc_hash": "24d7c57671be202d6a9f7a103d93e4f749a215ef8815a5a58f8fe75b91037d6c", "ref_doc_id": "d7a8c841-2634-4954-99ce-bf4241308431"}, "158b692a-7ec1-4af5-bb42-4e2f6d3cc2ad": {"doc_hash": "a59cfecfc107df862c9e8b40b0be908cb49f9024d2de6711782df6b6113c8634", "ref_doc_id": "d350a4be-b8a0-4b03-b0d4-7c965fa18abd"}, "fce1ba28-f54b-44b6-a24d-6a7e5ecd97e9": {"doc_hash": "066ed6ec2eef872c75cd17d666e2b10b98b1e60a14e6eb47a36918f3e54b2451", "ref_doc_id": "0ef14852-83e2-4429-869e-3384de05af97"}, "090ed244-b2d5-4ccb-a266-0d4fb5bcba9a": {"doc_hash": "14aac7834c0d8c50dd097a756053f42abde8223793225e2d0bc1d3c85e60c500", "ref_doc_id": "627c606b-520a-4fc2-883e-258ea18d1ba9"}, "fbdb4073-6df6-4699-9252-82e8356c8c34": {"doc_hash": "e11a92f7266109818ab72a15db4347d93cd98948e71c6c2b0399107522d76659", "ref_doc_id": "5fb1c77f-0fd2-4253-8ce5-987c9c6b9718"}, "67a0324c-6fe7-40b1-be46-627a9ad2233e": {"doc_hash": "0bdf28247df400501bbfa8c521bc6c81e02d5206aa3b219f15649c3d97991dd1", "ref_doc_id": "727ae153-069f-4bad-9283-6c25b5eed07f"}, "2c09db04-2675-4b0b-9bbc-de3ad5d1c664": {"doc_hash": "5039b03a9588506aaf2cd0f10e9ba3c6e5e6f10052d25f8da3fc11cbcbfc76e2", "ref_doc_id": "17e04d86-1cca-4f25-85db-eaeafa02e575"}, "12af3b18-6bc7-41e3-9fe5-494324982f8a": {"doc_hash": "18dfbe6c0959eb0a00eb9fec5c62341d7fddaa1e8e15b9ca638b863e4fa3442e", "ref_doc_id": "a2e93cd9-3b0b-4f27-ac99-059e8a76b03e"}, "cb13c36b-d5d4-4a9f-8491-4ca9a13a3987": {"doc_hash": "6dee7ff73b09933caea6cf88cf2ade9b9943cdf033c058ebad3ef60d0b9fda07", "ref_doc_id": "88a2232d-8553-4d2d-8011-b24a5c3e7370"}, "6ce1d6a6-51d7-4efe-bae1-784868504592": {"doc_hash": "2d5ecc484837658c71224f36f97df7c2d2334b78ac2cad9ba9a382d751d8ddb9", "ref_doc_id": "75f1f244-a235-4305-9c7f-2cb28e21c5f3"}, "a2c9d2b5-1c33-434f-9976-f0dc07b82b15": {"doc_hash": "85841702afa7eb7068e217db638154c4b78fbc7db3d782ea1e50b73a11703534", "ref_doc_id": "1fe440f1-aa33-4c34-81c3-52cabb12e457"}, "7bb9d668-765b-4cab-a2d7-ede1e49fc77f": {"doc_hash": "92715940e11085f4d92624b0e07df0a88a43dc8112457dbe06d7b388d5ea5e8f", "ref_doc_id": "a97eb801-302b-491e-a50d-3561ed42ddc7"}, "1e8ba4c3-917a-4fe2-bbdf-05493604fcb4": {"doc_hash": "03a41312ee854347e8159652100f4a5480dcd75f2a1b41f112e888936d1ae863", "ref_doc_id": "261b10ec-031a-4407-8267-231ce7fda391"}, "575d903a-b2cf-4616-90b2-4d7b3bf52ca5": {"doc_hash": "c956df1ea4410eab11f462c8fb651bbe5076c5bd185b53f9e6edc43c082ab9d0", "ref_doc_id": "274809b3-12fc-4a47-b044-a014509ebad1"}, "42c38932-b5f3-4ca1-9d7e-9b2a9f9a6964": {"doc_hash": "5850e6910e120afcf35f6883eada3280063c2302949877e501fc22fee3897104", "ref_doc_id": "cb3bd1fc-a037-4475-9bcf-87df0437c95b"}, "00dd7c54-2741-4adb-9aef-5cbbd5ac3c9f": {"doc_hash": "c986e32bcb751d2be353f8355119bf2c9e52dacbf80fec4147edc944ada8ef65", "ref_doc_id": "50e0664c-c917-4d0e-a0d5-680efe8ede59"}, "2f0ae121-c09a-4eb8-9fe3-b0125220c0a2": {"doc_hash": "2c29a1fadc21f2aeab323105f0dc89ce89fe9d5bbc385532ed745e5dcb515ea3", "ref_doc_id": "ef10b3f7-7efe-4e64-ab51-25ca3093b75d"}, "18e3f684-448e-4e44-ab33-df8e8228579d": {"doc_hash": "ba410f2a82cfef6860617aec68eb0795ae6ad4f08f4f8fa6f5d6c2fde0d206b9", "ref_doc_id": "5beb6031-2fe1-481b-beff-741786a969c2"}, "fa616148-0587-45b6-82e4-766d343d1652": {"doc_hash": "c05027dd48450837f21103debebd5aef6d3035690495d46f8d80ccf087e3e45a", "ref_doc_id": "99d5f1a2-401a-42d4-8eb1-b51009ed22eb"}, "fc826763-bec0-4f45-986a-b93cb6c60a3f": {"doc_hash": "dcc8c0114e5163afebb935e89f575ad90232e151028febcdff72b2accffa249d", "ref_doc_id": "355fbc9b-0b18-4433-a65f-8a86adec65b5"}, "b80bca60-f6aa-4f27-a855-fe18b88ba2fc": {"doc_hash": "0e97444cdb841fedbd88cef3b6a787e2916e7a3e457fa771abc0ab45c2ea5d41", "ref_doc_id": "21199942-58e0-4859-bc7a-0f6ee3f37bd4"}, "c3afbdeb-3bb7-4df7-8503-e8daa3e6c830": {"doc_hash": "8cd3203654866e7adf7019a207e5c4ebd718a04c229e359c51763db025491c7f", "ref_doc_id": "21199942-58e0-4859-bc7a-0f6ee3f37bd4"}, "a73ba4d8-c6cc-4448-b825-e593aa3aa137": {"doc_hash": "4b940c2421ce678939cd66fbdd5c7849bbe8fdc1a43198d6caf9b38377db8fac", "ref_doc_id": "893f781a-3ebc-4402-a8a1-ecdea1abce5c"}, "d0b6c73f-1fc9-4f57-9c7a-e6532bf2403c": {"doc_hash": "d56bd1f4bb0e4b63fc6bfbe025498bc89d3f4082253dabd77c4f4324837dc9b9", "ref_doc_id": "893f781a-3ebc-4402-a8a1-ecdea1abce5c"}, "46b6babe-b7c4-4f00-984a-f7749b5e87c3": {"doc_hash": "64b3b0d548fa78ab761d276195f0311cbd8ef640843b0e6837fa65094c2dad4f", "ref_doc_id": "0354d90d-1077-4f8c-91e7-ba5f57e7145b"}, "60123adf-e9a4-4de4-8985-c2b1920ad22c": {"doc_hash": "10174112322dd5f45208737823b9b9da32cef2c02a21adf556ff433e1024b782", "ref_doc_id": "bf11f421-1744-4c25-b6fa-71afda8feaf6"}, "91893c9f-a6fd-45d5-afd6-fcbdd72a4577": {"doc_hash": "79a355d0b56678915f13140f1ab124a451c1b6ea711d0a21b8aaad567534a1f9", "ref_doc_id": "9334f872-2c3f-403c-8b28-a0f1dbe08614"}, "f07c6912-b33a-4398-b46f-a93bb7fb431f": {"doc_hash": "6e5588ffa77a39eb18adbd9e82d6272200becab20bb4c9ce7e0be2e00cfebb4c", "ref_doc_id": "63571516-7b79-4793-ab36-68eca6b1153c"}, "4b38c436-d3d9-4ef9-a0e6-55e1dc212ea2": {"doc_hash": "484d5b7f4e92a9b22ccf0f4fe828e68ccd69c86a82c17654d0a80d0200a385db", "ref_doc_id": "822263a0-8b86-4e38-81dc-307451364eef"}, "7159d3ff-2477-4cb3-9fe0-7ef020effd1a": {"doc_hash": "b5f9023a989d7cc7ee6a96e51902fc098a8e49947543c104409e1b979249a9b5", "ref_doc_id": "822263a0-8b86-4e38-81dc-307451364eef"}, "e8a4ce77-ff5a-447a-9434-5cfa39f360b8": {"doc_hash": "217afe8df161559e8c99681d2f9fe4f7e81a34d16e002ed66fe0af123c2af5e2", "ref_doc_id": "4ac156d7-1f5b-468f-a8be-d0b9ae53f19d"}, "0bc52f80-3f46-4a63-a512-8b1708456e63": {"doc_hash": "881ecd6696f01c36fe0880d69eb2f6f9b77bdc1ca8da02be6a865c54451b4a34", "ref_doc_id": "7dae1560-c3b2-4c30-8656-d79732c5747e"}, "71591de7-42e7-4c8f-b0dd-2088f2b8be1f": {"doc_hash": "a9b6dea86c466ea490861e191fa22c7172c519dc40188cd119a715c47febdef5", "ref_doc_id": "7dae1560-c3b2-4c30-8656-d79732c5747e"}, "bddc14c7-1bae-47c3-90a4-2f624fdfe14e": {"doc_hash": "e9133fb2d0bedb78add9e19432f557d2b69b210c2378562f00ca6ce431f416ad", "ref_doc_id": "7d07c078-8c04-4b93-b27e-37f0f861b119"}, "f10f0d8f-3a37-4e7a-976b-828da8e042bb": {"doc_hash": "7b88cd1a7bbbe3222f36cf6916064f51d9abc2dee3d86b7b1dbd532626618b0f", "ref_doc_id": "df6a132f-9aeb-4697-91ac-978e6214a708"}, "14e832c6-feaf-46ad-9b7b-fdb555a59ccb": {"doc_hash": "60955a99e0370053e3ed12426e16dec14890b4955e8a4e212d27a028a81ca8de", "ref_doc_id": "df6a132f-9aeb-4697-91ac-978e6214a708"}, "b82ca246-f89f-48df-add4-8f6c29c68c00": {"doc_hash": "dcf53b13e299a99d64681016252dab419ca303bf29c9d786a4d5dba9e342d542", "ref_doc_id": "c94039f9-42a3-4576-9ecf-3f4f8d5757a0"}, "01036672-0490-47fa-ac22-aafb48f3eabc": {"doc_hash": "6b1039b07a7079ff9353b7f1e61fa2564e35c842307e4394bafd1ffd6cd79a80", "ref_doc_id": "0b22a842-9dc5-478f-a4a3-6463d49e48a1"}, "40b1d0c0-c1ce-41d8-9fb0-ddd8a864b5bf": {"doc_hash": "1d689689e490c7bf3bd3692e4b96ee8c48af0baec66eee971d818aba691b6f04", "ref_doc_id": "7ad143f3-d087-47f3-ab3a-86c01da0a4f3"}, "510a3324-53c8-473e-9d97-13932633e897": {"doc_hash": "83a95bfd89223a7f70a7ab3a3242be10857422c138c88267155a353b5b70b623", "ref_doc_id": "3f0b3733-9ccd-4f0a-bf3e-3281d670693f"}, "ea647641-cb13-4dd9-81e7-8d686d82610c": {"doc_hash": "4766732085421d298be717736447efbb0912e5065f79ad9cf81657b64bdf599b", "ref_doc_id": "3f0b3733-9ccd-4f0a-bf3e-3281d670693f"}, "5023e770-6bb4-47a8-983d-cf4d360cf915": {"doc_hash": "6dd8621ae36febb58d051b196dc7bcbf852bee500a84335c9f38a86877c7c8f5", "ref_doc_id": "bfe1f6ca-d68f-4dd9-8597-816e73b0d48e"}, "b355eda9-a339-44a9-b6b8-4c54ff24c3f7": {"doc_hash": "c1118a8d856dd6cb3c25e04d0d86b1be87ab756a2e9d948a3d061f73033c2b8b", "ref_doc_id": "bfe1f6ca-d68f-4dd9-8597-816e73b0d48e"}, "85d6c4a4-5805-4cf9-a363-3e2c4e7efc9c": {"doc_hash": "23374ba6bb4cd372fe5498d4385e262355432ea7e626e130010bca09f0f1680a", "ref_doc_id": "a24381f1-6ee9-4c34-b556-420b3b8a1a18"}, "d6882cb5-1f1c-4a76-aaf4-baf6b8b4edba": {"doc_hash": "3fb461371116d3b40c3c8580fb6f6a263452dedbe3e1f90c78b7c690c48fba0d", "ref_doc_id": "a24381f1-6ee9-4c34-b556-420b3b8a1a18"}, "dfecbefc-244f-46a0-af2a-32081320275b": {"doc_hash": "266f0c199bb3e796b586c8a09f354673ce85dbe0a78df41c56ac449ac546a4af", "ref_doc_id": "e72e558c-b782-402d-94cf-d856646050ed"}, "d0048285-3393-4215-9e4d-c7045deb669d": {"doc_hash": "3dbf8eb559ecad5a1a3f3fe07b58ced945b30eb2d293eb07960c6a60389a8fc0", "ref_doc_id": "e72e558c-b782-402d-94cf-d856646050ed"}, "e2bada17-28db-4cd1-bc2c-b956a25eb601": {"doc_hash": "78d187aff6f8c91a0a89a4604da0b462abaf609ea522c8b4364bb5ee62b30ffe", "ref_doc_id": "d48125f8-7c95-47eb-a9a2-1da8a1f8d1d3"}, "04f32702-802b-4f2c-91e2-fb40c9220fdb": {"doc_hash": "459a7076a9f3d9ea9fd1632b9ff029628462e805de9a2b61c8f6bb6484d5d7ec", "ref_doc_id": "d48125f8-7c95-47eb-a9a2-1da8a1f8d1d3"}, "1bb99826-27d1-456d-9f94-c2b4e3ef8656": {"doc_hash": "4ee02bd5907fb75d900046b956a151f7e8c302322ec91d3d76311827d62ac23b", "ref_doc_id": "6d36d50c-b2f8-4e34-b77c-b8b2ce8c4c7e"}, "00a36b21-58a7-474a-a17a-d10b4f8300c3": {"doc_hash": "794b3e0278c0936f7a0ada1e6a91214fdfa9675ff5db2a28da7715c2ebb33684", "ref_doc_id": "6d36d50c-b2f8-4e34-b77c-b8b2ce8c4c7e"}, "90d36207-4639-4922-94da-64578289536e": {"doc_hash": "fd217ac116b201d1b5a011e1d59c229927abf4dc7dba768b2e1c71f7b53e2416", "ref_doc_id": "5f06904e-15ed-478e-abb4-9ab5271686aa"}, "826fbcd5-7820-4d18-a1de-f212746aa02a": {"doc_hash": "c0ad985e1c0709ab0e4659ee744d57f101f4d58d71cb4811df9439ea7e8713f0", "ref_doc_id": "45524569-78a2-4cde-aeb8-f128e646ac8f"}, "416c2337-af97-4392-9e24-7201eae83534": {"doc_hash": "f950866becccb9d8dd28b5803942fe0eb783aeb56427a3bfa6288bbd8204c07e", "ref_doc_id": "54ac6568-4203-4084-88eb-d6769455a89e"}, "a3703e7a-190d-4d88-b7d8-a648ddf7af4f": {"doc_hash": "22590a8d590116bb7137ec2a274d7a4e780bb70d25c386217044c7360370c5d8", "ref_doc_id": "dfe714d2-e1b6-4c25-a61a-518c33a5575c"}, "329c01f5-9b2e-4679-8727-a2823e175a4d": {"doc_hash": "255c4c5106f3f16c859006f16a749385d25f152d460fb8a60c8f78ab57be9b46", "ref_doc_id": "2474c822-8a9e-4dd8-a474-649c2dfbc97c"}, "77f3c97e-985e-43d4-a5ed-f6e4852dba6a": {"doc_hash": "09f7550ab1755f3ba8175dbf75967f53a71d242f51737c3cd35fd699a4f41e33", "ref_doc_id": "06b045bd-cd6d-430f-8c13-ff1213437634"}, "7c02b933-8112-4689-991e-7864cbf19ca2": {"doc_hash": "243fbe57034b6a8f85819cd9b8e32a707fb313e7612d253a82b66830d40291d1", "ref_doc_id": "2f815988-3baf-4cef-bb1f-7d61cd3ecb20"}, "8a1f3948-ce71-4a21-bc4a-415be4dd4d50": {"doc_hash": "6a8a211d2f0ec7963bfed92379d9121b934f250431346a2f6f0c058d7b21df09", "ref_doc_id": "17ce5e70-c8c4-4601-ae49-464564d311be"}, "edd9ee8d-07c2-4600-90db-bd22c7974238": {"doc_hash": "24c0fbd1f5c8b57542ef2d427371b1a2f839a1134aefdd135028eeb99b3baf3a", "ref_doc_id": "2997e529-0822-4056-8dbf-671fcb6ccef2"}, "03026346-e9a1-41c4-b320-6f173c8decf6": {"doc_hash": "583cbe4c45d7a87d9efa4c42bc5de109b355e362b2cfb0512e4e3a07a5dc9903", "ref_doc_id": "926f26ab-4fe6-4e4e-b1c6-66a39add1580"}, "4971ef85-72d8-4528-8643-0f434c8cbed2": {"doc_hash": "282979a74d97c88b2d60f402ffaf9a9381e1c564fae6e419e0b20b3478a3223e", "ref_doc_id": "e89191dc-b97a-4a9c-8461-c1425bdfadff"}, "38e3f1a1-dbc5-4944-a358-5cabd0de0d5c": {"doc_hash": "a67e690f36e9c65ad345169c8d2b57dd8875da6b974d6b38a94324be33ede96b", "ref_doc_id": "17f53c4f-575e-44e1-ae90-aeed834cf636"}, "ed7f8025-a763-429e-b3c7-364ce0b6e3c5": {"doc_hash": "3f41669cbb2d0b05e0e09e0d0383437f02abfff8b748cbc187d66f4162593126", "ref_doc_id": "2216b8e9-52cc-4b9f-b043-030039650ed0"}, "e0734d32-68ea-48cf-9c80-13d000238241": {"doc_hash": "c6c819bc704ffa71252c5d43336fd527bd41d7e3588400022fafae632d71a0c7", "ref_doc_id": "726d259d-7897-4875-8bbc-80122e5e06d4"}, "c1955189-8c32-4f54-9088-726fdc835e7b": {"doc_hash": "e03805b417c0256bd31187682a7171ec1db09b36065bd0a62a9739e079dec84f", "ref_doc_id": "25e68918-db07-4fbd-a4d2-9649117a93a5"}, "f4435aba-8a4b-4390-a9f7-c050bb7aac90": {"doc_hash": "3e596c3572058fb6b953d7567e30bfd92911b9e3f276aff764b6d2615034a93c", "ref_doc_id": "a6bf6155-a92c-4749-bc77-db31e0933ae6"}, "290db0ce-65cc-43dd-a71c-8ca00e74bac3": {"doc_hash": "19d81b6f9850717e4f3105db89975bad903572b5093f33b18b92bd6e2416b847", "ref_doc_id": "ceae5917-d4c9-4bbf-96fd-1a624d6b468c"}, "5dc69619-818d-42fc-a539-8713554dc9bd": {"doc_hash": "4be9f2b4650fafd3b08cb7f4b324193141f19b6d0e63b4189cd0345ee88a7586", "ref_doc_id": "7e5a266a-b720-4f03-b559-72dae883ad6b"}, "1f143515-5f74-473e-b90b-14f80f898115": {"doc_hash": "fc5fb8c2bb099cc6a09b22f29b90074f536b337c576828d061673316d37bfcd1", "ref_doc_id": "5ffb5848-f30c-4cd1-acd7-593f5911b540"}, "a59775cd-a041-406e-8fbc-c108f640e4df": {"doc_hash": "1fac911e960124b19727525fb20cfb502e10ade33e6a243d3cf728e0061bcdcb", "ref_doc_id": "93566e10-b601-4b41-b65e-649524bd5359"}, "52b37959-3467-4a41-897c-626d538c467a": {"doc_hash": "f80fd28a8bdcdbcfb8687bc655ce31372cc179f153c63dbe95077d12d514a1c9", "ref_doc_id": "76706c26-ad0f-4ab9-9344-2d9bb4b8f0bf"}, "8192c6ae-e949-4627-b834-d8c1bfbee1c7": {"doc_hash": "a0bbeae990cdace0863765614ba3a8dad22b98f99acfd4b6c512b7dfb40d69ab", "ref_doc_id": "b7e65899-2610-40f9-b4b4-2854fdb336c2"}, "16590794-e6c7-4a20-9ade-b4c80f1fab62": {"doc_hash": "47050b96aa84482f328337873db42c06ceacc8b9c94eff5dc6c7e1973d5732dc", "ref_doc_id": "5fb8a789-0d8e-42f3-a909-641e696f3a64"}, "993587df-319b-476d-a277-af1a9bc2532a": {"doc_hash": "071d0c49378cd7dd983626158fa033b87bef6130a10c13707d66c77ac3437035", "ref_doc_id": "54b19a41-4b7c-46b7-ab78-ad6a37f4aea9"}, "3b144cc8-10ff-4f9e-be1a-f2c31ab2618d": {"doc_hash": "368c378a67d084e47830d651b0d25978fcab6f853011cc6c4f6cdeeb20f3d945", "ref_doc_id": "54e0e8a5-533d-4d9b-8409-cf348235ba04"}, "e8fbe001-34b6-4a32-bb59-03abd03b578b": {"doc_hash": "b0c7b1f464d5dd8b9213b52075bc505f5008ecd6521d26166c8a9973d95ffdfe", "ref_doc_id": "5c12a383-a724-42b8-ad82-60fa5bb05798"}, "82ac4e76-0d4b-4044-a5a5-9aeac3b8cd2b": {"doc_hash": "995e243f8ae4367816765f1e59a8caae250e7c49e85753b601d4fa9b0d834baf", "ref_doc_id": "27aefa1e-9901-4e6a-8636-26162d8e33b9"}, "e3e436e3-bc48-4eb7-9314-4f4c3813f316": {"doc_hash": "eccf6235343f1090d8a8e0988e1eed4dfd9e0bf4f5f18c02461280a17bf3df0d", "ref_doc_id": "f04354b3-730e-4646-be45-1f868be95f12"}, "56a86f1e-9706-4c58-9283-d417a3ea3953": {"doc_hash": "a2ddcdb957a79f1e75ec47dc906ca3f21e4250d5245efd7e2a69cd8b04ab1d60", "ref_doc_id": "a97c2773-c169-44f8-8180-e0e8eb5596cc"}, "57b81c63-7d90-4b01-a1d9-6de1aea87228": {"doc_hash": "26704d76e6b60ce537ae4bc6ed81de16f70c7a464c527885ff3c3d7127325752", "ref_doc_id": "0f2e6bec-c011-4b50-9a7f-c3310682035a"}, "44e8abd9-f6cf-4189-bf71-0df491339798": {"doc_hash": "e74278a6d2e8496e3b902fb17ab886681b5b62b5b6116503c1c71388af9d092c", "ref_doc_id": "d2c6eeba-463b-40a2-85a9-64edec91ea1f"}, "f3b758dd-070c-4319-89c0-e2f0a70f8988": {"doc_hash": "ccd891880706678c291c1a65e04d50dd819183405fe3133727532966ff5199ba", "ref_doc_id": "b84693ac-29f4-4d46-8330-a9ac96f93297"}, "28390e55-c742-4d60-8d97-4a9e56e3338d": {"doc_hash": "adfa01834cf8fdf5f2f28bf32826ad98c7dc4d5a573977881b03d4b71a1a27f3", "ref_doc_id": "9bae20ff-1e7d-453c-8786-99018c8f3aab"}, "6c84e63b-65d2-410e-ba83-af14d1e52504": {"doc_hash": "a660afb9978f0b03775344f858cc95dd11ba20c04ff76e6ded0ab9a47fc9a2b6", "ref_doc_id": "661bed5a-685f-4927-94e4-668b5a14d86b"}, "5a4800bb-9d21-4359-b693-dc5655e980e9": {"doc_hash": "c0bfdf3188939473bfa1ab9670aa54fb096cccda646a08e6dd10b365033970a8", "ref_doc_id": "661bed5a-685f-4927-94e4-668b5a14d86b"}, "e1c747c8-f399-4d1b-a0e5-847e262f457f": {"doc_hash": "35667643ee1fd3889d6f5a40a21ec13a4a3ba6efd4988420289047b401a56c65", "ref_doc_id": "e3c58480-196b-4c10-a19c-b7421d8df1b0"}, "634b3a30-1f59-428e-8049-e37875aa1567": {"doc_hash": "ffa268d2559ffb497d2fb325e59082af3bcd19e423696fd82ae7a844e851039a", "ref_doc_id": "a442861f-e7c3-4f77-9b0b-d929e01bc4d3"}, "01e408dc-411e-48b7-a22f-d54e295b5e37": {"doc_hash": "c7089467e7e79e4c8f116b55284c233f679f8c98b86c20c9eb70e5b769c4bc61", "ref_doc_id": "772cc019-fd83-444d-b093-cadea3cad6da"}, "c7e05597-bc46-4247-b94c-9da413c04f06": {"doc_hash": "62c091481530a94cb96a7fcfb86d112fa45cb50a55eda1e803c180fd52690bb9", "ref_doc_id": "772cc019-fd83-444d-b093-cadea3cad6da"}, "b91c5ef0-8a42-4eee-b86a-35fc71cee60b": {"doc_hash": "4bd8d07c5f37237cc6e6fbabe56b4664bd0c09de6698cb9e057fedca83d4ca21", "ref_doc_id": "a9c2260f-e8fc-475b-9cb2-108fec145301"}, "c94f9853-bfb3-48bc-92f9-2039a8d7602e": {"doc_hash": "d7d24b4e425b67635ad0c937249c6b334f231e74971922aadb1b54fed1101147", "ref_doc_id": "c632f830-aa83-4fa5-ad9c-786d969f24e0"}, "0de573cf-e894-4295-a78f-f177ffc49e81": {"doc_hash": "594a40fffc5a72fe95e73987f9e29ae73f288efdc0b032240a393c93e6b5369a", "ref_doc_id": "1d3e02ea-4951-4ea8-9eaf-e0a65b47a4e2"}, "7078d875-a724-4cf0-8038-fbd380461fb8": {"doc_hash": "e6f2ebf98bd8f55aaab785e3d1f950f72937f096f0adbad847170ce42096a15a", "ref_doc_id": "b5fd944c-94cc-4088-83bb-cc9d3ab7a782"}, "abed33f9-4a01-4211-bc5d-de0d3f2ac85a": {"doc_hash": "1dc780462e88fb81e1dbeb4d09ab4a0b69204deac056efbdc6386abdf227163f", "ref_doc_id": "b5fd944c-94cc-4088-83bb-cc9d3ab7a782"}, "a88b5526-c818-43ab-85f3-422de7672090": {"doc_hash": "d229650a63533ea189dbaceeb92b874beec828abea520ba6d230ab308f5c65f6", "ref_doc_id": "a44aeda0-c79d-44d8-91ff-747169a5cbee"}, "454b7a86-41ae-4209-8717-88ad3da2c6d2": {"doc_hash": "e47a3a806451176076993e9b176bcb562f1a74eff7977bdcc577db9251d32121", "ref_doc_id": "a44aeda0-c79d-44d8-91ff-747169a5cbee"}, "b8de74f8-009e-4739-a494-85d42f2a5189": {"doc_hash": "2f4d66f8b9e7304368dc74e495b59dd1b9c707e859d7c62f6092b2774ba469d3", "ref_doc_id": "5775bed2-0724-4d6c-9f91-ad2509c057ae"}, "d18df246-18ee-4893-94fc-c276d0dbbd9d": {"doc_hash": "d79566f68be7b2efc3838adc951300751709980749ab7cd5a5e44fc81413c289", "ref_doc_id": "5775bed2-0724-4d6c-9f91-ad2509c057ae"}, "3ba15767-1938-47c3-8631-73e0a5a74ef1": {"doc_hash": "8246ebb9a9ba0469c54920b2b2e4e2323564566bd431784a216bd255852251ad", "ref_doc_id": "191f31e5-8703-4458-bda5-52a255fa0aa8"}, "6bbf9615-df32-449f-b330-f05004702450": {"doc_hash": "178db03c4d914756df25c28ec1959913c427a215ebe72a5d758d0bee62d78e3b", "ref_doc_id": "de6de8cd-c1b8-43b0-a5c3-3ea94b57675f"}, "b29da23c-eb0f-47b0-9482-adbf688d83f5": {"doc_hash": "95144b31abd1bda4c80e728e805a7a75745b7990a12ee2a55c56e55f0c52ec26", "ref_doc_id": "19f073ff-bbee-479b-b397-9ab75a642722"}, "41e50cb1-adba-4455-8b04-249f67cedc5f": {"doc_hash": "ab9fc7bb4d118612b3d9cec290dd76de156c1c665d60c829842f69d44d45150f", "ref_doc_id": "3dfdfbbe-c85c-4b1e-8b23-2428f4a0c24d"}, "bd9e15ab-3178-4524-be90-d16b1d00628e": {"doc_hash": "d28bf7b9b86445f83b7de608edc128dd1e77de3f115cf40df38737e1dba524d9", "ref_doc_id": "1c871c3f-c01e-470c-b2ba-db433ac6bf80"}, "9c419638-9c0d-4b3d-98c6-97dab3f1946d": {"doc_hash": "1b8a33f6abbcdf4dbc3f41bed0b62a3eb2813c9eafc1516bdcd1c98d01cbea88", "ref_doc_id": "59bd979f-508f-4fd6-a92f-6351c5c02320"}, "2179cbe4-62de-463d-b8ba-fb7797e46856": {"doc_hash": "aff03896d04686aa4af6fe3cec4d73a1ed437767d0b04ddac52aaaf0972379df", "ref_doc_id": "90aede91-8f1c-4d72-af2c-99ec4b6e6073"}, "75e2d352-2b99-417a-a45c-ae8d64493cf2": {"doc_hash": "9345a2b64e3c91eda98c029222552e3dcff1a3064c3c9fae36259be3de564ced", "ref_doc_id": "2a24f7f7-192d-4f31-bb42-fea17c558007"}, "056c79a2-c025-41fa-93a5-d10925dea0d5": {"doc_hash": "820d00a0f6ee8094e1c64ab6bf1bd4d419ce85185a2f47a5efe732a6367f5d1e", "ref_doc_id": "f84244f5-bd27-4b1d-85c5-7d08aeb152a9"}, "6e16f9a0-1ca4-4645-8be7-f855a96f7098": {"doc_hash": "6ee9ca198ebf44b8fb3a49dd7b45e6a90f835c2b9d8f1564810e41aff0ae0e84", "ref_doc_id": "7c04d424-1174-42a0-b670-159795dc5edc"}, "d834629b-9224-4491-9e1c-bcdb4d2d31d3": {"doc_hash": "36d690c616917ba5703d483a8e57160cb7b14779dbcb17bacc7d983857ffc1fe", "ref_doc_id": "eec73f3c-8c2a-46c8-9c4c-163ea47a42da"}, "cfbf8a9e-d300-42e0-86cd-907d0d600a26": {"doc_hash": "7042fcf39319a177f7a2b09161e3ba8002f3a2059b1f24063f0e649564c1213f", "ref_doc_id": "a187bc3e-a0ee-48b8-8ac6-4b870d598867"}, "e930d448-cce1-4628-9adb-d0a327a6814f": {"doc_hash": "96b1d036133cd693c6eb7e91330f02a0e27adf6bafa137d194759e451b562920", "ref_doc_id": "170dadfc-4114-4ef7-99d9-c0b70ad30742"}, "712198e4-720c-4f74-8f58-483d2c32eed5": {"doc_hash": "defb402ad658cc4715d8dd9a86a6ba19b3f3fdd3e49dbea520ee61cd7deff3bc", "ref_doc_id": "d488e6fa-23af-4e1e-b57e-a76ae707ba17"}, "64f5d340-5b50-4d2f-804a-e7ce56414ab8": {"doc_hash": "be957b9882cf7d27b1cb71ea816d6c6b41e91d47fad83c405a6e86788be651bc", "ref_doc_id": "de7cc68f-5f27-42a3-ba6d-baf1f0b5fa6d"}, "09e51e98-b374-42cf-b6f5-4f8a3e65b80d": {"doc_hash": "c1179fbcf8aabee5d1180b7d7008e27bcb6fc3429af12298791fbbf745747389", "ref_doc_id": "f40dc871-08ac-414d-aab6-7d6564623b84"}, "97dcd730-bee1-4952-bf94-32617faeed85": {"doc_hash": "124de8365516c44a3f4cb850fd4df3044463f6f4acd2da96df449edb7a31db93", "ref_doc_id": "d44a3620-294e-4edc-ab54-ea79a41cfcc0"}, "dfa667d2-49e2-4956-b915-40320ed44a35": {"doc_hash": "921414532de6e5c8af02f6fc0597daefeec9824ddc621d43823f46700eccad17", "ref_doc_id": "22ac7c1a-0471-474f-9596-d1163ce43b2e"}, "e02463e2-03de-47b2-b9af-da270498626b": {"doc_hash": "13b4e43604d1f4ea799fa0f57057d8f2059e1d010835f806e53072df49a6c351", "ref_doc_id": "d70e8673-2d5c-4f71-831a-8dc443321596"}, "27ceef58-8ed2-4100-8cff-e6473adbc9c9": {"doc_hash": "bc0dd38e388ac4d317cf3179011c532848904e6268bd7b4c8ac86c4aadc5bcb9", "ref_doc_id": "971f5035-f12e-4e2a-a610-7c84ae471b94"}, "40bc5295-5805-4797-bd3c-1e61d1c39dd6": {"doc_hash": "494de16d6d52adfcd24e057cb42e85c5b26ac073031ccffd58a1ee181a430895", "ref_doc_id": "971f5035-f12e-4e2a-a610-7c84ae471b94"}, "af7ff672-6ab4-4a6d-9684-dfffa5fca5be": {"doc_hash": "653471baac42af8909ab7dff1a267195a5037491a6285857597d9786a902e94e", "ref_doc_id": "f23ac8f5-f0dc-4ee9-843b-7013db1a9663"}, "0046ba3d-ab8f-498f-9821-b3d1928c7b92": {"doc_hash": "ad598c19eb4a049fa6425a241757ff21b6becb11bc6f67287f23f1018e674e0c", "ref_doc_id": "f23ac8f5-f0dc-4ee9-843b-7013db1a9663"}, "a82ca2e3-34f6-4756-bee9-e6d6a0cd3bb6": {"doc_hash": "86551281f7e63339de01aea61c1c91eaa884d16440f8a2a820fdf363e4332176", "ref_doc_id": "5eb950a6-42f9-4bae-a201-0517e84a621d"}, "dbafb3ed-b5fd-48de-b6e5-0eeb67a0210d": {"doc_hash": "21c8818ba9a56351a02ca2c24bc52aeb2714930322c9227220bc9171790a0ec1", "ref_doc_id": "2e88584d-da10-4fcf-a713-1351658c2553"}, "585216c4-3a9b-4764-8210-098419010237": {"doc_hash": "ae73284eb38d82007125168de54fb72420c24a15c16e0320674646b2890946df", "ref_doc_id": "6577e924-2e07-4990-a65d-10b412405651"}, "1277b973-1bf5-40b3-83f6-483744ab302d": {"doc_hash": "538277918c3a06c36a2129e1245b284c443620a911c7b8c0d44a9f83a6918aa6", "ref_doc_id": "350aa6fc-67a4-4a4e-94ce-9e76e70901cc"}, "8e0e599c-8f25-4a3c-b91d-7bf4be37aed4": {"doc_hash": "58516b58566863f1c988e5e48c26dfaf210416b5d579afc1c703555ecbad2c6d", "ref_doc_id": "e0a38699-3a9f-4959-acac-a10f6cd3eedc"}, "eed2712a-a74f-4863-97d5-f5387e927b28": {"doc_hash": "a35988e3bd37848202d5c9bc8ad96280f70e2746843cca7147b4e35209b8ef85", "ref_doc_id": "f5170a1a-24c4-4478-89ad-4341a0108c6e"}, "d9e623c1-9ff4-4d8c-8dcc-75fcedab04d6": {"doc_hash": "12f57c0679ad41049315940a4f66f382fdbbed59e9c017d7e032897aa32421bc", "ref_doc_id": "0d982ed5-25b9-4858-b7b3-c7f493969896"}, "ff21d60b-a167-43b8-8e27-d38ec1832bc0": {"doc_hash": "ad873ad5da34731befef507f31548d88c5f95a60dd9db3cbdb554579b8107b4d", "ref_doc_id": "64eac5fd-e635-478d-b227-b7938c8a3ab9"}, "d77e3e3f-7779-4458-a076-82cee750a74c": {"doc_hash": "fc4e65954faf11de220ccf30e81e005a3a668b3cec0c680aee3c8f7de45cdeda", "ref_doc_id": "4e087112-5746-4a71-a607-644442d7ef26"}, "ca1636da-1fba-4f40-b60c-a8b7c053fa37": {"doc_hash": "b4f2d1660f6d331a86a365bbbdd40e9bf4402d0f42bb7695428c0f633868acec", "ref_doc_id": "b77ddef4-24d2-4599-8a26-3ceb39d934be"}, "e357dbe1-1712-466f-a7ba-868095de8c61": {"doc_hash": "580f8ba4b616a9a5bd4cc3288feec1e48dd06e7bced0ffa0288a7a1c19f2656a", "ref_doc_id": "47f78103-6b65-4f99-98de-a6c968bc23ab"}}, "docstore/ref_doc_info": {"1cf0bc84-4f19-48ed-8fb1-6928bc429853": {"node_ids": ["44aae20f-64f6-4313-9d4e-1007f36541d5"], "metadata": {"page_label": "1", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "fc49187f-8138-4140-b291-bab8c53111c7": {"node_ids": ["7ea269a8-7360-4d2d-ad75-8f0dd75aca58", "a05de66a-a4cd-46c9-925a-f8c71f4d87d0"], "metadata": {"page_label": "2", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "cad8c60e-57da-4b63-9b61-54b93306b7b8": {"node_ids": ["2b7f3b72-f1a5-49dc-968c-4732c031ec90", "62ed9e17-9110-429f-b071-a875b0b5b23f"], "metadata": {"page_label": "3", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6c36ef3b-ec00-4991-8993-fc0fa6c9af08": {"node_ids": ["dadec924-b3e5-4ac9-9d9d-08ce26e51c32", "431846f2-a241-491f-a310-f87428bd313f"], "metadata": {"page_label": "4", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3c35c144-0e93-4fbd-b142-36072f24c9b9": {"node_ids": ["302ddf55-dfde-4527-a4bd-236e2de9f884", "3b73e086-deb0-4ec8-8561-72207e630872"], "metadata": {"page_label": "5", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "7e0304c1-0d9e-4aa1-afd6-24ecdc69460e": {"node_ids": ["f7c0e0a0-0161-4430-85c8-30fd55ad4a9e"], "metadata": {"page_label": "6", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2c450a3e-aa63-423d-a940-7237b75165ec": {"node_ids": ["5bbc9c4f-4cbc-45b5-92c2-d115c20a7b65", "d36f84bd-d341-4972-8494-e520da286a0d"], "metadata": {"page_label": "7", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ae00076f-441a-4c9b-a5e1-de6e5841889f": {"node_ids": ["ea8279ec-6956-463b-8eca-e65f45f30e80"], "metadata": {"page_label": "8", "file_name": "2202.03371v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2202.03371v1.pdf", "file_type": "application/pdf", "file_size": 289550, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3864b547-5f35-47c3-bd66-9140804d6dd3": {"node_ids": ["4d4f40c4-bf31-4496-986d-aa1649cbf507"], "metadata": {"page_label": "1", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "8b8c8275-cb6c-426b-b691-7d7c3b606aff": {"node_ids": ["7abb6a27-e84b-411a-ac37-d3b2a0feb79d"], "metadata": {"page_label": "2", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "73733ab9-eb31-41c1-9e8a-ec7a760ba395": {"node_ids": ["9eb6d914-1d45-4049-b338-557573b25ef9"], "metadata": {"page_label": "3", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "aef93de9-8bc6-4d31-8d67-7ab14ba49674": {"node_ids": ["1e74bb63-bcd4-4b31-9caf-7e39e0556847"], "metadata": {"page_label": "4", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "aca900af-3be4-4d8a-afd6-e68003af2128": {"node_ids": ["17a668bd-752d-48ab-a8fa-8973d6de5e8e"], "metadata": {"page_label": "5", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "b99a776c-cc1f-4e87-8916-3ac9330f9d24": {"node_ids": ["e12e51ca-83b9-4596-aca8-38344ae5111f"], "metadata": {"page_label": "6", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "4a2addc3-6c7b-4685-b0cb-6a31df257332": {"node_ids": ["f218c722-6185-4fbc-b1c4-fc5c944eee5b"], "metadata": {"page_label": "7", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c3d27641-f4da-41a8-b37a-671561b9682c": {"node_ids": ["f5521f12-f02b-4334-9b01-f472d8ddae7e"], "metadata": {"page_label": "8", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "51e8b4de-84b5-4c2c-959f-2a25de83158d": {"node_ids": ["a82f34de-5905-47d8-8fb6-6d3b8b023df2"], "metadata": {"page_label": "9", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6bff5288-dec1-4e62-a90a-cb3d551b2957": {"node_ids": ["e340042d-bcb0-4d71-ab9d-0d1d2c9a0bca"], "metadata": {"page_label": "10", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e3cc3918-a8b2-48ac-91e1-52a8f7e11e59": {"node_ids": ["ed321e9b-0c17-4887-8cbc-08e80694c702"], "metadata": {"page_label": "11", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "8dfca318-5a22-4272-bc12-332d28ce54c9": {"node_ids": ["09ceeb29-1dbe-488b-bdc3-f1056a023a6b"], "metadata": {"page_label": "12", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ce7c14e5-0e9d-46d3-8773-1260cb331065": {"node_ids": ["62a1f5dc-7e95-4c1f-9507-c55080f8a1e6"], "metadata": {"page_label": "13", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2aacb4cf-4b39-49eb-ac3e-eb423aebc7d9": {"node_ids": ["42c32953-702a-4fad-8b80-3a2b86038006"], "metadata": {"page_label": "14", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "a808c423-24ef-4e78-978c-3c8102cb821b": {"node_ids": ["1e024b56-1659-437a-9700-11cd9b23ff28"], "metadata": {"page_label": "15", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "a9e72ea7-21ee-4a1c-a710-a1b9ddfb7674": {"node_ids": ["eb6b518b-90ad-45c7-9e3a-0e4998f99022"], "metadata": {"page_label": "16", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ed344fa9-1b7b-4c6c-b38f-dad78e4591c8": {"node_ids": ["e1f2fedc-c5fb-4b60-8a69-edf0ce4ba64a", "5be40d05-4b33-4e39-a03e-b19688f5d676"], "metadata": {"page_label": "17", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "99792d17-8ba7-4669-a571-8039beeaae9a": {"node_ids": ["2e548b25-ab47-4732-b6d5-92fe237cf2ba"], "metadata": {"page_label": "18", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ee0485e9-1d43-409c-b790-6169c05ec167": {"node_ids": ["c8cf56dc-da54-4541-89ee-b66233b0c7e1"], "metadata": {"page_label": "19", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "f037292a-85ff-4105-a24b-7455076cea27": {"node_ids": ["307fa96c-69a4-41e8-beaa-7bfb4cc5f8b8"], "metadata": {"page_label": "20", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "400f475d-5691-4a67-841b-36688db2a4cf": {"node_ids": ["6131f19e-cb58-4718-81e4-09698ab9451a", "16043eea-1e0f-4d5f-b52b-67ac1a5e8bae"], "metadata": {"page_label": "21", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "090eca36-29ef-45d9-bf03-ebd78d053609": {"node_ids": ["2620c189-b8b4-4344-a53c-6c9e0328f3b2"], "metadata": {"page_label": "22", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2eaf6042-758a-4e20-a01c-b126bb27f90b": {"node_ids": ["8453c056-4770-4d6d-a882-862d42c07e47"], "metadata": {"page_label": "23", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "8b0caec0-65cd-47cb-b812-7deec3927471": {"node_ids": ["8d058450-2a2a-4f44-8132-c2bbe5358952"], "metadata": {"page_label": "24", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "eedb5d77-eda4-476e-9b3e-e34a862ca41a": {"node_ids": ["a993290a-7898-40f6-b477-0bf30940c1d2"], "metadata": {"page_label": "25", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "a1487321-7801-47a3-8b62-ae34823edc14": {"node_ids": ["9f80178a-f532-4fb2-8bc2-00e1e7cec020"], "metadata": {"page_label": "26", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "fef6a84b-7e1a-4a15-bb77-483ff59ffef5": {"node_ids": ["7727578f-6869-4fd6-a34a-9d90a2cfca7b"], "metadata": {"page_label": "27", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2bad9372-40d9-4b3b-b3f3-f7a8d3793fd2": {"node_ids": ["225cd80f-bd6f-4343-ba5e-c7332f70ab5e"], "metadata": {"page_label": "28", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "1057d7cc-e7e9-4637-992b-69f7f53d80d6": {"node_ids": ["8578e875-b982-4b96-855a-bfd22cab2e01"], "metadata": {"page_label": "29", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6f7e90e0-2fa3-4da9-90bc-c07182ed2114": {"node_ids": ["83daae95-38dc-4b8a-a0e9-d5bf1ee59992"], "metadata": {"page_label": "30", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2691b756-3e5a-4d3c-9200-ffabb02b378a": {"node_ids": ["e0b0d568-7e95-43ab-9d5b-8f5d0bc8fee5"], "metadata": {"page_label": "31", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "39ade76d-4273-444c-ba32-232ba7a8c3a9": {"node_ids": ["bc909015-b43a-4271-acdd-d3c0ca16999e"], "metadata": {"page_label": "32", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6b00766c-1d53-4d5b-9a03-87fa07cbaa5d": {"node_ids": ["8547566c-ae08-4ca5-bae2-7ebc3eeb1787"], "metadata": {"page_label": "33", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "a7367521-4b50-4ed8-812e-af774415413f": {"node_ids": ["ccfa5162-5f0e-4cb3-b860-41124cf6187b"], "metadata": {"page_label": "34", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6088c380-cb3e-4237-8a4a-9a5ec66b22e0": {"node_ids": ["b083478b-45c6-41ae-aff4-fa4f4f98616f"], "metadata": {"page_label": "35", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "e20b769f-4c85-4fe8-98d3-95fedf992891": {"node_ids": ["b3857fe9-6a8f-4890-a920-3930b1e7eb3a"], "metadata": {"page_label": "36", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "5ee08117-200b-4e1e-9ca5-f5a333460148": {"node_ids": ["6e45c91e-2517-4388-aee7-8263ef5f5463"], "metadata": {"page_label": "37", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "3241ca7d-cf4f-498c-b6ff-fc8ee4a5fc4f": {"node_ids": ["b3b655a4-f266-49c6-b548-e9e4ac6d88a8"], "metadata": {"page_label": "38", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c885dd46-1885-4680-b45f-729dc8cbd4c8": {"node_ids": ["c47b5631-fd71-4912-b7ff-2b7f86c6c8b6"], "metadata": {"page_label": "39", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c82b2a01-42ac-4a2d-bae2-416181fe7796": {"node_ids": ["acc9b0ba-c380-48fd-bd61-e3e633059e6b", "1cfb6efe-aa32-41c5-b43b-eaa9077e59d7"], "metadata": {"page_label": "40", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "0888883f-ca58-44a8-836f-232acbb45464": {"node_ids": ["90266fdc-f90f-44eb-9ff8-d1babe7026c9", "01dcc353-0a12-4db6-abc7-0cae833ebdf8"], "metadata": {"page_label": "41", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "94b24187-79c0-4578-b7b9-ed0172f76e2b": {"node_ids": ["3fccce86-a7ff-4ee2-b29d-6748f0fa3cf9", "906b1dc3-1307-4e75-b83c-b283d3055292"], "metadata": {"page_label": "42", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c8263fa6-df96-418b-81f7-c988498c8f38": {"node_ids": ["37aa9c0c-540b-4534-aa85-327a648b5650", "06156c45-8aae-4d99-ba85-1e4530cdb14e"], "metadata": {"page_label": "43", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "ebd0a0b9-efa4-4618-aba2-24c8ca40f252": {"node_ids": ["d264ac55-67cf-4a98-a719-e9c0bab30979", "0e3ad696-6851-4ea8-9361-c73936ebf239"], "metadata": {"page_label": "44", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "c692eb65-ba63-430c-9f10-7bcd4c88b062": {"node_ids": ["718dd368-afdc-4c7b-9def-9871b0d2300f", "4457c319-ccfc-43fd-a730-5489ce3c62bb"], "metadata": {"page_label": "45", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "4d500a24-754d-419a-96c6-39f8e8ff868d": {"node_ids": ["c907e78c-8170-46e6-8b31-6d48a664c96b", "e92e0471-6548-4fb9-ab08-f274275ab898"], "metadata": {"page_label": "46", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "f1b086c1-22e1-4117-8ec1-0dff3d646703": {"node_ids": ["7ad393b2-3ad1-4625-b08c-b8117ba82f78", "4718c3d7-139e-43ad-8f1e-8b2fdf45f7d3"], "metadata": {"page_label": "47", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "76f442f7-285d-4bce-9e61-ffb24148240c": {"node_ids": ["04eec716-5102-43ed-b3b3-ce391a16df5d", "83b501f3-92f0-4f39-b559-daebd309dd7d"], "metadata": {"page_label": "48", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "bf8f8c83-ed31-499f-99a1-98049710487b": {"node_ids": ["83c71cca-4da4-4a91-b591-ed7b0892485a"], "metadata": {"page_label": "49", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "6b1fb81b-4525-461c-81fe-8b6300df297f": {"node_ids": ["7ff387b3-6e42-4fd4-8e48-3a4d3039e28f"], "metadata": {"page_label": "50", "file_name": "2306.07377v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2306.07377v1.pdf", "file_type": "application/pdf", "file_size": 2910627, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "2eebf101-fe44-4e65-9b26-d16a6f3fea72": {"node_ids": ["fde49924-31c9-430b-8aad-34286417b0ec"], "metadata": {"page_label": "1", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f4b9e9f5-ffb5-4d9a-8c8f-411ed4f0ee78": {"node_ids": ["dc164f5c-b6ca-4cc9-8560-002be969953b"], "metadata": {"page_label": "2", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "658501cc-22a7-431e-a0ba-f0c1d40a13cc": {"node_ids": ["3503ad07-253c-43fa-afe8-df305bf5ae27", "aa7fd295-404c-40a4-a5e3-142ce4b8d30e"], "metadata": {"page_label": "3", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3c4c6d05-91cf-46ef-be46-b6e213e426e3": {"node_ids": ["2a894447-6661-4d46-b084-c482f50bd1d4", "48ce95a7-fd2c-4ee0-9873-f52975254451"], "metadata": {"page_label": "4", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "fd498739-8e5a-4c7f-aa1a-3a3abc3eec10": {"node_ids": ["c378954c-b525-4deb-8693-091245ecb204", "bd922aa5-bc7f-40b1-9c7f-be083170925d"], "metadata": {"page_label": "5", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "96d96ca3-b8cc-4f9d-83ae-990e7310483c": {"node_ids": ["c8492990-3cc3-4e6b-92ac-db7bac4e1d4b"], "metadata": {"page_label": "6", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "08ce4d48-c7d4-484a-bca0-6d99c85818ab": {"node_ids": ["5a0020db-1ff4-420b-94eb-977141939801"], "metadata": {"page_label": "7", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "fab280d9-b044-4f1f-8cc8-3af99641d9e6": {"node_ids": ["dfa1d953-404e-40c8-bac5-2e3e65ee64a6"], "metadata": {"page_label": "8", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "635cd94a-f448-45d2-871c-552bf4d5d189": {"node_ids": ["bc8ded55-c754-47fb-939d-6bcd74a47820"], "metadata": {"page_label": "9", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "8f8e5fe2-bfbc-4721-81e1-50f7b5e42d1d": {"node_ids": ["bf076420-90f7-428d-b2f9-37e1e98cea38"], "metadata": {"page_label": "10", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "78a453aa-4ef7-49bd-918b-c021b1a277c3": {"node_ids": ["c64596a9-9f3e-4437-8f92-72e1cf74c52a", "6b9cf5a8-5b0e-4b15-9fd7-9822cc708161"], "metadata": {"page_label": "11", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "805f8da0-1849-4e62-86ae-138388034d4e": {"node_ids": ["0a3f6a32-2797-43f2-8358-9a54d3d840a2"], "metadata": {"page_label": "12", "file_name": "2310.07282v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2310.07282v2.pdf", "file_type": "application/pdf", "file_size": 554738, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9c430753-7c2d-414f-b707-e50f9c43f353": {"node_ids": ["2c3d612c-b103-4c1f-9fa5-b782e7074002"], "metadata": {"page_label": "1", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0d097ee2-813f-447d-b568-38598951c098": {"node_ids": ["1e0e1eba-71a0-4a00-b27a-0c2907614df0"], "metadata": {"page_label": "2", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a8aaa4c8-dbb4-4d68-9958-a815ca1a9348": {"node_ids": ["c86744b6-f16a-48ee-8ade-d146439499cd"], "metadata": {"page_label": "3", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dafd02e1-cc66-4f4d-9953-e2860149c41b": {"node_ids": ["0654b99f-2090-43af-80b6-8f932d69533b"], "metadata": {"page_label": "4", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "496a0755-6c23-4b1c-9cbf-6f8480be3815": {"node_ids": ["f95f138a-ac5c-4368-9c1a-914766bf44aa"], "metadata": {"page_label": "5", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d7a8c841-2634-4954-99ce-bf4241308431": {"node_ids": ["b648c630-f976-400b-ae29-23a2dd6c5d5f"], "metadata": {"page_label": "6", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d350a4be-b8a0-4b03-b0d4-7c965fa18abd": {"node_ids": ["158b692a-7ec1-4af5-bb42-4e2f6d3cc2ad"], "metadata": {"page_label": "7", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0ef14852-83e2-4429-869e-3384de05af97": {"node_ids": ["fce1ba28-f54b-44b6-a24d-6a7e5ecd97e9"], "metadata": {"page_label": "8", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "627c606b-520a-4fc2-883e-258ea18d1ba9": {"node_ids": ["090ed244-b2d5-4ccb-a266-0d4fb5bcba9a"], "metadata": {"page_label": "9", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5fb1c77f-0fd2-4253-8ce5-987c9c6b9718": {"node_ids": ["fbdb4073-6df6-4699-9252-82e8356c8c34"], "metadata": {"page_label": "10", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "727ae153-069f-4bad-9283-6c25b5eed07f": {"node_ids": ["67a0324c-6fe7-40b1-be46-627a9ad2233e"], "metadata": {"page_label": "11", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "17e04d86-1cca-4f25-85db-eaeafa02e575": {"node_ids": ["2c09db04-2675-4b0b-9bbc-de3ad5d1c664"], "metadata": {"page_label": "12", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a2e93cd9-3b0b-4f27-ac99-059e8a76b03e": {"node_ids": ["12af3b18-6bc7-41e3-9fe5-494324982f8a"], "metadata": {"page_label": "13", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "88a2232d-8553-4d2d-8011-b24a5c3e7370": {"node_ids": ["cb13c36b-d5d4-4a9f-8491-4ca9a13a3987"], "metadata": {"page_label": "14", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "75f1f244-a235-4305-9c7f-2cb28e21c5f3": {"node_ids": ["6ce1d6a6-51d7-4efe-bae1-784868504592"], "metadata": {"page_label": "15", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "1fe440f1-aa33-4c34-81c3-52cabb12e457": {"node_ids": ["a2c9d2b5-1c33-434f-9976-f0dc07b82b15"], "metadata": {"page_label": "16", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a97eb801-302b-491e-a50d-3561ed42ddc7": {"node_ids": ["7bb9d668-765b-4cab-a2d7-ede1e49fc77f"], "metadata": {"page_label": "17", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "261b10ec-031a-4407-8267-231ce7fda391": {"node_ids": ["1e8ba4c3-917a-4fe2-bbdf-05493604fcb4"], "metadata": {"page_label": "18", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "274809b3-12fc-4a47-b044-a014509ebad1": {"node_ids": ["575d903a-b2cf-4616-90b2-4d7b3bf52ca5"], "metadata": {"page_label": "19", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "cb3bd1fc-a037-4475-9bcf-87df0437c95b": {"node_ids": ["42c38932-b5f3-4ca1-9d7e-9b2a9f9a6964"], "metadata": {"page_label": "20", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "50e0664c-c917-4d0e-a0d5-680efe8ede59": {"node_ids": ["00dd7c54-2741-4adb-9aef-5cbbd5ac3c9f"], "metadata": {"page_label": "21", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ef10b3f7-7efe-4e64-ab51-25ca3093b75d": {"node_ids": ["2f0ae121-c09a-4eb8-9fe3-b0125220c0a2"], "metadata": {"page_label": "22", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5beb6031-2fe1-481b-beff-741786a969c2": {"node_ids": ["18e3f684-448e-4e44-ab33-df8e8228579d"], "metadata": {"page_label": "23", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "99d5f1a2-401a-42d4-8eb1-b51009ed22eb": {"node_ids": ["fa616148-0587-45b6-82e4-766d343d1652"], "metadata": {"page_label": "24", "file_name": "2311.12882v3.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2311.12882v3.pdf", "file_type": "application/pdf", "file_size": 343567, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "355fbc9b-0b18-4433-a65f-8a86adec65b5": {"node_ids": ["fc826763-bec0-4f45-986a-b93cb6c60a3f"], "metadata": {"page_label": "1", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "21199942-58e0-4859-bc7a-0f6ee3f37bd4": {"node_ids": ["b80bca60-f6aa-4f27-a855-fe18b88ba2fc", "c3afbdeb-3bb7-4df7-8503-e8daa3e6c830"], "metadata": {"page_label": "2", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "893f781a-3ebc-4402-a8a1-ecdea1abce5c": {"node_ids": ["a73ba4d8-c6cc-4448-b825-e593aa3aa137", "d0b6c73f-1fc9-4f57-9c7a-e6532bf2403c"], "metadata": {"page_label": "3", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0354d90d-1077-4f8c-91e7-ba5f57e7145b": {"node_ids": ["46b6babe-b7c4-4f00-984a-f7749b5e87c3"], "metadata": {"page_label": "4", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bf11f421-1744-4c25-b6fa-71afda8feaf6": {"node_ids": ["60123adf-e9a4-4de4-8985-c2b1920ad22c"], "metadata": {"page_label": "5", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9334f872-2c3f-403c-8b28-a0f1dbe08614": {"node_ids": ["91893c9f-a6fd-45d5-afd6-fcbdd72a4577"], "metadata": {"page_label": "6", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "63571516-7b79-4793-ab36-68eca6b1153c": {"node_ids": ["f07c6912-b33a-4398-b46f-a93bb7fb431f"], "metadata": {"page_label": "7", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "822263a0-8b86-4e38-81dc-307451364eef": {"node_ids": ["4b38c436-d3d9-4ef9-a0e6-55e1dc212ea2", "7159d3ff-2477-4cb3-9fe0-7ef020effd1a"], "metadata": {"page_label": "8", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "4ac156d7-1f5b-468f-a8be-d0b9ae53f19d": {"node_ids": ["e8a4ce77-ff5a-447a-9434-5cfa39f360b8"], "metadata": {"page_label": "9", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7dae1560-c3b2-4c30-8656-d79732c5747e": {"node_ids": ["0bc52f80-3f46-4a63-a512-8b1708456e63", "71591de7-42e7-4c8f-b0dd-2088f2b8be1f"], "metadata": {"page_label": "10", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7d07c078-8c04-4b93-b27e-37f0f861b119": {"node_ids": ["bddc14c7-1bae-47c3-90a4-2f624fdfe14e"], "metadata": {"page_label": "11", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "df6a132f-9aeb-4697-91ac-978e6214a708": {"node_ids": ["f10f0d8f-3a37-4e7a-976b-828da8e042bb", "14e832c6-feaf-46ad-9b7b-fdb555a59ccb"], "metadata": {"page_label": "12", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c94039f9-42a3-4576-9ecf-3f4f8d5757a0": {"node_ids": ["b82ca246-f89f-48df-add4-8f6c29c68c00"], "metadata": {"page_label": "13", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0b22a842-9dc5-478f-a4a3-6463d49e48a1": {"node_ids": ["01036672-0490-47fa-ac22-aafb48f3eabc"], "metadata": {"page_label": "14", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7ad143f3-d087-47f3-ab3a-86c01da0a4f3": {"node_ids": ["40b1d0c0-c1ce-41d8-9fb0-ddd8a864b5bf"], "metadata": {"page_label": "15", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3f0b3733-9ccd-4f0a-bf3e-3281d670693f": {"node_ids": ["510a3324-53c8-473e-9d97-13932633e897", "ea647641-cb13-4dd9-81e7-8d686d82610c"], "metadata": {"page_label": "16", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "bfe1f6ca-d68f-4dd9-8597-816e73b0d48e": {"node_ids": ["5023e770-6bb4-47a8-983d-cf4d360cf915", "b355eda9-a339-44a9-b6b8-4c54ff24c3f7"], "metadata": {"page_label": "17", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a24381f1-6ee9-4c34-b556-420b3b8a1a18": {"node_ids": ["85d6c4a4-5805-4cf9-a363-3e2c4e7efc9c", "d6882cb5-1f1c-4a76-aaf4-baf6b8b4edba"], "metadata": {"page_label": "18", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e72e558c-b782-402d-94cf-d856646050ed": {"node_ids": ["dfecbefc-244f-46a0-af2a-32081320275b", "d0048285-3393-4215-9e4d-c7045deb669d"], "metadata": {"page_label": "19", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d48125f8-7c95-47eb-a9a2-1da8a1f8d1d3": {"node_ids": ["e2bada17-28db-4cd1-bc2c-b956a25eb601", "04f32702-802b-4f2c-91e2-fb40c9220fdb"], "metadata": {"page_label": "20", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "6d36d50c-b2f8-4e34-b77c-b8b2ce8c4c7e": {"node_ids": ["1bb99826-27d1-456d-9f94-c2b4e3ef8656", "00a36b21-58a7-474a-a17a-d10b4f8300c3"], "metadata": {"page_label": "21", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5f06904e-15ed-478e-abb4-9ab5271686aa": {"node_ids": ["90d36207-4639-4922-94da-64578289536e"], "metadata": {"page_label": "22", "file_name": "2401.06775v2.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2401.06775v2.pdf", "file_type": "application/pdf", "file_size": 2065823, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "45524569-78a2-4cde-aeb8-f128e646ac8f": {"node_ids": ["826fbcd5-7820-4d18-a1de-f212746aa02a"], "metadata": {"page_label": "1", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "54ac6568-4203-4084-88eb-d6769455a89e": {"node_ids": ["416c2337-af97-4392-9e24-7201eae83534"], "metadata": {"page_label": "2", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "dfe714d2-e1b6-4c25-a61a-518c33a5575c": {"node_ids": ["a3703e7a-190d-4d88-b7d8-a648ddf7af4f"], "metadata": {"page_label": "3", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2474c822-8a9e-4dd8-a474-649c2dfbc97c": {"node_ids": ["329c01f5-9b2e-4679-8727-a2823e175a4d"], "metadata": {"page_label": "4", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "06b045bd-cd6d-430f-8c13-ff1213437634": {"node_ids": ["77f3c97e-985e-43d4-a5ed-f6e4852dba6a"], "metadata": {"page_label": "5", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2f815988-3baf-4cef-bb1f-7d61cd3ecb20": {"node_ids": ["7c02b933-8112-4689-991e-7864cbf19ca2"], "metadata": {"page_label": "6", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "17ce5e70-c8c4-4601-ae49-464564d311be": {"node_ids": ["8a1f3948-ce71-4a21-bc4a-415be4dd4d50"], "metadata": {"page_label": "7", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2997e529-0822-4056-8dbf-671fcb6ccef2": {"node_ids": ["edd9ee8d-07c2-4600-90db-bd22c7974238"], "metadata": {"page_label": "8", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "926f26ab-4fe6-4e4e-b1c6-66a39add1580": {"node_ids": ["03026346-e9a1-41c4-b320-6f173c8decf6"], "metadata": {"page_label": "9", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e89191dc-b97a-4a9c-8461-c1425bdfadff": {"node_ids": ["4971ef85-72d8-4528-8643-0f434c8cbed2"], "metadata": {"page_label": "10", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "17f53c4f-575e-44e1-ae90-aeed834cf636": {"node_ids": ["38e3f1a1-dbc5-4944-a358-5cabd0de0d5c"], "metadata": {"page_label": "11", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2216b8e9-52cc-4b9f-b043-030039650ed0": {"node_ids": ["ed7f8025-a763-429e-b3c7-364ce0b6e3c5"], "metadata": {"page_label": "12", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "726d259d-7897-4875-8bbc-80122e5e06d4": {"node_ids": ["e0734d32-68ea-48cf-9c80-13d000238241"], "metadata": {"page_label": "13", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "25e68918-db07-4fbd-a4d2-9649117a93a5": {"node_ids": ["c1955189-8c32-4f54-9088-726fdc835e7b"], "metadata": {"page_label": "14", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a6bf6155-a92c-4749-bc77-db31e0933ae6": {"node_ids": ["f4435aba-8a4b-4390-a9f7-c050bb7aac90"], "metadata": {"page_label": "15", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "ceae5917-d4c9-4bbf-96fd-1a624d6b468c": {"node_ids": ["290db0ce-65cc-43dd-a71c-8ca00e74bac3"], "metadata": {"page_label": "16", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7e5a266a-b720-4f03-b559-72dae883ad6b": {"node_ids": ["5dc69619-818d-42fc-a539-8713554dc9bd"], "metadata": {"page_label": "17", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5ffb5848-f30c-4cd1-acd7-593f5911b540": {"node_ids": ["1f143515-5f74-473e-b90b-14f80f898115"], "metadata": {"page_label": "18", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "93566e10-b601-4b41-b65e-649524bd5359": {"node_ids": ["a59775cd-a041-406e-8fbc-c108f640e4df"], "metadata": {"page_label": "19", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "76706c26-ad0f-4ab9-9344-2d9bb4b8f0bf": {"node_ids": ["52b37959-3467-4a41-897c-626d538c467a"], "metadata": {"page_label": "20", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b7e65899-2610-40f9-b4b4-2854fdb336c2": {"node_ids": ["8192c6ae-e949-4627-b834-d8c1bfbee1c7"], "metadata": {"page_label": "21", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5fb8a789-0d8e-42f3-a909-641e696f3a64": {"node_ids": ["16590794-e6c7-4a20-9ade-b4c80f1fab62"], "metadata": {"page_label": "22", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "54b19a41-4b7c-46b7-ab78-ad6a37f4aea9": {"node_ids": ["993587df-319b-476d-a277-af1a9bc2532a"], "metadata": {"page_label": "23", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "54e0e8a5-533d-4d9b-8409-cf348235ba04": {"node_ids": ["3b144cc8-10ff-4f9e-be1a-f2c31ab2618d"], "metadata": {"page_label": "24", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5c12a383-a724-42b8-ad82-60fa5bb05798": {"node_ids": ["e8fbe001-34b6-4a32-bb59-03abd03b578b"], "metadata": {"page_label": "25", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "27aefa1e-9901-4e6a-8636-26162d8e33b9": {"node_ids": ["82ac4e76-0d4b-4044-a5a5-9aeac3b8cd2b"], "metadata": {"page_label": "26", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f04354b3-730e-4646-be45-1f868be95f12": {"node_ids": ["e3e436e3-bc48-4eb7-9314-4f4c3813f316"], "metadata": {"page_label": "27", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a97c2773-c169-44f8-8180-e0e8eb5596cc": {"node_ids": ["56a86f1e-9706-4c58-9283-d417a3ea3953"], "metadata": {"page_label": "28", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0f2e6bec-c011-4b50-9a7f-c3310682035a": {"node_ids": ["57b81c63-7d90-4b01-a1d9-6de1aea87228"], "metadata": {"page_label": "29", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d2c6eeba-463b-40a2-85a9-64edec91ea1f": {"node_ids": ["44e8abd9-f6cf-4189-bf71-0df491339798"], "metadata": {"page_label": "30", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b84693ac-29f4-4d46-8330-a9ac96f93297": {"node_ids": ["f3b758dd-070c-4319-89c0-e2f0a70f8988"], "metadata": {"page_label": "31", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "9bae20ff-1e7d-453c-8786-99018c8f3aab": {"node_ids": ["28390e55-c742-4d60-8d97-4a9e56e3338d"], "metadata": {"page_label": "32", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "661bed5a-685f-4927-94e4-668b5a14d86b": {"node_ids": ["6c84e63b-65d2-410e-ba83-af14d1e52504", "5a4800bb-9d21-4359-b693-dc5655e980e9"], "metadata": {"page_label": "33", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e3c58480-196b-4c10-a19c-b7421d8df1b0": {"node_ids": ["e1c747c8-f399-4d1b-a0e5-847e262f457f"], "metadata": {"page_label": "34", "file_name": "2403.19802v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2403.19802v1.pdf", "file_type": "application/pdf", "file_size": 772208, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a442861f-e7c3-4f77-9b0b-d929e01bc4d3": {"node_ids": ["634b3a30-1f59-428e-8049-e37875aa1567"], "metadata": {"page_label": "1", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "772cc019-fd83-444d-b093-cadea3cad6da": {"node_ids": ["01e408dc-411e-48b7-a22f-d54e295b5e37", "c7e05597-bc46-4247-b94c-9da413c04f06"], "metadata": {"page_label": "2", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a9c2260f-e8fc-475b-9cb2-108fec145301": {"node_ids": ["b91c5ef0-8a42-4eee-b86a-35fc71cee60b"], "metadata": {"page_label": "3", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "c632f830-aa83-4fa5-ad9c-786d969f24e0": {"node_ids": ["c94f9853-bfb3-48bc-92f9-2039a8d7602e"], "metadata": {"page_label": "4", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "1d3e02ea-4951-4ea8-9eaf-e0a65b47a4e2": {"node_ids": ["0de573cf-e894-4295-a78f-f177ffc49e81"], "metadata": {"page_label": "5", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b5fd944c-94cc-4088-83bb-cc9d3ab7a782": {"node_ids": ["7078d875-a724-4cf0-8038-fbd380461fb8", "abed33f9-4a01-4211-bc5d-de0d3f2ac85a"], "metadata": {"page_label": "6", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a44aeda0-c79d-44d8-91ff-747169a5cbee": {"node_ids": ["a88b5526-c818-43ab-85f3-422de7672090", "454b7a86-41ae-4209-8717-88ad3da2c6d2"], "metadata": {"page_label": "7", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5775bed2-0724-4d6c-9f91-ad2509c057ae": {"node_ids": ["b8de74f8-009e-4739-a494-85d42f2a5189", "d18df246-18ee-4893-94fc-c276d0dbbd9d"], "metadata": {"page_label": "8", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "191f31e5-8703-4458-bda5-52a255fa0aa8": {"node_ids": ["3ba15767-1938-47c3-8631-73e0a5a74ef1"], "metadata": {"page_label": "9", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "de6de8cd-c1b8-43b0-a5c3-3ea94b57675f": {"node_ids": ["6bbf9615-df32-449f-b330-f05004702450"], "metadata": {"page_label": "10", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "19f073ff-bbee-479b-b397-9ab75a642722": {"node_ids": ["b29da23c-eb0f-47b0-9482-adbf688d83f5"], "metadata": {"page_label": "11", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "3dfdfbbe-c85c-4b1e-8b23-2428f4a0c24d": {"node_ids": ["41e50cb1-adba-4455-8b04-249f67cedc5f"], "metadata": {"page_label": "12", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "1c871c3f-c01e-470c-b2ba-db433ac6bf80": {"node_ids": ["bd9e15ab-3178-4524-be90-d16b1d00628e"], "metadata": {"page_label": "13", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "59bd979f-508f-4fd6-a92f-6351c5c02320": {"node_ids": ["9c419638-9c0d-4b3d-98c6-97dab3f1946d"], "metadata": {"page_label": "14", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "90aede91-8f1c-4d72-af2c-99ec4b6e6073": {"node_ids": ["2179cbe4-62de-463d-b8ba-fb7797e46856"], "metadata": {"page_label": "15", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2a24f7f7-192d-4f31-bb42-fea17c558007": {"node_ids": ["75e2d352-2b99-417a-a45c-ae8d64493cf2"], "metadata": {"page_label": "16", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f84244f5-bd27-4b1d-85c5-7d08aeb152a9": {"node_ids": ["056c79a2-c025-41fa-93a5-d10925dea0d5"], "metadata": {"page_label": "17", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "7c04d424-1174-42a0-b670-159795dc5edc": {"node_ids": ["6e16f9a0-1ca4-4645-8be7-f855a96f7098"], "metadata": {"page_label": "18", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "eec73f3c-8c2a-46c8-9c4c-163ea47a42da": {"node_ids": ["d834629b-9224-4491-9e1c-bcdb4d2d31d3"], "metadata": {"page_label": "19", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "a187bc3e-a0ee-48b8-8ac6-4b870d598867": {"node_ids": ["cfbf8a9e-d300-42e0-86cd-907d0d600a26"], "metadata": {"page_label": "20", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "170dadfc-4114-4ef7-99d9-c0b70ad30742": {"node_ids": ["e930d448-cce1-4628-9adb-d0a327a6814f"], "metadata": {"page_label": "21", "file_name": "2409.16860v1.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\arxiv\\2409.16860v1.pdf", "file_type": "application/pdf", "file_size": 136302, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d488e6fa-23af-4e1e-b57e-a76ae707ba17": {"node_ids": ["712198e4-720c-4f74-8f58-483d2c32eed5"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40250743.txt", "file_name": "pubmed_40250743.txt", "file_type": "text/plain", "file_size": 2534, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "de7cc68f-5f27-42a3-ba6d-baf1f0b5fa6d": {"node_ids": ["64f5d340-5b50-4d2f-804a-e7ce56414ab8"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251084.txt", "file_name": "pubmed_40251084.txt", "file_type": "text/plain", "file_size": 2664, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f40dc871-08ac-414d-aab6-7d6564623b84": {"node_ids": ["09e51e98-b374-42cf-b6f5-4f8a3e65b80d"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40251634.txt", "file_name": "pubmed_40251634.txt", "file_type": "text/plain", "file_size": 2693, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d44a3620-294e-4edc-ab54-ea79a41cfcc0": {"node_ids": ["97dcd730-bee1-4952-bf94-32617faeed85"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40252975.txt", "file_name": "pubmed_40252975.txt", "file_type": "text/plain", "file_size": 3225, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "22ac7c1a-0471-474f-9596-d1163ce43b2e": {"node_ids": ["dfa667d2-49e2-4956-b915-40320ed44a35"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40256630.txt", "file_name": "pubmed_40256630.txt", "file_type": "text/plain", "file_size": 1417, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "d70e8673-2d5c-4f71-831a-8dc443321596": {"node_ids": ["e02463e2-03de-47b2-b9af-da270498626b"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272658.txt", "file_name": "pubmed_40272658.txt", "file_type": "text/plain", "file_size": 3810, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "971f5035-f12e-4e2a-a610-7c84ae471b94": {"node_ids": ["27ceef58-8ed2-4100-8cff-e6473adbc9c9", "40bc5295-5805-4797-bd3c-1e61d1c39dd6"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\pubmed\\pubmed_40272787.txt", "file_name": "pubmed_40272787.txt", "file_type": "text/plain", "file_size": 4593, "creation_date": "2025-04-24", "last_modified_date": "2025-04-24"}}, "f23ac8f5-f0dc-4ee9-843b-7013db1a9663": {"node_ids": ["af7ff672-6ab4-4a6d-9684-dfffa5fca5be", "0046ba3d-ab8f-498f-9821-b3d1928c7b92"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\NLP-IA[1].pptx", "file_name": "NLP-IA[1].pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 4000471, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "5eb950a6-42f9-4bae-a201-0517e84a621d": {"node_ids": ["a82ca2e3-34f6-4756-bee9-e6d6a0cd3bb6"], "metadata": {"page_label": "1", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "2e88584d-da10-4fcf-a713-1351658c2553": {"node_ids": ["dbafb3ed-b5fd-48de-b6e5-0eeb67a0210d"], "metadata": {"page_label": "2", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "6577e924-2e07-4990-a65d-10b412405651": {"node_ids": ["585216c4-3a9b-4764-8210-098419010237"], "metadata": {"page_label": "3", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "350aa6fc-67a4-4a4e-94ce-9e76e70901cc": {"node_ids": ["1277b973-1bf5-40b3-83f6-483744ab302d"], "metadata": {"page_label": "4", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "e0a38699-3a9f-4959-acac-a10f6cd3eedc": {"node_ids": ["8e0e599c-8f25-4a3c-b91d-7bf4be37aed4"], "metadata": {"page_label": "5", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "f5170a1a-24c4-4478-89ad-4341a0108c6e": {"node_ids": ["eed2712a-a74f-4863-97d5-f5387e927b28"], "metadata": {"page_label": "6", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "0d982ed5-25b9-4858-b7b3-c7f493969896": {"node_ids": ["d9e623c1-9ff4-4d8c-8dcc-75fcedab04d6"], "metadata": {"page_label": "7", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "64eac5fd-e635-478d-b227-b7938c8a3ab9": {"node_ids": ["ff21d60b-a167-43b8-8e27-d38ec1832bc0"], "metadata": {"page_label": "8", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "4e087112-5746-4a71-a607-644442d7ef26": {"node_ids": ["d77e3e3f-7779-4458-a076-82cee750a74c"], "metadata": {"page_label": "9", "file_name": "Pipeline RAG Complet.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pdf", "file_type": "application/pdf", "file_size": 6026305, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "b77ddef4-24d2-4599-8a26-3ceb39d934be": {"node_ids": ["ca1636da-1fba-4f40-b60c-a8b7c053fa37"], "metadata": {"file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Pipeline RAG Complet.pptx", "file_name": "Pipeline RAG Complet.pptx", "file_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation", "file_size": 5663846, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}, "47f78103-6b65-4f99-98de-a6c968bc23ab": {"node_ids": ["e357dbe1-1712-466f-a7ba-868095de8c61"], "metadata": {"page_label": "1", "file_name": "Your paragraph text.pdf", "file_path": "C:\\Users\\LENOVO\\Documents\\My Web Sites\\Chatbot\\data\\user_uploads\\Your paragraph text.pdf", "file_type": "application/pdf", "file_size": 1394120, "creation_date": "2025-04-21", "last_modified_date": "2025-04-21"}}}}